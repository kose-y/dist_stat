import torch
from .pet_utils import *
class PET():
    def __init__(self, y, emat, G, D, mu=1e-6, TType=torch.DoubleTensor, eps=1e-20):
        """
        y: count data, d x 1
        emat: "E" matrix generated by E_generate, d x p dense (or sparse)
        g: neighboorhood matrix (sparse), p x p sparse
        d: difference matrix (sparse) to define loss, (#edges) x p sparse
        mu: smoothness parameter
        """
        emat_typename = torch.typename(emat).split('.')[-1]
        self.d = emat.shape[0]
        self.p = emat.shape[1]
        if emat.is_cuda:
            SpTType = getattr(torch.cuda.sparse, emat_typename)
        else:
            SpTType = getattr(torch.sparse, emat_typename)
        self.y = y.type(TType).view(-1, 1)
        self.emat = emat.type(TType)
        self.G= G.type(SpTType)
        self.D = D.type(SpTType)
        self.mu = mu
        self.eps = eps
        # scale E to have unit l1 column norms : already done in generator
        # copute |N_j|  = g @ 1
        self.N = torch.mm(self.G, torch.ones(G.shape[1], 1).type(TType))
        # a_j = -2 * mu * |N_j|, j = 1, ..., p
        self.a  = -2 * self.mu * self.N 
        # initialize: lambda_j = 1, j = 1, ..., p
        self.lambd = torch.ones(G.shape[0], 1).type(TType)
        self.lambd_prev = torch.ones_like(self.lambd)
        
        self.prev_obj=inf

    def update(self):
        # update z
        el = torch.mm(self.emat, self.lambd) 
        gl = torch.mm(self.G, self.lambd)
        z = self.y * self.emat * self.lambd.view(1,-1)/(el + self.eps)
        # update b
        b = self.mu * (self.N*self.lambd + gl) -1
        # update c 
        c = torch.sum(z, dim=0).view(-1,1)
        # update lambda
        if self.mu != 0:
            self.lambd = (-b - torch.sqrt(b**2 - 4* self.a * c))/(2 * self.a + self.eps)
        else:
            self.lambd = -c/(b+self.eps)
    def get_objective(self):
        el = torch.mm(self.emat, self.lambd)
        likelihood = torch.sum(self.y * torch.log(el+self.eps) - el)
        dl = torch.mm(self.D, self.lambd)
        penalty = - self.mu/2.0 * torch.sum(dl**2)
        return  likelihood + penalty
    def check_convergence(self, tol, verbose=True, check_obj=False, check_interval=1):
        obj = None
        diff_norm = torch.max(torch.abs(self.lambd_prev - self.lambd))
        if check_obj:
            obj = self.get_objective()
            reldiff = abs(self.prev_obj - obj)/((abs(obj)+1)*check_interval)
            converged = reldiff < tol
            self.prev_obj = obj
        else:
            reldiff = None
            converged = diff_norm < tol
        return (diff_norm, reldiff, obj), converged
    def run(self, maxiter=1000, tol=1e-5, check_interval=1, verbose=True, check_obj=False):
        if verbose:
            print("Starting...")
            print("d={}, p={}".format(self.d, self.p))
            if not check_obj:
                print("%6s\t%13s\t%15s" % ("iter", "maxdiff", "time"))
            else:
                print("%6s\t%13s\t%15s\t%15s\t%10s" % ("iter", "maxdiff", "reldiff", "obj", "time" ))
            print('-'*80)
        t0 = time.time()
        t_start = t0
        print(self.get_objective())

        for i in range(maxiter):
            self.lambd_prev.copy_(self.lambd)
            self.update()
            if (i+1) % check_interval ==0:
                t1 = time.time()
                (maxdiff, reldiff, obj), converged = self.check_convergence(tol, verbose, check_obj, 
                                                                            check_interval)
                if verbose:
                    if not check_obj:
                        print("%6d\t%13.4e\t%10.5f" % (i+1, maxdiff, t1-t0))
                    else:
                        print("%6d\t%13.4e\t%15.9e\t%15.9e\t%10.5f" % (i+1, maxdiff, reldiff,
                                                                            obj, t1-t0))
                if converged: break
                t0 = t1

        if verbose:
            print('-'*80)
            print("Completed. total time: {}".format(time.time()-t_start))

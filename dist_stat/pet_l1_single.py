import torch
from .pet_utils import *

class PET_L1():
    def __init__(self, y, emat, D, sig, tau, rho=1e-2, TType=torch.DoubleTensor, eps=1e-20):
        """
        y: count data, d x 1
        emat: "E" matrix generated by E_generate, d x p dense (or sparse)
        d: difference matrix (sparse) to define loss, (#edges) x p sparse
        mu: smoothness parameter
        """
        emat_typename = torch.typename(emat).split('.')[-1]
        self.d = emat.shape[0]
        self.p = emat.shape[1]
        if emat.is_cuda:
            SpTType = getattr(torch.cuda.sparse, emat_typename)
        else:
            SpTType = getattr(torch.sparse, emat_typename)
        self.y = y.type(TType).view(-1, 1)

        self.E = emat.type(SpTType)
        self.Et = self.E.t().coalesce()
        self.E = self.E.coalesce()

        #self.G= G.type(SpTType)
        self.D = D.type(SpTType)
        self.Dt = self.D.t().coalesce()
        self.D = self.D.coalesce()


        self.rho = rho

        self.z = - torch.ones(self.E.shape[0], 1).type(TType)
        self.w = torch.zeros(self.D.shape[0], 1).type(TType)
        self.lambd = torch.ones(self.E.shape[1], 1).type(TType)
        self.lambd_old = torch.ones_like(self.lambd)
        self.Et1     = torch.mm(self.Et, torch.ones(self.E.shape[0], 1).type(TType))
        #self.eps = eps
        # scale E to have unit l1 column norms : already done in generator
        # copute |N_j|  = g @ 1
        # self.N = torch.mm(self.G, torch.ones(G.shape[1], 1).type(TType))
        # a_j = -2 * mu * |N_j|, j = 1, ..., p
        #self.a  = -2 * self.mu * self.N 
        # initialize: lambda_j = 1, j = 1, ..., p
        self.lambd = torch.ones(self.E.shape[1], 1).type(TType)
        self.lambd_prev = torch.ones_like(self.lambd)
        
        self.prev_obj=inf
        
        self.tau = tau
        self.sig = sig
        self.rho = rho
        self.eps = eps
        

    def update(self):
        tau = self.tau; sig = self.sig
        Etz = torch.mm(self.Et, self.z)
        Dtw = torch.mm(self.Dt, self.w)
        self.lambd = torch.clamp(self.lambd - tau * (Etz + Dtw + self.Et1), min=0.0)
        lambd_tilde = 2*self.lambd - self.lambd_old
        el = torch.mm(self.E, self.lambd_tilde) 
        self.z = self.z + sig * el
        tmp = torch.sqrt(self.z**2 + 4*sig * self.y)
        self.z = 0.5 * (self.z - tmp)
        dl = torch.mm(self.D, lambd_tilde)
        self.w = self.w + sig * dl
        self.w = torch.clamp(self.w, max=self.rho, min=-self.rho)
    def get_objective(self):
        el = torch.mm(self.E, self.lambd)
        likelihood = torch.sum(self.y * torch.log(el+self.eps) - el)
        dl = torch.mm(self.D, self.lambd)
        penalty = - self.rho * torch.sum(dl.abs())
        return  likelihood + penalty
    def check_convergence(self, tol, verbose=True, check_obj=False, check_interval=1):
        obj = None
        diff_norm = torch.max(torch.abs(self.lambd_prev - self.lambd))
        if check_obj:
            obj = self.get_objective()
            reldiff = abs(self.prev_obj - obj)/((abs(obj)+1)*check_interval)
            converged = reldiff < tol
            self.prev_obj = obj
        else:
            reldiff = None
            converged = diff_norm < tol
        return (diff_norm, reldiff, obj), converged
    def run(self, maxiter=1000, tol=1e-5, check_interval=1, verbose=True, check_obj=False):
        if verbose:
            print("Starting...")
            print("d={}, p={}".format(self.d, self.p))
            if not check_obj:
                print("%6s\t%13s\t%15s" % ("iter", "maxdiff", "time"))
            else:
                print("%6s\t%13s\t%15s\t%15s\t%10s" % ("iter", "maxdiff", "reldiff", "obj", "time" ))
            print('-'*80)
        t0 = time.time()
        t_start = t0
        print(self.get_objective())

        for i in range(maxiter):
            self.lambd_prev.copy_(self.lambd)
            self.update()
            if (i+1) % check_interval ==0:
                t1 = time.time()
                (maxdiff, reldiff, obj), converged = self.check_convergence(tol, verbose, check_obj, 
                                                                            check_interval)
                if verbose:
                    if not check_obj:
                        print("%6d\t%13.4e\t%10.5f" % (i+1, maxdiff, t1-t0))
                    else:
                        print("%6d\t%13.4e\t%15.9e\t%15.9e\t%10.5f" % (i+1, maxdiff, reldiff,
                                                                            obj, t1-t0))
                if converged: break
                t0 = t1

        if verbose:
            print('-'*80)
            print("Completed. total time: {}".format(time.time()-t_start))

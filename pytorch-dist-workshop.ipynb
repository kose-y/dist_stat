{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Distributed Matrix Data Structure and Its Statistical Applications on PyTorch\n",
    "\n",
    "**The Programming Workshop at the Inaugural Kenneth Lange Symposium, Feb 21-22, 2020**\n",
    "\n",
    "_Seyoon Ko and Joong-Ho (Johann) Won_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Synopsis\n",
    "\n",
    "We developed a distributed matrix operation package suitable for distributed matrix-vector operations and distributed tall-and-thin (or wide-and-short) matrices.\n",
    "The code runs on both multi-node machines and multi-GPU machines using PyTorch.\n",
    "We have applied this package for four statistical applications, namely, nonnegative matrix factorization (NMF), multidimensional scaling (MDS), positron emission tomography (PET), and $\\ell_1$-regularized Cox regression.\n",
    "In particular, $200,000 \\times 500,000$ $\\ell_1$-regularized Cox regression with the UK Biobank dataset was the biggest joint multivariate survival analysis to our knowledge. \n",
    "In this workshop, we provide small examples that run on a single node, and demonstrate multi-GPU usage on our machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "* Brief introduction to PyTorch operations\n",
    "* `torch.distributed` package\n",
    "* Distributed matrix data structure in package `dist_stat`\n",
    "* Applications: Nonnegative matrix factorization and $\\ell_1$-penalized Cox regression\n",
    "* Demonstration on multi-GPU machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. It has two goals of development:\n",
    "* A replacement for NumPy to use the power of GPUs $\\rightarrow$ optimization of numerical operations\n",
    "* A deep learning research platform that provides maximum flexibility and speed $\\rightarrow$ optimization of automatic gradient computation e.g. backpropagation\n",
    "\n",
    "We are trying to exploit the former in a distributed environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic PyTorch Operations\n",
    "We introduce simple operations on PyTorch. Note that Python uses 0-based, row-major ordering, like C and C++ (cf. R and Julia have 1-based, column-major ordering). First we import the PyTorch\n",
    "library. This is similar to `library()` in R and equivalent to `import ...` in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning) # hide `UserWarning`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensor Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One may create an uninitialized tensor. This creates a 3 × 4 tensor (matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.4066e+30, 3.0740e-41, 8.4066e+30, 3.0740e-41],\n",
       "        [8.4066e+30, 3.0740e-41, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, 4) # uninitialized tensor. Julia equivalent: Array{Float32}(undef, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following is equivalent to `set.seed()` in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe536fd3b70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100) # Julia equivalent: Random.seed!(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This generates a tensor initialized with random values from (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1117, 0.8158, 0.2626, 0.4839],\n",
       "        [0.6765, 0.7539, 0.2627, 0.0428],\n",
       "        [0.2080, 0.1180, 0.1217, 0.7356]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(3, 4) # from Unif(0, 1). \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also generate a tensor filled with zeros or ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.ones(3, 4) # torch.zeros(3, 4)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A tensor can be created from standard Python data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([3, 4, 5, 6])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A tensor can be created in certain datatype (default: float32) and on certain device (default: CPU) of choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4., 5., 6.], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double precision\n",
    "w = torch.tensor([3, 4, 5, 6], dtype=torch.float64)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# # on GPU number zero. will not run if CUDA GPU is not present.\n",
    "# w = torch.tensor([3, 4, 5, 6], device='cuda:0')\n",
    "# w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shape of a tensor can be accessed by appending `.shape` to the tensor name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A tensor can have datatype and location changed by the method `.to()`. The arguments are similar to choosing datatype and device of the new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = w.to(device = \"cpu\", dtype=torch.int32)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following are standard method of indexing tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7356)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2, 3] # indexing: zero-based, returns a 0-dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The indexing always returns a (sub)tensor, even for scalars (treated as zero-dimensional tensors).\n",
    "A standard Python number can be returned by using .item()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7355988621711731"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2, 3].item() # A standard Python floating-point number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To get a column from a tensor, we use the indexing as below. The syntax is similar but slightly\n",
    "different from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4839, 0.0428, 0.7356])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, 3] # 3rd column. The leftmost column is 0th. cf. y[, 4] in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The following is for taking a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2080, 0.1180, 0.1217, 0.7356])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2, :] # 2nd row. The top row is 0th. cf. y[3, ] in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here we provide an example of simple operations on PyTorch. Addition using the operator ‘+’ acts\n",
    "just like anyone can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = y + z # a simple addition.\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here is another form of addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.add(y, z) # another syntax for addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The operators ending with an underscore (`_`) changes the value of the tensor in-place. Otherwise, the argument never changes. Unlike methods ending with `!` in Julia, this rule is strictly enforced in PyTorch. (The underscore determines usage of the keyword `const` in C++-level.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(z) # in-place addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can concatenate the tensors using the function `cat()`, which resembles `c()`, `cbind()`, and\n",
    "`rbind()` in R, `cat()`, `vcat()`, `hcat()` in Julia. The second argument indicates the dimension that the tesors are concatenated\n",
    "along: zero means by concatenation by rows, and one means by columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((y, z), 0) # along the rows. cf. vcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356, 1.0000, 1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((y, z), 1) # along the columns. cf. hcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summation/reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Calling `.sum()`, `.prod()`, `mean()` methods of a tensor do the obvious. Optional argument determines the dimension of reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5933)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9962, 4.6878, 3.6469, 4.2623])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(0) # reduces rows, columnwise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.6739, 5.7359, 5.1834])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(1) # reduces columns, rowwise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5933)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum((0, 1)) # reduces rows and columns -> a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Matrix transpose is performed by appending `.t()` to a tensor. Matrix multiplication is carried out by the method `torch.mm()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6739, 5.6739, 5.6739],\n",
       "        [5.7359, 5.7359, 5.7359],\n",
       "        [5.1834, 5.1834, 5.1834]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(y, z.t()) # Note: y is 3 x 4, z is 3 x 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `torch.distributed`: Distributed subpackage for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`torch.distributed` is the subpackage for distributed operations on PyTorch. The interface is mostly inspired by the message passing interface (MPI). The available backends are:\n",
    "\n",
    "* Gloo, a collective communication library developed by Facebook, included in PyTorch. Full support for CPU, partial collective communication only for GPU.\n",
    "* MPI, a good-old communication standard. The most flexible, but PyTorch needs to be compiled from its source to use it as a backend. Full support for GPU if the MPI installation is \"CUDA-aware\".\n",
    "* NCCL, Nvidia Collective Communications Library, collective communication only for multiple GPUs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this workshop, we use Gloo for its full functionalities on CPU and runnability on Jupyter Notebook. The experiments in our paper use MPI for running multi-node setting and multi-GPU setting with basically the same code. The interface below is specific for Gloo backend. For MPI backend, please consult with a section from [distributed package tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends) or [our example code](https://github.com/kose-y/dist_stat/tree/master/examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "def init_process(rank, size, fn, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "def run_process(size, fn):\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, fn))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        \n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each distributed function from now on will have `rank` and `size` as the two arguments. When `run_process` is called with the communicator size `size` and the function name `fn`, `size` process will be launched, and each of them will call `fn` with `rank` of each process and the `size`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Point-to-point communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://pytorch.org/tutorials/_images/send_recv.png)\n",
    "\n",
    "Figure courtesy of: https://pytorch.org/tutorials/_images/send_recv.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def point_to_point(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    elif rank == 1:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    dist.barrier()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  2  has data  tensor(0.)\n",
      "Rank  3  has data  tensor(0.)\n",
      "Rank  1  has data  tensor(1.)\n",
      "Rank  0  has data  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, point_to_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collective communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Scatter | Gather | \n",
    "|:---|:---|\n",
    "| ![](https://pytorch.org/tutorials/_images/scatter.png) | ![](https://pytorch.org/tutorials/_images/gather.png) |\n",
    "\n",
    "Table courtesy of: https://pytorch.org/tutorials/intermediate/dist_tuto.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Reduce | All-reduce | \n",
    "|:---|:---|\n",
    "| ![](https://pytorch.org/tutorials/_images/reduce.png) | ![](https://pytorch.org/tutorials/_images/all_reduce.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| Broadcast | All-gather |\n",
    "|:---|:---|\n",
    "| ![](https://pytorch.org/tutorials/_images/broadcast.png) | ![](https://pytorch.org/tutorials/_images/all_gather.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The below is the code for simple Monte Carlo $\\pi$ estimation. 100,000 $x_i$ and $y_i$ are sampled from $Unif(0,1)$ for each process, and we measure the proportion of $(x_i, y_i)$s inside the unit quartercircle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mc_pi(n, size):\n",
    "    # this code is executed on each process.\n",
    "    x = torch.rand((n), dtype=torch.float64)\n",
    "    y = torch.rand((n), dtype=torch.float64)\n",
    "    # compute local estimate of pi\n",
    "    r = torch.mean((x**2 + y**2 < 1).to(dtype=torch.float64))*4\n",
    "    dist.all_reduce(r) # sum of 'r's in each device is stored in 'r'\n",
    "    return r / size\n",
    "\n",
    "def run_mc_pi(rank, size):\n",
    "    n = 100000\n",
    "    torch.manual_seed(100 + rank)\n",
    "    r = mc_pi(n, size)\n",
    "    if rank == 0:\n",
    "        print(r.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14356\n"
     ]
    }
   ],
   "source": [
    "run_process(4, run_mc_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `distmat`: Distributed Matrices on PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tensor operations and communication package, we created a data structure for a distributed matrix. \n",
    "* Each process (enumerated by rank) holds a contiguous block of the full data matrix by rows or columns.\n",
    "* The data may be a sparse matrix. \n",
    "* If GPUs are involved, each process controls a GPU whose index matches the process rank. \n",
    "* From now on: [100] × 100 matrix split over four processes means...\n",
    "    * Rank 0 process keeps rows [0:25). of the matrix, in row-major ordering.\n",
    "    * Rank 3 process keeps rows [75:100).\n",
    "* Limitation: length of distributed dimension should be divisible by number of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dist_stat.distmat as distmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `distgen_ones()`: Creates a distributed matrix filled with ones\n",
    "- `distgen_zeros()`: Creates a distributed matrix filled with zeros\n",
    "- `distgen_uniform()`: Creates a distributed matrix from uniform distribution\n",
    "- `distgen_normal()`: Creates a distributed matrix from stndard normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unif(rank, size):\n",
    "    A = distmat.distgen_uniform(8, 8, TType=torch.DoubleTensor) # the default is row-distributed, row-major ordering data. \n",
    "    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: Rank  1  has data  tensor([[0.5459, 0.3573, 0.2406, 0.5600, 0.6501, 0.9692, 0.5168, 0.2422],\n",
      "        [0.4312, 0.5917, 0.3425, 0.2202, 0.7030, 0.5629, 0.9259, 0.7612]],\n",
      "       dtype=torch.float64)Matrix A: Rank  2  has data  tensor([[0.8381, 0.4943, 0.5984, 0.6167, 0.6128, 0.8593, 0.1344, 0.5146],\n",
      "        [0.1479, 0.4238, 0.5144, 0.7051, 0.8133, 0.1795, 0.2721, 0.4631]],\n",
      "       dtype=torch.float64)\n",
      "\n",
      "Matrix A: Rank  3  has data  tensor([[0.6214, 0.7096, 0.7099, 0.7977, 0.8779, 0.1236, 0.5650, 0.0655],\n",
      "        [0.1210, 0.0454, 0.5070, 0.1860, 0.5035, 0.1454, 0.8090, 0.4991]],\n",
      "       dtype=torch.float64)\n",
      "Matrix A: Rank  0  has data  tensor([[0.6941, 0.3464, 0.9751, 0.7911, 0.4274, 0.4460, 0.5522, 0.9559],\n",
      "        [0.9405, 0.2215, 0.3271, 0.1352, 0.6283, 0.3030, 0.1302, 0.1811]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, create_unif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distributed matrix can also be created from local chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_chunks(rank, size):\n",
    "    torch.manual_seed(100 + rank)\n",
    "    chunk = torch.randn(2, 4)\n",
    "    print(\"rank \", rank, \"has chunk\", chunk)\n",
    "    A = distmat.THDistMat.from_chunks(chunk)\n",
    "    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)    \n",
    "    if rank == 0:\n",
    "        print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank  1 has chunk tensor([[-1.3905, -0.8152, -0.3204,  0.7377],\n",
      "        [-1.7534,  0.6033, -0.2520, -0.4373]])\n",
      "rank  2 has chunk tensor([[ 0.9907,  0.3349,  1.1497, -0.5498],\n",
      "        [-0.1046,  2.0104, -0.7886, -0.1246]])\n",
      "rank  3 has chunk tensor([[ 1.7286, -0.4007,  2.5587,  1.6848],\n",
      "        [-1.6571, -0.2811,  0.7743, -0.9554]])\n",
      "rank  0 has chunk tensor([[ 0.3607, -0.2859, -0.3938,  0.2429],\n",
      "        [-1.3833, -2.3134, -0.3172, -0.8660]])\n",
      "Matrix A: Rank  0  has data  tensor([[ 0.3607, -0.2859, -0.3938,  0.2429],\n",
      "        [-1.3833, -2.3134, -0.3172, -0.8660]])Matrix A: Rank  3  has data  tensor([[ 1.7286, -0.4007,  2.5587,  1.6848],\n",
      "        [-1.6571, -0.2811,  0.7743, -0.9554]])Matrix A: Rank  1  has data  tensor([[-1.3905, -0.8152, -0.3204,  0.7377],\n",
      "        [-1.7534,  0.6033, -0.2520, -0.4373]])Matrix A: Rank  2  has data  tensor([[ 0.9907,  0.3349,  1.1497, -0.5498],\n",
      "        [-0.1046,  2.0104, -0.7886, -0.1246]])\n",
      "\n",
      "\n",
      "\n",
      "[8, 4]\n"
     ]
    }
   ],
   "source": [
    "run_process(4, from_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be distributed from the master process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_data(rank, size):\n",
    "    if rank == 0:\n",
    "        data = torch.rand(4, 2)\n",
    "        print(\"master data: \", data)\n",
    "    else:\n",
    "        data = None\n",
    "    \n",
    "    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n",
    "    print('data_dist: ', 'Rank ', rank, ' has data ', data_dist.chunk)\n",
    "    dist.barrier()\n",
    "    print(data_dist.shape) # shape of the distributed matrix\n",
    "    dist.barrier()\n",
    "    print(data_dist.sizes) # sizes along the distributed dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master data:  tensor([[0.7118, 0.7876],\n",
      "        [0.4183, 0.9014],\n",
      "        [0.9969, 0.7565],\n",
      "        [0.2239, 0.3023]])\n",
      "data_dist:  Rank  0  has data  tensor([[0.7118, 0.7876]], dtype=torch.float64)\n",
      "data_dist:  Rank  2  has data  tensor([[0.9969, 0.7565]], dtype=torch.float64)\n",
      "data_dist:  Rank  1  has data  tensor([[0.4183, 0.9014]], dtype=torch.float64)\n",
      "data_dist:  Rank  3  has data  tensor([[0.2239, 0.3023]], dtype=torch.float64)\n",
      "[4, 2]\n",
      "[4, 2]\n",
      "[4, 2]\n",
      "[4, 2]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "run_process(4, dist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The default is to create a row-major matrix. They can easily be changed to a column-major matrix by transposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the basic functions work naturally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elemwise_1(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier() # dist.barrier() waits until all other processes reach the same line. It is used to make the output easier to read. \n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier()\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk)    \n",
    "    C = A + B\n",
    "    dist.barrier()\n",
    "    print(\"C: rank \", rank, \"has chunk\", C.chunk)    \n",
    "    C.add_(A) # functionally equivalent to C += A\n",
    "    dist.barrier()\n",
    "    print(\"C: rank \", rank, \"has chunk\", C.chunk)\n",
    "    \n",
    "    logC = C.log()\n",
    "    dist.barrier()\n",
    "    print(\"logC: rank \", rank, \"has chunk\", logC.chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "B: rank  1 has chunk tensor([[0.3271, 0.1352]], dtype=torch.float64)\n",
      "B: rank  3 has chunk tensor([[0.1302, 0.1811]], dtype=torch.float64)\n",
      "B: rank  0 has chunk tensor([[0.9405, 0.2215]], dtype=torch.float64)\n",
      "B: rank  2 has chunk tensor([[0.6283, 0.3030]], dtype=torch.float64)\n",
      "C: rank  2 has chunk tensor([[1.0556, 0.7490]], dtype=torch.float64)\n",
      "C: rank  3 has chunk tensor([[0.6824, 1.1369]], dtype=torch.float64)\n",
      "C: rank  1 has chunk tensor([[1.3022, 0.9263]], dtype=torch.float64)\n",
      "C: rank  0 has chunk tensor([[1.6346, 0.5680]], dtype=torch.float64)\n",
      "C: rank  2 has chunk tensor([[1.4830, 1.1949]], dtype=torch.float64)\n",
      "C: rank  0 has chunk tensor([[2.3287, 0.9144]], dtype=torch.float64)\n",
      "C: rank  1 has chunk tensor([[2.2773, 1.7175]], dtype=torch.float64)\n",
      "C: rank  3 has chunk tensor([[1.2346, 2.0928]], dtype=torch.float64)\n",
      "logC: rank  2 has chunk tensor([[0.3941, 0.1781]], dtype=torch.float64)\n",
      "logC: rank  3 has chunk tensor([[0.2107, 0.7385]], dtype=torch.float64)\n",
      "logC: rank  1 has chunk tensor([[0.8230, 0.5409]], dtype=torch.float64)\n",
      "logC: rank  0 has chunk tensor([[ 0.8453, -0.0895]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, elemwise_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting (similar to Julia's dot operation) also works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_broadcasting(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2) # [4] x 2 \n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_ones(4, 1) # [4] x 1\n",
    "    dist.barrier()\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk)\n",
    "    A.add_(B) # B treated as [4] x 2 matrix\n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    C = 2 * torch.ones(1, 2, dtype=torch.float64) # 1 x 2\n",
    "    A.add_(C) # C treated as [4] x 2 matrix\n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "B: rank  0 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "B: rank  2 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "B: rank  3 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "B: rank  1 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[1.6941, 1.3464]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[1.9751, 1.7911]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[1.4274, 1.4460]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[1.5522, 1.9559]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[3.4274, 3.4460]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[3.9751, 3.7911]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[3.6941, 3.3464]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[3.5522, 3.9559]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, dim_broadcasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For general functions, we have `.apply()` and `.apply_binary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elemwise_2(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier()\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk) \n",
    "    Asqp1 = A.apply(lambda x: x**2 + 1) # anonymous function definition: x -> x .^ 2 .+ 1 in Julia\n",
    "    dist.barrier()\n",
    "    print(\"Asqp1: rank \", rank, \"has chunk\", Asqp1.chunk)    \n",
    "    AsqpBsq = A.apply_binary(B, lambda x, y: x**2 + y**2) # anonymous function definition: (x, y) -> x.^2 .+ y .^2 in Julia\n",
    "    dist.barrier()\n",
    "    print(\"AsqpBsq: rank \", rank, \"has chunk\", AsqpBsq.chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "B: rank  1 has chunk tensor([[0.3271, 0.1352]], dtype=torch.float64)\n",
      "B: rank  3 has chunk tensor([[0.1302, 0.1811]], dtype=torch.float64)\n",
      "B: rank  0 has chunk tensor([[0.9405, 0.2215]], dtype=torch.float64)\n",
      "B: rank  2 has chunk tensor([[0.6283, 0.3030]], dtype=torch.float64)\n",
      "Asqp1: rank  0 has chunk tensor([[1.4818, 1.1200]], dtype=torch.float64)\n",
      "Asqp1: rank  1 has chunk tensor([[1.9508, 1.6259]], dtype=torch.float64)\n",
      "Asqp1: rank  3 has chunk tensor([[1.3049, 1.9137]], dtype=torch.float64)\n",
      "Asqp1: rank  2 has chunk tensor([[1.1826, 1.1989]], dtype=torch.float64)\n",
      "AsqpBsq: rank  0 has chunk tensor([[1.3663, 0.1691]], dtype=torch.float64)\n",
      "AsqpBsq: rank  2 has chunk tensor([[0.5774, 0.2907]], dtype=torch.float64)\n",
      "AsqpBsq: rank  3 has chunk tensor([[0.3219, 0.9465]], dtype=torch.float64)\n",
      "AsqpBsq: rank  1 has chunk tensor([[1.0578, 0.6442]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, elemwise_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions (sum, max, min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summations, minimums, and maximums can be carried out in a way similar to local tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reductions(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    dist.barrier()\n",
    "    print(\"sum of A: \", A.sum())\n",
    "    dist.barrier()\n",
    "    print(\"maximum of A: \", A.max())\n",
    "    dist.barrier()\n",
    "    print(\"minimum of A: \", A.min())\n",
    "    \n",
    "    sumA_row = A.sum(0) # row sum, output: a tensor with the same values on all processes \n",
    "    sumA_col = A.sum(1) # col sum, output: a distributed matrix\n",
    "    dist.barrier()\n",
    "    print(\"row sum of A: \", sumA_row)\n",
    "    dist.barrier()\n",
    "    print(\"sumA_col: rank \", rank, \"has chunk\", sumA_col.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "sumA_col: rank  1 has chunk tensor([[1.7662]], dtype=torch.float64)\n",
      "sumA_col: rank  3 has chunk tensor([[1.5081]], dtype=torch.float64)\n",
      "sumA_col: rank  2 has chunk tensor([[0.8733]], dtype=torch.float64)\n",
      "sumA_col: rank  0 has chunk tensor([[1.0406]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, reductions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonals(rank, size):\n",
    "    if rank == 0:\n",
    "        p = 4\n",
    "        data = torch.randn(p, p)\n",
    "        print(\"master data: \", data)\n",
    "    else:\n",
    "        data = None\n",
    "        \n",
    "    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n",
    "    \n",
    "    diag1 = data_dist.diag() # distributed diagonal\n",
    "    print(\"diag1: rank \", rank, \"has chunk\", diag1.chunk)\n",
    "    \n",
    "    diag2 = data_dist.diag(distribute=False) # diagonal gathered for each process\n",
    "    print(\"diag2: \", diag2)\n",
    "    \n",
    "    data_dist.fill_diag_(0) # fill the diagonals with zeros\n",
    "    print(\"data_dist: rank \", rank, \"has chunk\", data_dist.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master data:  tensor([[ 0.6857,  0.7877, -0.9778,  2.1302],\n",
      "        [-3.1896,  1.5914, -0.0247, -0.8466],\n",
      "        [ 1.4205, -1.5741, -0.3572, -0.3097],\n",
      "        [ 1.1705, -0.5410, -0.7116,  0.0575]])\n",
      "diag1: rank  0 has chunk tensor([[0.6857]], dtype=torch.float64)\n",
      "diag1: rank  2 has chunk tensor([[-0.3572]], dtype=torch.float64)\n",
      "diag1: rank  1 has chunk tensor([[1.5914]], dtype=torch.float64)\n",
      "diag1: rank  3 has chunk tensor([[0.0575]], dtype=torch.float64)\n",
      "diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "data_dist: rank  0 has chunk tensor([[ 0.0000,  0.7877, -0.9778,  2.1302]], dtype=torch.float64)\n",
      "data_dist: rank  3 has chunk tensor([[ 1.1705, -0.5410, -0.7116,  0.0000]], dtype=torch.float64)\n",
      "\n",
      "data_dist: rank  2 has chunk tensor([[ 1.4205, -1.5741,  0.0000, -0.3097]], dtype=torch.float64)\n",
      "data_dist: rank  1 has chunk tensor([[-3.1896,  0.0000, -0.0247, -0.8466]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, diagonals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six different scenarios of matrix-matrix multiplications, each representing a different configuration of the split dimension of two input\n",
    "matrices and the output matrix, were considered and implemented. \n",
    "\n",
    "| Secnario | $A$ | $B$ | $AB$ | Description | Usage |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "| 1 | $r \\times$ [$p$] | $[p] \\times q$ | $r \\times$ [$q$]| Inner product, result distributed. | $V^T X$ |\n",
    "| 2 | $[p] \\times q$ | $[q] \\times r$ | $[p] \\times r$ | Fat matrix multiplied by a thin and tall matrix. | $X W^T$ |\n",
    "| 3 | $r \\times$ [$p$] | $[p] \\times s$ | $r \\times s$   | Inner product, result broadcasted. Inner product between two thin matrices. | $V^T V$, $W W^T$ |                                                                           \n",
    "| 4 | $[p] \\times r$ | $r \\times$ [$q$] | $[p] \\times q$ | Outer product, may require large amount of memory. For computing objective function. | $VW$ |\n",
    "| 5 | $[p] \\times r$ | $r \\times s$   | $[p] \\times s$ | A distributed matrix multiplied by a small, distributed matrix. | $VC$ where $C = WW^T$; $CW$ where $C = V^T V$ (transposed) |\n",
    "| 6 | $r \\times$ [$p$] | $p \\times s$   | $r \\times s$   | A distributed matrix multiplied by a thin-and-tall broadcasted matrix. | Matrix-broadcasted vector multiplications. |\n",
    "\n",
    "\n",
    "\n",
    "The implementation of each case is carried\n",
    "out using the collective communication directives. Matrix multiplication scenarios are automatically selected based on the shapes of the input matrices A and\n",
    "B, except for the Scenarios 1 and 3 sharing the same input structure. Those two are further\n",
    "distinguished by the shape of output, AB. The nonnegative matrix factorization involves Scenarios 1 to 5.\n",
    "Scenario 6 is for matrix-vector multiplications, where broadcasting small vectors is almost\n",
    "always efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonnegative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate a nonnegative data matrix $X \\in \\mathbb{R}^{m \\times p}$ by $VW$, $V \\in \\mathbb{R}^{m \\times r}$ and $W \\in \\mathbb{R}^{r \\times p}$. In a simple setting, NMF minimizes\n",
    "\\begin{align*}\n",
    "f(V, W) =  \\|X - VW\\|_\\mathrm{F}^2.\n",
    "\\end{align*}\n",
    "\n",
    "Multiplicative algorithm [Lee and Seung, 1999, 2001], which can be understood as a case of MM algorithm:\n",
    "\\begin{align*}\n",
    "V^{n+1} &= V^n \\odot [X (W^n)^T] \\oslash [V^n W^n (W^n)^T] \\\\\n",
    "W^{n+1} &= W^n \\odot [(V^{n+1})^T X] \\oslash [(V^{n+1})^T V^{n+1} W^n],\n",
    "\\end{align*}\n",
    "where $\\odot$ and $\\oslash$ denote elementwise multiplication and division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a simplified version. The full object-oriented version is included in our package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf(rank, size):\n",
    "    p = 8; q = 12; r = 3 # p is \"big\", q is \"big\", r is \"small\".\n",
    "    maxiter = 1000\n",
    "    TensorType=torch.DoubleTensor\n",
    "    data = distmat.distgen_uniform(p, q, TType=TensorType) # [p] x q\n",
    "    V = distmat.distgen_uniform(p, r, TType=TensorType) # [p] x r\n",
    "    W = distmat.distgen_uniform(q, r, TType=TensorType).t() # r x [q]\n",
    "    for i in range(maxiter):\n",
    "        XWt =  distmat.mm(data, W.t()) # Scenario 2, input ([p] x q), ([q] x r), output ([p] x r)\n",
    "        WWt =  distmat.mm(W, W.t()) # Scenario 3, input (r x [q]), ([q] x r), output (r x r)\n",
    "        VWWt = distmat.mm(V, WWt) # Scenario 5, input ([p] x r), (r x r), output ([p] x r)\n",
    "        V.mul_(XWt).div_(VWWt) # in-place op\n",
    "\n",
    "        VtX  = distmat.mm(V.t(), data, out_sizes=W.sizes) # Scenario 1, input (r x [p]), ([p] x q), output (r x [q])\n",
    "        VtV  = distmat.mm(V.t(), V) # Scenario 3, input (r x [p]), ([p] x r), output r x r\n",
    "        VtVW = distmat.mm(VtV, W) # Scenario 5 (transposed), input (r x r), (r x [q]), output r x [q]\n",
    "        W = W.mul_(VtX).div_(VtVW) # in-place op\n",
    "        if (i+1) % 100 == 0:\n",
    "            # print objective\n",
    "            outer = distmat.mm(V, W) # Scenario 4, input ([p] x r), (r x [q]), output [p] x q\n",
    "            val = ((data - outer)**2).sum()\n",
    "            if rank == 0:\n",
    "                print(\"Iteration {}: {}\".format(i+1, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: 2.182438128886236\n",
      "Iteration 200: 2.171416631750278\n",
      "Iteration 300: 2.169680386094355\n",
      "Iteration 400: 2.168717940886758\n",
      "Iteration 500: 2.167978637258126\n",
      "Iteration 600: 2.167353482884231\n",
      "Iteration 700: 2.1668354282699847\n",
      "Iteration 800: 2.1664281438361552\n",
      "Iteration 900: 2.1661187200995116\n",
      "Iteration 1000: 2.165885899848217\n"
     ]
    }
   ],
   "source": [
    "run_process(4, nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the package `dist_stat`, we have implemented...\n",
    "\n",
    "* Nonnegative matrix factorization:\n",
    "    * Multiplicative method\n",
    "    * Alternating proximal gradient method\n",
    "* Positron Emission Tomography:\n",
    "    * with $\\ell_2$-penalty, MM method\n",
    "    * with $\\ell_1$-penlaty, primal-dual method\n",
    "* Multidimensional Scaling\n",
    "* $\\ell_1$-regularized Cox regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_mult(rank, size):\n",
    "    import dist_stat.nmf as nmf\n",
    "    p = 8; q = 12; r = 3\n",
    "    maxiter = 3000\n",
    "    TensorType=torch.DoubleTensor\n",
    "    torch.manual_seed(100)\n",
    "    data = distmat.distgen_uniform(p, q, TType=TensorType, set_from_master=True) # to guarantee same input matrices throughout experiments\n",
    "    driver = nmf.NMF(data, r)\n",
    "    V, W = driver.run(maxiter=maxiter, tol=1e-5, check_interval=100, check_obj=True) \n",
    "    # if check_obj=False, the objective value is not estimated.\n",
    "    # the convergence is determined based on maximum change in V and W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "p=8, q=12, r=3\n",
      "  iter\t      V_maxdiff\t      W_maxdiff\t        reldiff\t            obj\t      time\n",
      "--------------------------------------------------------------------------------\n",
      "   100\t2.269086434e-04\t7.859795175e-04\t            inf\t2.450653178e+00\t   0.43956\n",
      "   200\t4.741822670e-05\t1.487613723e-04\t3.867071318e-04\t2.449319301e+00\t   0.42831\n",
      "   300\t2.064260964e-05\t7.563094246e-05\t2.621296866e-05\t2.449228887e+00\t   0.47664\n",
      "   400\t2.000570493e-05\t7.475431599e-05\t2.011233401e-05\t2.449159516e+00\t   0.39664\n",
      "   500\t2.812954891e-05\t9.523429625e-05\t3.124107173e-05\t2.449051764e+00\t   0.39500\n",
      "   600\t4.381995135e-05\t1.446528443e-04\t5.162722771e-05\t2.448873708e+00\t   0.43200\n",
      "   700\t6.619583670e-05\t2.162175833e-04\t8.868082228e-05\t2.448567886e+00\t   0.43274\n",
      "   800\t9.643257393e-05\t3.092734985e-04\t1.528160691e-04\t2.448040970e+00\t   0.43074\n",
      "   900\t1.325321757e-04\t4.134715304e-04\t2.479164259e-04\t2.447186356e+00\t   0.45170\n",
      "  1000\t1.650634082e-04\t4.965977026e-04\t3.522855042e-04\t2.445972390e+00\t   0.40009\n",
      "  1100\t1.783495447e-04\t5.148614934e-04\t4.124739364e-04\t2.444551602e+00\t   0.39521\n",
      "  1200\t1.640825205e-04\t4.541542368e-04\t3.852711367e-04\t2.443225027e+00\t   0.39688\n",
      "  1300\t1.314835115e-04\t3.496689842e-04\t2.881911715e-04\t2.442233006e+00\t   0.39607\n",
      "  1400\t9.656804741e-05\t2.477632207e-04\t1.817436148e-04\t2.441607516e+00\t   0.39439\n",
      "  1500\t6.842890808e-05\t1.702706661e-04\t1.049224902e-04\t2.441246451e+00\t   0.40291\n",
      "  1600\t4.845288571e-05\t1.175770934e-04\t5.961547715e-05\t2.441041312e+00\t   0.39642\n",
      "  1700\t3.491156633e-05\t8.303925294e-05\t3.477617383e-05\t2.440921650e+00\t   0.41044\n",
      "  1800\t2.577709669e-05\t6.035303685e-05\t2.118545696e-05\t2.440848754e+00\t   0.39643\n",
      "  1900\t1.952574334e-05\t4.515135725e-05\t1.351999715e-05\t2.440802234e+00\t   0.39840\n",
      "  2000\t1.515135760e-05\t3.469121126e-05\t9.012603356e-06\t2.440771224e+00\t   0.41280\n",
      "--------------------------------------------------------------------------------\n",
      "Completed. total time: 8.287988901138306\n"
     ]
    }
   ],
   "source": [
    "run_process(4, nmf_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternating projected gradient (APG) with ridge penalties:\n",
    "\n",
    "\\begin{align*}\n",
    "f(V, W; \\epsilon) =  \\|X - VW\\|_\\mathrm{F}^2 + \\frac{\\epsilon}{2} \\|V\\|_\\mathrm{F}^2 + \\frac{\\epsilon}{2} \\|W\\|_\\mathrm{F}^2\n",
    "\\end{align*}\n",
    "is minimized. \n",
    "\n",
    "The corresponding APG update is given by\n",
    "\\begin{align*}\n",
    "V^{n+1} &= P_+ \\left((1 - \\sigma_n \\epsilon) V^n - \\sigma_n (V^n W^n (W^n)^T - X (W^n)^T) \\right) \\\\\n",
    "W^{n+1} &= P_+ \\left((1 - \\tau_n \\epsilon) W^n - \\tau_n ((V^{n+1})^T V^{n+1} W^n - (V^{n+1})^TX ) \\right).\n",
    "\\end{align*}\n",
    "\n",
    "The below is the APG with $\\epsilon=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_apg(rank, size):\n",
    "    import dist_stat.nmf_pg as nmf\n",
    "    p = 8; q = 12; r = 3\n",
    "    maxiter = 3000\n",
    "    TensorType=torch.DoubleTensor\n",
    "    torch.manual_seed(100)\n",
    "    data = distmat.distgen_uniform(p, q, TType=TensorType, set_from_master=True)\n",
    "    driver = nmf.NMF(data, r)\n",
    "    V, W = driver.run(maxiter=maxiter, tol=1e-5, check_interval=100, check_obj=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "p=8, q=12, r=3\n",
      "  iter\t      V_maxdiff\t      W_maxdiff\t        reldiff\t            obj\t      time\n",
      "--------------------------------------------------------------------------------\n",
      "   100\t2.424162256e-03\t3.832382365e-03\t            inf\t2.491386053e+00\t   0.57124\n",
      "   200\t4.781538175e-04\t4.835312406e-04\t1.270388614e-02\t2.447588284e+00\t   0.61747\n",
      "   300\t1.557436503e-04\t1.730102196e-04\t4.787130896e-04\t2.445938668e+00\t   0.57069\n",
      "   400\t1.891190770e-04\t1.971794805e-04\t1.873943553e-04\t2.445293040e+00\t   0.60628\n",
      "   500\t1.988674663e-04\t2.022453200e-04\t1.796544182e-04\t2.444674189e+00\t   0.60988\n",
      "   600\t2.006114927e-04\t2.003183483e-04\t1.786643789e-04\t2.444058858e+00\t   0.59347\n",
      "   700\t1.974948598e-04\t1.938984347e-04\t1.720535717e-04\t2.443466398e+00\t   0.58271\n",
      "   800\t1.902900729e-04\t1.837831141e-04\t1.594379137e-04\t2.442917466e+00\t   0.61845\n",
      "   900\t1.795209595e-04\t1.706543356e-04\t1.419437735e-04\t2.442428835e+00\t   0.59964\n",
      "  1000\t1.658913058e-04\t1.553432628e-04\t1.213973120e-04\t2.442010984e+00\t   0.59792\n",
      "  1100\t1.502856314e-04\t1.387773767e-04\t9.986312239e-05\t2.441667288e+00\t   0.59055\n",
      "  1200\t1.336529807e-04\t1.218600833e-04\t7.919658999e-05\t2.441394742e+00\t   0.61170\n",
      "  1300\t1.168795022e-04\t1.053656452e-04\t6.073685599e-05\t2.441185735e+00\t   0.60851\n",
      "  1400\t1.006952236e-04\t8.987846338e-05\t4.520438315e-05\t2.441030185e+00\t   0.58848\n",
      "  1500\t8.562931901e-05\t7.577858676e-05\t3.277190496e-05\t2.440917420e+00\t   0.59483\n",
      "  1600\t7.200861323e-05\t6.326137565e-05\t2.322727525e-05\t2.440837498e+00\t   0.61387\n",
      "  1700\t5.998482412e-05\t5.237538927e-05\t1.614933164e-05\t2.440781932e+00\t   0.60628\n",
      "  1800\t4.957510521e-05\t4.306504944e-05\t1.104878262e-05\t2.440743916e+00\t   0.58538\n",
      "  1900\t4.070410505e-05\t3.520949373e-05\t7.458682630e-06\t2.440718253e+00\t   0.57506\n",
      "--------------------------------------------------------------------------------\n",
      "Completed. total time: 11.346128940582275\n"
     ]
    }
   ],
   "source": [
    "run_process(4, nmf_apg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it goes large-scale with GPU or SIMD acceleration, the time difference becomes much smaller with faster convergence. APG also avoids numerical underflow that may happen in multiplicative algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$-regularized Cox Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maximize\n",
    "$$\n",
    "f(\\beta) = L(\\beta) - \\lambda \\|\\beta\\|_1,\n",
    "$$\n",
    "where $L(\\beta)$ is the Log-partial likelihood of Cox proportional hazards model:\n",
    "\\begin{align*}\n",
    "L (\\beta) = \\sum_{i=1}^m \\delta_i \\left[\\beta^T x_i - \\log \\left(\\sum_{j: y_j \\ge y_i} \\exp(\\beta^T x_j)\\right)\\right]. \n",
    "\\end{align*}\n",
    "\n",
    "* $y_i = \\min \\{t_i, c_i\\}$\n",
    "    * $t_i$: time to event\n",
    "    * $c_i$: right-censoring time for that sample\n",
    "* $\\delta = (\\delta_1, \\dotsc, \\delta_m)^T$\n",
    "    * $\\delta_i= I_{\\{t_i \\le c_i\\}}$: indicator for censoredness of sample $i$.  \n",
    "\n",
    "The gradient of $L(\\beta)$ is given by \n",
    "\\begin{align*}\n",
    "\\nabla L(\\beta) = X^T (I-P) \\delta,\n",
    "\\end{align*} \n",
    "  \n",
    "where $w_i = \\exp(x_i^T \\beta)$, $W_j = \\sum_{i: y_i \\ge y_j} w_i$, $P = (\\pi_{ij})$, and \n",
    "$$\n",
    "\\pi_{ij} = I(y_i \\ge y_j) w_i/W_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the proximal gradient method:\n",
    "\n",
    "\\begin{align*}\n",
    "w_i^{n+1} &= \\exp(x_i^T \\beta); \\;\\; W_j^{n+1} = \\sum_{i: y_i \\ge y_j} w_i^{n+1}\\\\\n",
    "\\pi_{ij}^{n+1} &= I(t_i \\ge t_j) w_i^{n+1} / W_j^{n+1} \\\\\n",
    "\\Delta^{n+1} &= X^T (I - P^{n+1}) \\delta, \\;\\; \\text{where $P^{n+1} = (\\pi_{ij}^{n+1})$} \\\\\n",
    "\\beta^{n+1} &= \\mathcal{S}_{\\lambda}(\\beta^n + \\sigma \\Delta^{n+1}),\n",
    "\\end{align*}\n",
    "\n",
    "* $\\mathcal{S}_\\lambda(\\cdot)$ is the soft-thresholding operator, the proximity operator of $\\lambda \\|\\cdot \\|_1$. \n",
    "$$\n",
    "  [\\mathcal{S}_{\\lambda}(u)]_i = \\mathrm{sign}(u_i) (|u_i| - \\lambda)_+.\n",
    "$$\n",
    "* Convergence guaranteed when $\\sigma \\le 1/(2 \\|X\\|_2^2)$.\n",
    "* $W_j$ can be computed using `cumsum` function when the data are sorted in nonincreasing order of $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cox_l1(rank, size):\n",
    "    import dist_stat.cox as cox\n",
    "    n = 8; p = 12\n",
    "    lambd = 0.001\n",
    "    maxiter = 3000\n",
    "    torch.manual_seed(100)\n",
    "    TensorType = torch.DoubleTensor\n",
    "    # The below is how one would create a column-major matrix.\n",
    "    # We want column-major matrix to invoke matrix multiplication scenario 3. (`beta` is distributed.)\n",
    "    X = distmat.distgen_normal(p, n, TType=TensorType, set_from_master=True).t()     \n",
    "    torch.manual_seed(200)\n",
    "    delta = torch.multinomial(torch.tensor([1., 1.]), n, replacement=True).float().view(-1, 1).type(TensorType) # 50% censored, 50% noncensored\n",
    "    dist.broadcast(delta, 0) # same delta shared across processes\n",
    "    cox_driver = cox.COX(X, delta, lambd, seed=300, TType=TensorType, sigma='power') # power iteration to estimate matrix norm\n",
    "    beta = cox_driver.run(maxiter, tol=1e-5, check_interval=100, check_obj=True)\n",
    "    zeros = (beta == 0).type(torch.int64).sum() # elementwise equality (resulting in uint8 type) casted to int64 then summed up. \n",
    "                                                                    # omitting the casting will cause overflow on high-dimensional data.\n",
    "    if rank == 0:\n",
    "        print(\"number of zeros:\", zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing max singular value...\n",
      "iteration 0\n",
      "done computing max singular value:  tensor(5.5130, dtype=torch.float64)\n",
      "step size:  0.016451195454478245\n",
      "Starting...\n",
      "n=8, p=12\n",
      "  iter\t        maxdiff\t        reldiff\t            obj\t      time\n",
      "--------------------------------------------------------------------------------\n",
      "   100\t4.263906103e-03\t            inf\t-6.928162346e-01\t   0.35777\n",
      "   200\t2.137461323e-03\t1.988562069e-03\t-4.120260836e-01\t   0.37372\n",
      "   300\t1.407481825e-03\t7.553056657e-04\t-3.128646711e-01\t   0.34536\n",
      "   400\t1.007443583e-03\t3.915195766e-04\t-2.634000845e-01\t   0.33774\n",
      "   500\t7.746264875e-04\t2.342474319e-04\t-2.344826456e-01\t   0.33509\n",
      "   600\t6.264630031e-04\t1.527405316e-04\t-2.159107600e-01\t   0.33607\n",
      "   700\t5.266433517e-04\t1.052134521e-04\t-2.032509415e-01\t   0.33670\n",
      "   800\t4.567976771e-04\t7.524416395e-05\t-1.942647959e-01\t   0.34043\n",
      "   900\t4.066494457e-04\t5.527596186e-05\t-1.876996717e-01\t   0.33610\n",
      "  1000\t3.700033916e-04\t4.142489420e-05\t-1.827999355e-01\t   0.33719\n",
      "  1100\t3.429047528e-04\t3.152303512e-05\t-1.790831076e-01\t   0.33860\n",
      "  1200\t2.903550321e-04\t2.943983961e-05\t-1.756220950e-01\t   0.33951\n",
      "  1300\t2.754370962e-04\t2.312241598e-05\t-1.729100437e-01\t   0.33692\n",
      "  1400\t2.627810270e-04\t1.848812238e-05\t-1.707455549e-01\t   0.34392\n",
      "  1500\t2.498584959e-04\t1.492477462e-05\t-1.690008475e-01\t   0.33812\n",
      "  1600\t2.418480673e-04\t1.209101955e-05\t-1.675891132e-01\t   0.33861\n",
      "  1700\t2.457311543e-04\t9.832383412e-06\t-1.664422225e-01\t   0.33639\n",
      "--------------------------------------------------------------------------------\n",
      "Completed. total time: 5.811282634735107\n",
      "number of zeros: tensor(5)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, cox_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate 10,000 x 10,000 examples on 2-8 GPUs on our server. The scripts in the `examples` directory are designed to automatically select the GPU device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure can also be utilized on multi-node clusters. The structure was used for the analysis of 200,000 x 500,000 UK Biobank data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI-only, lightweight, more flexible version in Julia is in preparation. CUDA-aware MPI support for the central MPI interface [MPI.jl](https://github.com/JuliaParallel/MPI.jl) was added in the process."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "pytorch-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

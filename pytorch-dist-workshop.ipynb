{"cells":[{"metadata":{"slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"# A Distributed Matrix Data Structure and Its Statistical Applications on PyTorch"},{"metadata":{"slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"**The Programming Workshop at the Inaugural Kenneth Lange Symposium, Feb 21-22, 2020**\n\n_Seyoon Ko and Joong-Ho (Johann) Won_"},{"metadata":{"slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"## Synopsis"},{"metadata":{"slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"We developed a distributed matrix operation package suitable for distributed matrix-vector operations and distributed tall-and-thin (or wide-and-short) matrices.\nThe code runs on both multi-node machines and multi-GPU machines using PyTorch.\nWe have applied this package for four statistical applications, namely, nonnegative matrix factorization (NMF), multidimensional scaling (MDS), positron emission tomography (PET), and $\\ell_1$-regularized Cox regression.\nIn particular, $200,000 \\times 500,000$ $\\ell_1$-regularized Cox regression with the UK Biobank dataset was the biggest joint multivariate survival analysis to our knowledge. \nIn this workshop, we provide small examples that run on a single node, and demonstrate multi-GPU usage on our machine."},{"metadata":{"slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"## Contents\n\n* Brief introduction to PyTorch operations\n* `torch.distributed` package\n* Distributed matrix data structure in package `dist_stat`\n* Applications: Nonnegative matrix factorization and $\\ell_1$-penalized Cox regression\n* Demonstration on multi-GPU machine"},{"metadata":{"slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"## Introduction to PyTorch"},{"metadata":{"slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. It has two goals of development:\n* A replacement for NumPy to use the power of GPUs $\\rightarrow$ optimization of numerical operations\n* A deep learning research platform that provides maximum flexibility and speed $\\rightarrow$ optimization of automatic gradient computation e.g. backpropagation\n\nWe are trying to exploit the former in a distributed environment."},{"metadata":{"slideshow":{"slide_type":"-"}},"cell_type":"markdown","source":"## Basic PyTorch Operations\nWe introduce simple operations on PyTorch. Note that Python uses 0-based, row-major ordering, like C and C++ (cf. R and Julia have 1-based, column-major ordering). First we import the PyTorch\nlibrary. This is similar to `library()` in R and equivalent to `import ...` in Julia."},{"metadata":{"trusted":false},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\",category=UserWarning) # hide `UserWarning`s","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"-"},"trusted":false},"cell_type":"code","source":"import torch","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"-"},"trusted":false},"cell_type":"code","source":"torch.__version__","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tensor Creation"},{"metadata":{},"cell_type":"markdown","source":"One may create an uninitialized tensor. This creates a 3 × 4 tensor (matrix)."},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.empty(3, 4) # uninitialized tensor. Julia equivalent: Array{Float32}(undef, 3, 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following is equivalent to `set.seed()` in R."},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.manual_seed(100) # Julia equivalent: Random.seed!(100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This generates a tensor initialized with random values from (0, 1)."},{"metadata":{"trusted":false},"cell_type":"code","source":"y = torch.rand(3, 4) # from Unif(0, 1). \ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also generate a tensor filled with zeros or ones."},{"metadata":{"trusted":false},"cell_type":"code","source":"z = torch.ones(3, 4) # torch.zeros(3, 4)\nz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A tensor can be created from standard Python data."},{"metadata":{"trusted":false},"cell_type":"code","source":"w = torch.tensor([3, 4, 5, 6])\nw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A tensor can be created in certain datatype (default: float32) and on certain device (default: CPU) of choice "},{"metadata":{"trusted":false},"cell_type":"code","source":"# double precision\nw = torch.tensor([3, 4, 5, 6], dtype=torch.float64)\nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# # on GPU number zero. will not run if CUDA GPU is not present.\n# w = torch.tensor([3, 4, 5, 6], device='cuda:0')\n# w","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shape of a tensor can be accessed by appending `.shape` to the tensor name."},{"metadata":{"trusted":false},"cell_type":"code","source":"z.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Casting"},{"metadata":{},"cell_type":"markdown","source":"A tensor can have datatype and location changed by the method `.to()`. The arguments are similar to choosing datatype and device of the new tensor."},{"metadata":{"trusted":false},"cell_type":"code","source":"w = w.to(device = \"cpu\", dtype=torch.int32)\nw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Indexing"},{"metadata":{},"cell_type":"markdown","source":"The following are standard method of indexing tensors."},{"metadata":{"trusted":false},"cell_type":"code","source":"y[2, 3] # indexing: zero-based, returns a 0-dimensional tensor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The indexing always returns a (sub)tensor, even for scalars (treated as zero-dimensional tensors).\nA standard Python number can be returned by using .item()."},{"metadata":{"trusted":false},"cell_type":"code","source":"y[2, 3].item() # A standard Python floating-point number","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get a column from a tensor, we use the indexing as below. The syntax is similar but slightly\ndifferent from R."},{"metadata":{"trusted":false},"cell_type":"code","source":"y[:, 3] # 3rd column. The leftmost column is 0th. cf. y[, 4] in R","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following is for taking a row."},{"metadata":{"trusted":false},"cell_type":"code","source":"y[2, :] # 2nd row. The top row is 0th. cf. y[3, ] in R","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Simple operations"},{"metadata":{},"cell_type":"markdown","source":"Here we provide an example of simple operations on PyTorch. Addition using the operator ‘+’ acts\njust like anyone can expect:"},{"metadata":{"trusted":false},"cell_type":"code","source":"x = y + z # a simple addition.\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is another form of addition."},{"metadata":{"trusted":false},"cell_type":"code","source":"x = torch.add(y, z) # another syntax for addition","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The operators ending with an underscore (`_`) changes the value of the tensor in-place. Otherwise, the argument never changes. Unlike methods ending with `!` in Julia, this rule is strictly enforced in PyTorch. (The underscore determines usage of the keyword `const` in C++-level.)"},{"metadata":{"trusted":false},"cell_type":"code","source":"y.add_(z) # in-place addition","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Concatenation"},{"metadata":{},"cell_type":"markdown","source":"We can concatenate the tensors using the function `cat()`, which resembles `c()`, `cbind()`, and\n`rbind()` in R, `cat()`, `vcat()`, `hcat()` in Julia. The second argument indicates the dimension that the tesors are concatenated\nalong: zero means by concatenation by rows, and one means by columns."},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.cat((y, z), 0) # along the rows. cf. vcat","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.cat((y, z), 1) # along the columns. cf. hcat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summation/reduction"},{"metadata":{},"cell_type":"markdown","source":"Calling `.sum()`, `.prod()`, `mean()` methods of a tensor do the obvious. Optional argument determines the dimension of reduction."},{"metadata":{"trusted":false},"cell_type":"code","source":"y","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y.sum(0) # reduces rows, columnwise sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y.sum(1) # reduces columns, rowwise sum","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y.sum((0, 1)) # reduces rows and columns -> a single number.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Algebra"},{"metadata":{},"cell_type":"markdown","source":"Matrix transpose is performed by appending `.t()` to a tensor. Matrix multiplication is carried out by the method `torch.mm()`."},{"metadata":{"trusted":false},"cell_type":"code","source":"torch.mm(y, z.t()) # Note: y is 3 x 4, z is 3 x 4. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## `torch.distributed`: Distributed subpackage for PyTorch"},{"metadata":{},"cell_type":"markdown","source":"`torch.distributed` is the subpackage for distributed operations on PyTorch. The interface is mostly inspired by the message passing interface (MPI). The available backends are:\n\n* Gloo, a collective communication library developed by Facebook, included in PyTorch. Full support for CPU, partial collective communication only for GPU.\n* MPI, a good-old communication standard. The most flexible, but PyTorch needs to be compiled from its source to use it as a backend. Full support for GPU if the MPI installation is \"CUDA-aware\".\n* NCCL, Nvidia Collective Communications Library, collective communication only for multiple GPUs on the same machine."},{"metadata":{},"cell_type":"markdown","source":"For this workshop, we use Gloo for its full functionalities on CPU and runnability on Jupyter Notebook. The experiments in our paper use MPI for running multi-node setting and multi-GPU setting with basically the same code. The interface below is specific for Gloo backend. For MPI backend, please consult with a section from [distributed package tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends) or [our example code](https://github.com/kose-y/dist_stat/tree/master/examples)."},{"metadata":{"trusted":false},"cell_type":"code","source":"import os\nimport torch\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\n\ndef init_process(rank, size, fn, backend='gloo'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\ndef run_process(size, fn):\n    processes = []\n    for rank in range(size):\n        p = Process(target=init_process, args=(rank, size, fn))\n        p.start()\n        processes.append(p)\n        \n    for p in processes:\n        p.join()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each distributed function from now on will have `rank` and `size` as the two arguments. When `run_process` is called with the communicator size `size` and the function name `fn`, `size` process will be launched, and each of them will call `fn` with `rank` of each process and the `size`.  "},{"metadata":{},"cell_type":"markdown","source":"### Point-to-point communication"},{"metadata":{},"cell_type":"markdown","source":"![](https://pytorch.org/tutorials/_images/send_recv.png)\n\nFigure courtesy of: https://pytorch.org/tutorials/_images/send_recv.png"},{"metadata":{"trusted":false},"cell_type":"code","source":"\"\"\"Blocking point-to-point communication.\"\"\"\n\ndef point_to_point(rank, size):\n    tensor = torch.zeros(1)\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        dist.send(tensor=tensor, dst=1)\n    elif rank == 1:\n        # Receive tensor from process 0\n        dist.recv(tensor=tensor, src=0)\n    dist.barrier()\n    print('Rank ', rank, ' has data ', tensor[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, point_to_point)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Collective communication"},{"metadata":{},"cell_type":"markdown","source":"| | | \n|:---|:---|\n| ![](https://pytorch.org/tutorials/_images/scatter.png) | ![](https://pytorch.org/tutorials/_images/gather.png) |\n| Scatter | Gather |\n| ![](https://pytorch.org/tutorials/_images/reduce.png) | ![](https://pytorch.org/tutorials/_images/all_reduce.png) |\n| Reduce | All-reduce |\n| ![](https://pytorch.org/tutorials/_images/broadcast.png) | ![](https://pytorch.org/tutorials/_images/all_gather.png) |\n| Broadcast | All-gather |\n\nTable courtesy of: https://pytorch.org/tutorials/intermediate/dist_tuto.html\n"},{"metadata":{},"cell_type":"markdown","source":"The below is the code for simple Monte Carlo $\\pi$ estimation. 100,000 $x_i$ and $y_i$ are sampled from $Unif(0,1)$ for each process, and we measure the proportion of $(x_i, y_i)$s inside the unit quartercircle. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def mc_pi(n, size):\n    # this code is executed on each process.\n    x = torch.rand((n), dtype=torch.float64)\n    y = torch.rand((n), dtype=torch.float64)\n    # compute local estimate of pi\n    r = torch.mean((x**2 + y**2 < 1).to(dtype=torch.float64))*4\n    dist.all_reduce(r) # sum of 'r's in each device is stored in 'r'\n    return r / size\n\ndef run_mc_pi(rank, size):\n    n = 100000\n    torch.manual_seed(100 + rank)\n    r = mc_pi(n, size)\n    if rank == 0:\n        print(r.item())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, run_mc_pi)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## `distmat`: Distributed Matrices on PyTorch"},{"metadata":{},"cell_type":"markdown","source":"Using the tensor operations and communication package, we created a data structure for a distributed matrix. \n* Each process (enumerated by rank) holds a contiguous block of the full data matrix by rows or columns.\n* The data may be a sparse matrix. \n* If GPUs are involved, each process controls a GPU whose index matches the process rank. \n* From now on: [100] × 100 matrix split over four processes means...\n    * Rank 0 process keeps rows [0:25). of the matrix, in row-major ordering.\n    * Rank 3 process keeps rows [75:100).\n* Limitation: length of distributed dimension should be divisible by number of processes."},{"metadata":{"trusted":false},"cell_type":"code","source":"import dist_stat.distmat as distmat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creation"},{"metadata":{},"cell_type":"markdown","source":"- `distgen_ones()`: Creates a distributed matrix filled with ones\n- `distgen_zeros()`: Creates a distributed matrix filled with zeros\n- `distgen_uniform()`: Creates a distributed matrix from uniform distribution\n- `distgen_normal()`: Creates a distributed matrix from stndard normal distribution "},{"metadata":{"trusted":false},"cell_type":"code","source":"def create_unif(rank, size):\n    A = distmat.distgen_uniform(8, 8, TType=torch.DoubleTensor) # the default is row-distributed, row-major ordering data. \n    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, create_unif)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A distributed matrix can also be created from local chunks."},{"metadata":{"trusted":false},"cell_type":"code","source":"def from_chunks(rank, size):\n    torch.manual_seed(100 + rank)\n    chunk = torch.randn(2, 4)\n    print(\"rank \", rank, \"has chunk\", chunk)\n    A = distmat.THDistMat.from_chunks(chunk)\n    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)    \n    if rank == 0:\n        print(A.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, from_chunks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data can be distributed from a master process."},{"metadata":{"trusted":false},"cell_type":"code","source":"def dist_data(rank, size):\n    if rank == 0:\n        data = torch.rand(4, 2)\n        print(\"master data: \", data)\n    else:\n        data = None\n    \n    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n    print('data_dist: ', 'Rank ', rank, ' has data ', data_dist.chunk)\n    dist.barrier()\n    print(data_dist.shape) # shape of the distributed matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, dist_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remark: The default is to create a row-major matrix. They can easily be changed to a column-major matrix by transposition."},{"metadata":{},"cell_type":"markdown","source":"### Elementwise operations"},{"metadata":{},"cell_type":"markdown","source":"Some of the basic functions work naturally. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def elemwise_1(rank, size):\n    A = distmat.distgen_uniform(4, 2)\n    dist.barrier() # dist.barrier() waits until all other processes reach the same line. It is used to make the output easier to read. \n    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n    B = distmat.distgen_uniform(4, 2)\n    dist.barrier()\n    print(\"B: rank \", rank, \"has chunk\", B.chunk)    \n    C = A + B\n    dist.barrier()\n    print(\"C: rank \", rank, \"has chunk\", C.chunk)    \n    C.add_(A) # functionally equivalent to C += A\n    dist.barrier()\n    print(\"C: rank \", rank, \"has chunk\", C.chunk)\n    \n    logC = C.log()\n    dist.barrier()\n    print(\"logC: rank \", rank, \"has chunk\", logC.chunk) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, elemwise_1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Broadcasting (similar to Julia's dot operation) also works as expected:"},{"metadata":{"trusted":false},"cell_type":"code","source":"def dim_broadcasting(rank, size):\n    A = distmat.distgen_uniform(4, 2) # [4] x 2 \n    dist.barrier()\n    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n    B = distmat.distgen_ones(4, 1) # [4] x 1\n    dist.barrier()\n    print(\"B: rank \", rank, \"has chunk\", B.chunk)\n    A.add_(B) # B treated as [4] x 2 matrix\n    dist.barrier()\n    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n    C = 2 * torch.ones(1, 2, dtype=torch.float64) # 1 x 2\n    A.add_(C) # C treated as [4] x 2 matrix\n    dist.barrier()\n    print(\"A: rank \", rank, \"has chunk\", A.chunk)     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, dim_broadcasting)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For general functions, we have `.apply()` and `.apply_binary()`."},{"metadata":{"trusted":false},"cell_type":"code","source":"def elemwise_2(rank, size):\n    A = distmat.distgen_uniform(4, 2)\n    dist.barrier()\n    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n    B = distmat.distgen_uniform(4, 2)\n    dist.barrier()\n    print(\"B: rank \", rank, \"has chunk\", B.chunk) \n    Asqp1 = A.apply(lambda x: x**2 + 1) # anonymous function definition: x -> x .^ 2 .+ 1 in Julia\n    dist.barrier()\n    print(\"Asqp1: rank \", rank, \"has chunk\", Asqp1.chunk)    \n    AsqpBsq = A.apply_binary(B, lambda x, y: x**2 + y**2) # anonymous function definition: (x, y) -> x.^2 .+ y .^2 in Julia\n    dist.barrier()\n    print(\"AsqpBsq: rank \", rank, \"has chunk\", AsqpBsq.chunk) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, elemwise_2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reductions (sum, max, min)"},{"metadata":{},"cell_type":"markdown","source":"Summations, minimums, and maximums can be carried out in a way similar to local tensors."},{"metadata":{"trusted":false},"cell_type":"code","source":"def reductions(rank, size):\n    A = distmat.distgen_uniform(4, 2)\n    dist.barrier()\n    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n    dist.barrier()\n    print(\"sum of A: \", A.sum())\n    dist.barrier()\n    print(\"maximum of A: \", A.max())\n    dist.barrier()\n    print(\"minimum of A: \", A.min())\n    \n    sumA_row = A.sum(0) # row sum, output: a tensor with the same values on all processes \n    sumA_col = A.sum(1) # col sum, output: a distributed matrix\n    dist.barrier()\n    print(\"row sum of A: \", sumA_row)\n    dist.barrier()\n    print(\"sumA_col: rank \", rank, \"has chunk\", sumA_col.chunk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, reductions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Diagonals"},{"metadata":{"trusted":false},"cell_type":"code","source":"def diagonals(rank, size):\n    if rank == 0:\n        p = 4\n        data = torch.randn(p, p)\n        print(\"master data: \", data)\n    else:\n        data = None\n        \n    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n    \n    diag1 = data_dist.diag() # distributed diagonal\n    print(\"diag1: rank \", rank, \"has chunk\", diag1.chunk)\n    \n    diag2 = data_dist.diag(distribute=False) # diagonal gathered for each process\n    print(\"diag2: \", diag2)\n    \n    data_dist.fill_diag_(0) # fill the diagonals with zeros\n    print(\"data_dist: rank \", rank, \"has chunk\", data_dist.chunk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, diagonals)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matrix multiplications"},{"metadata":{},"cell_type":"markdown","source":"Six different scenarios of matrix-matrix multiplications, each representing a different configuration of the split dimension of two input\nmatrices and the output matrix, were considered and implemented. \n\n| Secnario | $A$ | $B$ | $AB$ | Description | Usage |\n|:---|:---|:---|:---|:---|:---|\n| 1 | $r \\times$ [$p$] | $[p] \\times q$ | $r \\times$ [$q$]| Inner product, result distributed. | $V^T X$ |\n| 2 | $[p] \\times q$ | $[q] \\times r$ | $[p] \\times r$ | Fat matrix multiplied by a thin and tall matrix. | $X W^T$ |\n| 3 | $r \\times$ [$p$] | $[p] \\times s$ | $r \\times s$   | Inner product, result broadcasted. Inner product between two thin matrices. | $V^T V$, $W W^T$ |                                                                           \n| 4 | $[p] \\times r$ | $r \\times$ [$q$] | $[p] \\times q$ | Outer product, may require large amount of memory. For computing objective function. | $VW$ |\n| 5 | $[p] \\times r$ | $r \\times s$   | $[p] \\times s$ | A distributed matrix multiplied by a small, distributed matrix. | $VC$ where $C = WW^T$; $CW$ where $C = V^T V$ (transposed) |\n| 6 | $r \\times$ [$p$] | $p \\times s$   | $r \\times s$   | A distributed matrix multiplied by a thin-and-tall broadcasted matrix. | Matrix-broadcasted vector multiplications. |\n\n\n\nThe implementation of each case is carried\nout using the collective communication directives. Matrix multiplication scenarios are automatically selected based on the shapes of the input matrices A and\nB, except for the Scenarios 1 and 3 sharing the same input structure. Those two are further\ndistinguished by the shape of output, AB. The nonnegative matrix factorization involves Scenarios 1 to 5.\nScenario 6 is for matrix-vector multiplications, where broadcasting small vectors is almost\nalways efficient."},{"metadata":{},"cell_type":"markdown","source":"## Nonnegative Matrix Factorization (NMF)"},{"metadata":{},"cell_type":"markdown","source":"Approximate a nonnegative data matrix $X \\in \\mathbb{R}^{m \\times p}$ by $VW$, $V \\in \\mathbb{R}^{m \\times r}$ and $W \\in \\mathbb{R}^{r \\times p}$. In a simple setting, NMF minimizes\n\\begin{align*}\nf(V, W) =  \\|X - VW\\|_\\mathrm{F}^2.\n\\end{align*}\n\nMultiplicative algorithm [Lee and Seung, 1999, 2001], which can be understood as a case of MM algorithm:\n\\begin{align*}\nV^{n+1} &= V^n \\odot [X (W^n)^T] \\oslash [V^n W^n (W^n)^T] \\\\\nW^{n+1} &= W^n \\odot [(V^{n+1})^T X] \\oslash [(V^{n+1})^T V^{n+1} W^n],\n\\end{align*}\nwhere $\\odot$ and $\\oslash$ denote elementwise multiplication and division."},{"metadata":{},"cell_type":"markdown","source":"The following code is a simplified version. The full object-oriented version is included in our package."},{"metadata":{"trusted":false},"cell_type":"code","source":"def nmf(rank, size):\n    p = 8; q = 12; r = 3 # p is \"big\", q is \"big\", r is \"small\".\n    maxiter = 1000\n    TensorType=torch.DoubleTensor\n    data = distmat.distgen_uniform(p, q, TType=TensorType) # [p] x q\n    V = distmat.distgen_uniform(p, r, TType=TensorType) # [p] x r\n    W = distmat.distgen_uniform(q, r, TType=TensorType).t() # r x [q]\n    for i in range(maxiter):\n        XWt =  distmat.mm(data, W.t()) # Scenario 2, input ([p] x q), ([q] x r), output ([p] x r)\n        WWt =  distmat.mm(W, W.t()) # Scenario 3, input (r x [q]), ([q] x r), output (r x r)\n        VWWt = distmat.mm(V, WWt) # Scenario 5, input ([p] x r), (r x r), output ([p] x r)\n        V.mul_(XWt).div_(VWWt) # in-place op\n\n        VtX  = distmat.mm(V.t(), data, out_sizes=W.sizes) # Scenario 1, input (r x [p]), ([p] x q), output (r x [q])\n        VtV  = distmat.mm(V.t(), V) # Scenario 3, input (r x [p]), ([p] x r), output r x r\n        VtVW = distmat.mm(VtV, W) # Scenario 5 (transposed), input (r x r), (r x [q]), output r x [q]\n        W = W.mul_(VtX).div_(VtVW) # in-place op\n        if (i+1) % 100 == 0:\n            # print objective\n            outer = distmat.mm(V, W) # Scenario 4, input ([p] x r), (r x [q]), output [p] x q\n            val = ((data - outer)**2).sum()\n            if rank == 0:\n                print(\"Iteration {}: {}\".format(i+1, val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, nmf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inside the package `dist_stat`, we have implemented...\n\n* Nonnegative matrix factorization:\n    * Multiplicative method\n    * Alternating proximal gradient method\n* Positron Emission Tomography:\n    * with $\\ell_2$-penalty, MM method\n    * with $\\ell_1$-penlaty, primal-dual method\n* Multidimensional Scaling\n* $\\ell_1$-regularized Cox regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"def nmf_mult(rank, size):\n    import dist_stat.nmf as nmf\n    p = 8; q = 12; r = 3\n    maxiter = 3000\n    TensorType=torch.DoubleTensor\n    torch.manual_seed(100)\n    data = distmat.distgen_uniform(p, q, TType=TensorType, set_from_master=True) # to guarantee same input matrices throughout experiments\n    nmf_driver = nmf.NMF(data, r)\n    V, W = nmf_driver.run(maxiter=maxiter, tol=1e-5, check_interval=100, check_obj=True) \n    # if check_obj=False, the objective value is not estimated.\n    # the convergence is determined based on maximum change in V and W.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, nmf_mult)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alternating projected gradient (APG) with ridge penalties:\n\n\\begin{align*}\nf(V, W; \\epsilon) =  \\|X - VW\\|_\\mathrm{F}^2 + \\frac{\\epsilon}{2} \\|V\\|_\\mathrm{F}^2 + \\frac{\\epsilon}{2} \\|W\\|_\\mathrm{F}^2\n\\end{align*}\nis minimized. \n\nThe corresponding APG update is given by\n\\begin{align*}\nV^{n+1} &= P_+ \\left((1 - \\sigma_n \\epsilon) V^n - \\sigma_n (V^n W^n (W^n)^T - X (W^n)^T) \\right) \\\\\nW^{n+1} &= P_+ \\left((1 - \\tau_n \\epsilon) W^n - \\tau_n ((V^{n+1})^T V^{n+1} W^n - (V^{n+1})^TX ) \\right).\n\\end{align*}\n\nThe below is the APG with $\\epsilon=0$."},{"metadata":{"trusted":false},"cell_type":"code","source":"def nmf_apg(rank, size):\n    import dist_stat.nmf_pg as nmf\n    p = 8; q = 12; r = 3\n    maxiter = 3000\n    TensorType=torch.DoubleTensor\n    torch.manual_seed(100)\n    data = distmat.distgen_uniform(p, q, TType=TensorType, set_from_master=True)\n    nmf_driver = nmf.NMF(data, r)\n    V, W = nmf_driver.run(maxiter=maxiter, tol=1e-5, check_interval=100, check_obj=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, nmf_apg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When it goes large-scale with GPU or SIMD acceleration, the time difference becomes much smaller with faster convergence. APG also avoids numerical underflow that may happen in multiplicative algorithms. "},{"metadata":{},"cell_type":"markdown","source":"## $\\ell_1$-regularized Cox Regression"},{"metadata":{},"cell_type":"markdown","source":"We maximize\n$$\nf(\\beta) = L(\\beta) - \\lambda \\|\\beta\\|_1,\n$$\nwhere $L(\\beta)$ is the Log-partial likelihood of Cox proportional hazards model:\n\\begin{align*}\nL (\\beta) = \\sum_{i=1}^m \\delta_i \\left[\\beta^T x_i - \\log \\left(\\sum_{j: y_j \\ge y_i} \\exp(\\beta^T x_j)\\right)\\right]. \n\\end{align*}\n\n* $y_i = \\min \\{t_i, c_i\\}$\n    * $t_i$: time to event\n    * $c_i$: right-censoring time for that sample\n* $\\delta = (\\delta_1, \\dotsc, \\delta_m)^T$\n    * $\\delta_i= I_{\\{t_i \\le c_i\\}}$: indicator for censoredness of sample $i$.  \n\nThe gradient of $L(\\beta)$ is given by \n\\begin{align*}\n\\nabla L(\\beta) = X^T (I-P) \\delta,\n\\end{align*} \n  \nwhere $w_i = \\exp(x_i^T \\beta)$, $W_j = \\sum_{i: y_i \\ge y_j} w_i$, $P = (\\pi_{ij})$, and \n$$\n\\pi_{ij} = I(y_i \\ge y_j) w_i/W_j.\n$$"},{"metadata":{},"cell_type":"markdown","source":"We use the proximal gradient method:\n\n\\begin{align*}\nw_i^{n+1} &= \\exp(x_i^T \\beta); \\;\\; W_j^{n+1} = \\sum_{i: y_i \\ge y_j} w_i^{n+1}\\\\\n\\pi_{ij}^{n+1} &= I(t_i \\ge t_j) w_i^{n+1} / W_j^{n+1} \\\\\n\\Delta^{n+1} &= X^T (I - P^{n+1}) \\delta, \\;\\; \\text{where $P^{n+1} = (\\pi_{ij}^{n+1})$} \\\\\n\\beta^{n+1} &= \\mathcal{S}_{\\lambda}(\\beta^n + \\sigma \\Delta^{n+1}),\n\\end{align*}\n\n* $\\mathcal{S}_\\lambda(\\cdot)$ is the soft-thresholding operator, the proximity operator of $\\lambda \\|\\cdot \\|_1$. \n$$\n  [\\mathcal{S}_{\\lambda}(u)]_i = \\mathrm{sign}(u_i) (|u_i| - \\lambda)_+.\n$$\n* Convergence guaranteed when $\\sigma \\le 1/(2 \\|X\\|_2^2)$.\n* $W_j$ can be computed using `cumsum` function when the data are sorted in nonincreasing order of $y_i$."},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":false},"cell_type":"code","source":"def cox_l1(rank, size):\n    import dist_stat.cox_breslow as cox\n    n = 8; p = 12\n    lambd = 0.001\n    maxiter = 3000\n    torch.manual_seed(100)\n    TensorType = torch.DoubleTensor\n    # The below is how one would create a column-major matrix.\n    # We want column-major matrix to invoke matrix multiplication scenario 3. (`beta` is distributed.)\n    X = distmat.distgen_normal(p, n, TType=TensorType, set_from_master=True).t()     \n    torch.manual_seed(200)\n    delta = torch.multinomial(torch.tensor([1., 1.]), n, replacement=True).float().view(-1, 1).type(TensorType) # 50% censored, 50% noncensored\n    dist.broadcast(delta, 0) # same delta shared across processes\n    t = torch.arange(n, 0, -1) # Just assuming decreasing time\n    cox_driver = cox.COX(X, delta, lambd, time=t, seed=300, TType=TensorType, sigma='power') # power iteration to estimate matrix norm\n    beta = cox_driver.run(maxiter, tol=1e-5, check_interval=100, check_obj=True)\n    zeros = (beta == 0).type(torch.int64).sum() # elementwise equality (resulting in uint8 type) casted to int64 then summed up. \n                                                                    # omitting the casting will cause overflow on high-dimensional data.\n    if rank == 0:\n        print(\"number of zeros:\", zeros)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"run_process(4, cox_l1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multi-GPU Demonstration"},{"metadata":{},"cell_type":"markdown","source":"We demonstrate 10,000 x 10,000 examples on 2-8 GPUs on our server. The scripts in the `examples` directory are designed to automatically select the GPU device."},{"metadata":{},"cell_type":"markdown","source":"## Multi-node"},{"metadata":{},"cell_type":"markdown","source":"The data structure can also be utilized on multi-node clusters. The structure was used for the analysis of 200,000 x 500,000 UK Biobank data with 20 c5.18xlarge instances (36 physical cores, 144GB memory each)."},{"metadata":{},"cell_type":"markdown","source":"## Future Direction"},{"metadata":{},"cell_type":"markdown","source":"MPI-only, lightweight, more flexible version in Julia is in preparation. CUDA-aware MPI support for the central MPI interface [MPI.jl](https://github.com/JuliaParallel/MPI.jl) was added in the process. "}],"metadata":{"kernelspec":{"name":"conda-env-notebook-py","display_name":"Python [conda env:notebook] *","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}
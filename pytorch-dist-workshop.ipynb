{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Distributed Matrix Data Structure and Its Statistical Applications on PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is incomplete. At this time, it is uploaded for testing purposes, in preparation of tutorials to be presented in the programming workshop at the inaugural Lange's Symposium on Feb 21-22, 2020.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed a distributed matrix operation package suitable for distributed matrix-vector operations and distributed tall-and-thin (or wide-and-short) matrices.\n",
    "The code runs on both multi-node machines and multi-GPU machines using PyTorch.\n",
    "We apply this package for four statistical applications, namely, nonnegative matrix factorization (NMF), multidimensional scaling (MDS), positron emission tomography (PET), and $\\ell_1$-regularized Cox regression.\n",
    "In particular, $\\ell_1$-regularized Cox regression with the UK Biobank dataset was the biggest multivariate survival analysis to our knowledge. \n",
    "In this workshop, we provide small examples that run on a single node, and demonstrate multi-GPU usage on our own machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic PyTorch Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce simple operations on PyTorch. Note that Python uses 0-based, rowmajor ordering, like C and C++ (R is 1-based, column-major ordering). First we import the PyTorch\n",
    "library. This is equvalent to library() in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may create an uninitialized tensor. This creates a 3 Ã— 4 tensor (matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8836e-27, 4.5705e-41, 1.8836e-27, 4.5705e-41],\n",
       "        [1.4574e-43, 6.4460e-44, 1.4153e-43, 1.5274e-43],\n",
       "        [1.5695e-43, 1.6255e-43, 1.6956e-43, 5.6052e-44]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, 4) # uninitialized tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is equivalent to `set.seed()` in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f67ec191930>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a tensor initialized with random values from (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1117, 0.8158, 0.2626, 0.4839],\n",
       "        [0.6765, 0.7539, 0.2627, 0.0428],\n",
       "        [0.2080, 0.1180, 0.1217, 0.7356]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(3, 4) # from Unif(0, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate a tensor filled with zeros or ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.ones(3, 4) # torch.zeros(3, 4)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be created from standard Python data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([3, 4, 5, 6])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be created in certain datatype (default: float32) and on certain device (default: CPU) of choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4., 5., 6.], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double precision\n",
    "w = torch.tensor([3, 4, 5, 6], dtype=torch.float64)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on GPU number zero. will not run if CUDA GPU is not present.\n",
    "w = torch.tensor([3, 4, 5, 6], device='cuda:0')\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of a tensor can be accessed by appending `.shape` to the tensor name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can change datatype and location by the method `.to()`. The arguments are similar to choosing datatype and device of the new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = w.to(device = \"cpu\", dtype=torch.int32)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are standard method of indexing tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7356)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2, 3] # indexing: zero-based, returns a 0-dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing always returns a (sub)tensor, even for scalars (treated as zero-dimensional tensors).\n",
    "A standard Python number can be returned by using .item()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7355988621711731"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2, 3].item() # A standard Python floating-point number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a column from a tensor, we use the indexing as below. The syntax is similar but slightly\n",
    "different from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4839, 0.0428, 0.7356])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, 3] # 3rd column. The leftmost column is 0th. cf. y[, 4] in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is for taking a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2080, 0.1180, 0.1217, 0.7356])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2, :] # 2nd row. The top row is 0th. cf. y[3, ] in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide an example of simple operations on PyTorch. Addition using the operator â€˜+â€™ acts\n",
    "just like anyone can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = y + z # a simple addition.\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another form of addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.add(y, z) # another syntax for addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operators ending with an underscore (`_`) changes the value of the tensor in-place. Otherwise, the argument never changes. Unlike methods ending with `!` in Julia, this rule is strictly enforced in PyTorch. (The underscore determines usage of the keyword `const` in C++-level.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(z) # in-place addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate the tensors using the function `cat()`, which resembles `c()`, `cbind()`, and\n",
    "`rbind()` in R. The second argument indicates the dimension that the tesors are concatenated\n",
    "along: zero means by concatenation by rows, and one means by columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((y, z), 0) # along the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356, 1.0000, 1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((y, z), 1) # along the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can reshape a tensor, like changing the attribute `dim` in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1117, 1.8158, 1.2626, 1.4839, 1.6765, 1.7539, 1.2627, 1.0428, 1.2080,\n",
       "        1.1180, 1.1217, 1.7356])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(12) # 1-dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to one of the arguments of `view()` can be âˆ’1. The size of the reshaped tensor is inferred\n",
    "from the other dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158],\n",
       "        [1.2626, 1.4839],\n",
       "        [1.6765, 1.7539],\n",
       "        [1.2627, 1.0428],\n",
       "        [1.2080, 1.1180],\n",
       "        [1.1217, 1.7356]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape into (6)-by-2 tensor;\n",
    "# (6) is inferred from the other dimension\n",
    "y.view(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `.sum()`, `.mean()`, `.std()` methods of a tensor do the obvious. Optional argument determines the dimension of reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5933)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9962, 4.6878, 3.6469, 4.2623])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(0) # reduces rows, columnwise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.6739, 5.7359, 5.1834])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(1) # reduces columns, rowwise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5933)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum((0, 1)) # reduces rows and columns -> a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3828)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3321, 1.5626, 1.2156, 1.4208])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3058, 0.3384, 0.2961])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.std(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix transpose is performed by appending `.t()` to a tensor. Matrix multiplication is carried out by the method `torch.mm()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6739, 5.6739, 5.6739],\n",
       "        [5.7359, 5.7359, 5.7359],\n",
       "        [5.1834, 5.1834, 5.1834]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(y, z.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.distributed`: Distributed subpackage for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.distributed` is the subpackage for distributed operations on PyTorch. The interface is mostly inspired by the message passing interface (MPI). The available backends are:\n",
    "\n",
    "* Gloo, a collective communication library developed by Facebook, included in PyTorch. Full support for CPU, partial collective communication only for GPU.\n",
    "* MPI, a good-old communication standard. The most flexible, but PyTorch needs to be compiled from its source to use it as a backend. Full support for GPU if the MPI installation is \"CUDA-aware\".\n",
    "* NCCL, Nvidia Collective Communications Library, collective communication only for multiple GPUs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we use Gloo for its full functionalities on CPU and runnability on Jupyter Notebook. The experiments in our paper use MPI for running multi-node setting and multi-GPU setting with basically the same code. The interface below is specific for Gloo backend. For MPI backend, please consult with a section from [distributed package tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends) or [our code](https://github.com/kose-y/dist_stat/tree/master/examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "def init_process(rank, size, fn, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "def run_process(size, fn):\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, fn))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        \n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-to-point communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_images/send_recv.png)\n",
    "Figure courtesy of: https://pytorch.org/tutorials/_images/send_recv.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def point_to_point(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    elif rank == 1:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    dist.barrier()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  1  has data  tensor(1.)\n",
      "Rank  0  has data  tensor(1.)\n",
      "Rank  2  has data  tensor(0.)\n",
      "Rank  3  has data  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, point_to_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collective communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | \n",
    "|:---|:---|\n",
    "| ![](https://pytorch.org/tutorials/_images/scatter.png) | ![](https://pytorch.org/tutorials/_images/gather.png) |\n",
    "| Scatter | Gather |\n",
    "| ![](https://pytorch.org/tutorials/_images/reduce.png) | ![](https://pytorch.org/tutorials/_images/all_reduce.png) |\n",
    "| Reduce | All-reduce |\n",
    "| ![](https://pytorch.org/tutorials/_images/broadcast.png) | ![](https://pytorch.org/tutorials/_images/all_gather.png) |\n",
    "| Broadcast | All-gather |\n",
    "\n",
    "Table courtesy of: https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def broadcast(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor[0] = 7\n",
    "    dist.broadcast(tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  0  has data  tensor(7.)\n",
      "Rank  3  has data  tensor(7.)\n",
      "Rank  2  has data  tensor(7.)\n",
      "Rank  1  has data  tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce(rank, size):\n",
    "    tensor = torch.ones(1)\n",
    "    dist.reduce(tensor, 3)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  3  has data  tensor(4.)\n",
      "Rank  0  has data  tensor(4.)\n",
      "Rank  1  has data  tensor(3.)\n",
      "Rank  2  has data  tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_reduce(rank, size):\n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  2  has data  tensor(4.)\n",
      "Rank  1  has data  tensor(4.)\n",
      "Rank  3  has data  tensor(4.)\n",
      "Rank  0  has data  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, all_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_gather(rank, size):\n",
    "    tensors = [torch.zeros(1) for i in range(size)]\n",
    "    dat = torch.zeros(1)\n",
    "    dat[0] += rank\n",
    "    dist.all_gather(tensors, dat)\n",
    "    print('Rank ', rank, ' has data ', tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  2  has data  [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\n",
      "Rank  3  has data  [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\n",
      "Rank  1  has data  [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\n",
      "Rank  0  has data  [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\n"
     ]
    }
   ],
   "source": [
    "run_process(4, all_gather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `distmat`: Distributed Matrices on PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tensor operations and communication package, we created a data structure for a distributed matrix. In this structure, each process,\n",
    "enumerated by its rank, holds a contiguous block of the full data matrix by rows or columns.\n",
    "The data may be a sparse matrix. If GPUs are involved, each process controls a GPU whose\n",
    "index matches the process rank. For notational simplicity, we denote the dimension to split\n",
    "in square brackets. If a [100] Ã— 100 matrix is split over four processes, the process with rank\n",
    "0 keeps the first 25 rows of the matrix, and the rank 3 process takes the last 25 rows. For\n",
    "the sake of simplicity, we always assume that the size along the split dimension is divided\n",
    "by the number of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dist_stat.distmat as distmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `distgen_ones()`: Creates a distributed matrix filled with ones\n",
    "- `distgen_zeros()`: Creates a distributed matrix filled with zeros\n",
    "- `distgen_uniform()`: Creates a distributed matrix from uniform distribution\n",
    "- `distgen_normal()`: Creates a distributed matrix from stndard normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_unif(rank, size):\n",
    "    A = distmat.distgen_uniform(8, 8, TType=torch.DoubleTensor)\n",
    "    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: Rank  1  has data  tensor([[0.5459, 0.3573, 0.2406, 0.5600, 0.6501, 0.9692, 0.5168, 0.2422],\n",
      "        [0.4312, 0.5917, 0.3425, 0.2202, 0.7030, 0.5629, 0.9259, 0.7612]],\n",
      "       dtype=torch.float64)Matrix A: Rank  2  has data  tensor([[0.8381, 0.4943, 0.5984, 0.6167, 0.6128, 0.8593, 0.1344, 0.5146],\n",
      "        [0.1479, 0.4238, 0.5144, 0.7051, 0.8133, 0.1795, 0.2721, 0.4631]],\n",
      "       dtype=torch.float64)Matrix A: Rank  3  has data  tensor([[0.6214, 0.7096, 0.7099, 0.7977, 0.8779, 0.1236, 0.5650, 0.0655],\n",
      "        [0.1210, 0.0454, 0.5070, 0.1860, 0.5035, 0.1454, 0.8090, 0.4991]],\n",
      "       dtype=torch.float64)\n",
      "\n",
      "\n",
      "Matrix A: Rank  0  has data  tensor([[0.6941, 0.3464, 0.9751, 0.7911, 0.4274, 0.4460, 0.5522, 0.9559],\n",
      "        [0.9405, 0.2215, 0.3271, 0.1352, 0.6283, 0.3030, 0.1302, 0.1811]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, create_unif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distributed matrix can also be created from local chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def from_chunks(rank, size):\n",
    "    torch.manual_seed(100 + rank)\n",
    "    chunk = torch.randn(2, 4)\n",
    "    print(\"rank \", rank, \"has chunk\", chunk)\n",
    "    A = distmat.THDistMat.from_chunks(chunk)\n",
    "    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)    \n",
    "    if rank == 0:\n",
    "        print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank  3 has chunk tensor([[ 1.7286, -0.4007,  2.5587,  1.6848],\n",
      "        [-1.6571, -0.2811,  0.7743, -0.9554]])\n",
      "rank  2 has chunk tensor([[ 0.9907,  0.3349,  1.1497, -0.5498],\n",
      "        [-0.1046,  2.0104, -0.7886, -0.1246]])\n",
      "rank  0 has chunk tensor([[ 0.3607, -0.2859, -0.3938,  0.2429],\n",
      "        [-1.3833, -2.3134, -0.3172, -0.8660]])\n",
      "rank  1 has chunk tensor([[-1.3905, -0.8152, -0.3204,  0.7377],\n",
      "        [-1.7534,  0.6033, -0.2520, -0.4373]])\n",
      "Matrix A: Rank  3  has data  tensor([[ 1.7286, -0.4007,  2.5587,  1.6848],\n",
      "        [-1.6571, -0.2811,  0.7743, -0.9554]])Matrix A: Rank  1  has data  tensor([[-1.3905, -0.8152, -0.3204,  0.7377],\n",
      "        [-1.7534,  0.6033, -0.2520, -0.4373]])Matrix A: Rank  2  has data  tensor([[ 0.9907,  0.3349,  1.1497, -0.5498],\n",
      "        [-0.1046,  2.0104, -0.7886, -0.1246]])Matrix A: Rank  0  has data  tensor([[ 0.3607, -0.2859, -0.3938,  0.2429],\n",
      "        [-1.3833, -2.3134, -0.3172, -0.8660]])\n",
      "\n",
      "\n",
      "\n",
      "[8, 4]\n"
     ]
    }
   ],
   "source": [
    "run_process(4, from_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be distributed from the master process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_data(rank, size):\n",
    "    if rank == 0:\n",
    "        data = torch.rand(4, 2)\n",
    "        print(\"master data: \", data)\n",
    "    else:\n",
    "        data = None\n",
    "    \n",
    "    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n",
    "    print('data_dist: ', 'Rank ', rank, ' has data ', data_dist.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master data:  tensor([[0.7118, 0.7876],\n",
      "        [0.4183, 0.9014],\n",
      "        [0.9969, 0.7565],\n",
      "        [0.2239, 0.3023]])\n",
      "data_dist:  Rank  0  has data  tensor([[0.7118, 0.7876]], dtype=torch.float64)\n",
      "data_dist:  Rank  2  has data  tensor([[0.9969, 0.7565]], dtype=torch.float64)\n",
      "data_dist:  Rank  1  has data  tensor([[0.4183, 0.9014]], dtype=torch.float64)\n",
      "data_dist:  Rank  3  has data  tensor([[0.2239, 0.3023]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, dist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the basic functions work naturally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elemwise_1(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_uniform(4, 2)\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk)    \n",
    "    C = A + B\n",
    "    print(\"C: rank \", rank, \"has chunk\", C.chunk)    \n",
    "    C += A\n",
    "    print(\"C: rank \", rank, \"has chunk\", C.chunk)\n",
    "    \n",
    "    logC = C.log()\n",
    "    print(\"logC: rank \", rank, \"has chunk\", logC.chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "B: rank  0 has chunk tensor([[0.9405, 0.2215]], dtype=torch.float64)\n",
      "B: rank  3 has chunk tensor([[0.1302, 0.1811]], dtype=torch.float64)\n",
      "B: rank  2 has chunk tensor([[0.6283, 0.3030]], dtype=torch.float64)\n",
      "B: rank  1 has chunk tensor([[0.3271, 0.1352]], dtype=torch.float64)\n",
      "C: rank  0 has chunk tensor([[1.6346, 0.5680]], dtype=torch.float64)\n",
      "C: rank  1 has chunk tensor([[1.3022, 0.9263]], dtype=torch.float64)\n",
      "C: rank  2 has chunk tensor([[1.0556, 0.7490]], dtype=torch.float64)\n",
      "C: rank  3 has chunk tensor([[0.6824, 1.1369]], dtype=torch.float64)\n",
      "C: rank  0 has chunk tensor([[2.3287, 0.9144]], dtype=torch.float64)\n",
      "C: rank  1 has chunk tensor([[2.2773, 1.7175]], dtype=torch.float64)\n",
      "C: rank  3 has chunk tensor([[1.2346, 2.0928]], dtype=torch.float64)\n",
      "C: rank  2 has chunk tensor([[1.4830, 1.1949]], dtype=torch.float64)\n",
      "logC: rank  2 has chunk tensor([[0.3941, 0.1781]], dtype=torch.float64)\n",
      "logC: rank  3 has chunk tensor([[0.2107, 0.7385]], dtype=torch.float64)\n",
      "logC: rank  1 has chunk tensor([[0.8230, 0.5409]], dtype=torch.float64)\n",
      "logC: rank  0 has chunk tensor([[ 0.8453, -0.0895]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, elemwise_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting (similar to Julia's dot broadcasting) also works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dim_broadcasting(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2) # [4] x 2 \n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_ones(4, 1) # [4] x 1\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk)\n",
    "    A += B # B treated as [4] x 2 matrix\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    C = 2 * torch.ones(1, 2, dtype=torch.float64) # 1 x 2\n",
    "    A += C # C treated as [4] x 2 matrix\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "B: rank  0 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "B: rank  3 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "B: rank  2 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "B: rank  1 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[1.6941, 1.3464]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[1.9751, 1.7911]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[1.5522, 1.9559]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[1.4274, 1.4460]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[3.9751, 3.7911]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[3.6941, 3.3464]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[3.4274, 3.4460]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[3.5522, 3.9559]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, dim_broadcasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For general functions, we have `.apply()` and `.apply_binary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elemwise_2(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_uniform(4, 2)\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk) \n",
    "    Asqp1 = A.apply(lambda x: x**2 + 1)\n",
    "    print(\"Asqp1: rank \", rank, \"has chunk\", Asqp1.chunk)    \n",
    "    AsqpBsq = A.apply_binary(B, lambda x, y: x**2 + y**2)\n",
    "    print(\"AsqpBsq: rank \", rank, \"has chunk\", AsqpBsq.chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "B: rank  1 has chunk tensor([[0.3271, 0.1352]], dtype=torch.float64)\n",
      "B: rank  2 has chunk tensor([[0.6283, 0.3030]], dtype=torch.float64)\n",
      "B: rank  0 has chunk tensor([[0.9405, 0.2215]], dtype=torch.float64)\n",
      "B: rank  3 has chunk tensor([[0.1302, 0.1811]], dtype=torch.float64)\n",
      "Asqp1: rank  1 has chunk tensor([[1.9508, 1.6259]], dtype=torch.float64)\n",
      "Asqp1: rank  0 has chunk tensor([[1.4818, 1.1200]], dtype=torch.float64)\n",
      "Asqp1: rank  2 has chunk tensor([[1.1826, 1.1989]], dtype=torch.float64)\n",
      "Asqp1: rank  3 has chunk tensor([[1.3049, 1.9137]], dtype=torch.float64)\n",
      "AsqpBsq: rank  2 has chunk tensor([[0.5774, 0.2907]], dtype=torch.float64)\n",
      "AsqpBsq: rank  3 has chunk tensor([[0.3219, 0.9465]], dtype=torch.float64)\n",
      "AsqpBsq: rank  1 has chunk tensor([[1.0578, 0.6442]], dtype=torch.float64)\n",
      "AsqpBsq: rank  0 has chunk tensor([[1.3663, 0.1691]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, elemwise_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions (sum, max, min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summations, minimums, and maximums can be carried out in a way similar to local tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reductions(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    print(\"sum of A: \", A.sum())\n",
    "    print(\"maximum of A: \", A.max())\n",
    "    print(\"minimum of A: \", A.min())\n",
    "    \n",
    "    sumA_row = A.sum(0) # row sum, a tensor with the same values on all processes \n",
    "    sumA_col = A.sum(1) # col sum, a distributed matrix\n",
    "    print(\"row sum of A: \", sumA_row)\n",
    "    \n",
    "    print(\"sumA_col: rank \", rank, \"has chunk\", sumA_col.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/home/kose/anaconda3/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/DATA/home/kose/anaconda3/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/DATA/home/kose/anaconda3/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/DATA/home/kose/anaconda3/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "sumA_col: rank  2 has chunk tensor([[0.8733]], dtype=torch.float64)\n",
      "sumA_col: rank  0 has chunk tensor([[1.0406]], dtype=torch.float64)\n",
      "sumA_col: rank  1 has chunk tensor([[1.7662]], dtype=torch.float64)\n",
      "sumA_col: rank  3 has chunk tensor([[1.5081]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, reductions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diagonals(rank, size):\n",
    "    if rank == 0:\n",
    "        p = 4\n",
    "        data = torch.randn(p, p)\n",
    "        print(\"master data: \", data)\n",
    "    else:\n",
    "        data = None\n",
    "        \n",
    "    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n",
    "    \n",
    "    diag1 = data_dist.diag() # distributed diagonal\n",
    "    print(\"diag1: rank \", rank, \"has chunk\", diag1.chunk)\n",
    "    \n",
    "    diag2 = data_dist.diag(distribute=False) # diagonal gathered in each process\n",
    "    print(\"diag2: \", diag2)\n",
    "    \n",
    "    data_dist.fill_diag_(0) # fill the diagonals with zeros\n",
    "    print(\"data_dist: rank \", rank, \"has chunk\", data_dist.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master data:  tensor([[ 0.6857,  0.7877, -0.9778,  2.1302],\n",
      "        [-3.1896,  1.5914, -0.0247, -0.8466],\n",
      "        [ 1.4205, -1.5741, -0.3572, -0.3097],\n",
      "        [ 1.1705, -0.5410, -0.7116,  0.0575]])\n",
      "diag1: rank  0 has chunk tensor([[0.6857]], dtype=torch.float64)\n",
      "diag1: rank  2 has chunk tensor([[-0.3572]], dtype=torch.float64)\n",
      "diag1: rank  3 has chunk tensor([[0.0575]], dtype=torch.float64)\n",
      "diag1: rank  1 has chunk tensor([[1.5914]], dtype=torch.float64)\n",
      "diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n",
      "data_dist: rank  2 has chunk tensor([[ 1.4205, -1.5741,  0.0000, -0.3097]], dtype=torch.float64)\n",
      "data_dist: rank  3 has chunk tensor([[ 1.1705, -0.5410, -0.7116,  0.0000]], dtype=torch.float64)\n",
      "data_dist: rank  1 has chunk tensor([[-3.1896,  0.0000, -0.0247, -0.8466]], dtype=torch.float64)\n",
      "data_dist: rank  0 has chunk tensor([[ 0.0000,  0.7877, -0.9778,  2.1302]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, diagonals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six different scenarios of matrix-matrix multiplications, each representing a different configuration of the split dimension of two input\n",
    "matrices and the output matrix, were considered and implemented. \n",
    "\n",
    "< Content of Table 1 here >\n",
    "\n",
    "\n",
    "\n",
    "The implementation of each case is carried\n",
    "out using the collective communication directives. Matrix multiplication scenarios are automatically selected based on the shapes of the input matrices A and\n",
    "B, except for the Scenarios 1 and 3 sharing the same input structure. Those two are further\n",
    "distinguished by the shape of output, AB. The nonnegative matrix factorization involves Scenarios 1 to 5.\n",
    "Scenario 6 is for matrix-vector multiplications, where broadcasting small vectors is almost\n",
    "always efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dist_stat.distmm import *\n",
    "def test_distmm(rank, size):\n",
    "    TType = torch.DoubleTensor\n",
    "    p = 12; q = 8; r = 2\n",
    "    if rank==0:\n",
    "        fat   = TType(p, q).normal_()\n",
    "        thin1 = TType(q, r).normal_()\n",
    "        thin2 = TType(p, r).normal_()\n",
    "\n",
    "    else:\n",
    "        fat, thin1, thin2 = None, TType(q,r), TType(p,r)\n",
    "    dist.broadcast(thin1,0)\n",
    "    dist.broadcast(thin2,0)\n",
    "\n",
    "    fat_dist   = distmat.dist_data(fat, src=0, TType=TType)\n",
    "    thin1_dist = distmat.dist_data(thin1, src=0, TType=TType)\n",
    "    thin2_dist = distmat.dist_data(thin2, src=0, TType=TType)\n",
    "\n",
    "    # test distmm_thinthin_inner \n",
    "    if rank==0: \n",
    "        print(\"distmm_thinthin_inner: thin1^T x thin1\")\n",
    "        thin1_thin1 = torch.mm(torch.t(thin1), thin1)\n",
    "        print(\"thin1^T x thin1: \", thin1_thin1)\n",
    "\n",
    "    thin1_thin1_bd = distmm_thinthin_inner(thin1_dist.t(), thin1_dist)\n",
    "    print(\"rslt in rank %d: \"%(rank,), thin1_thin1_bd)\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"distmm_db_d: thin2 x thin1_thin1\")\n",
    "        correct = torch.mm(thin2, thin1_thin1)\n",
    "        print(correct)\n",
    "    rslt_dist = distmm_db_d(thin2_dist, thin1_thin1_bd)\n",
    "    print(\"rslt in rand %d: \"%(rank,), rslt_dist.chunk)\n",
    "    print(rslt_dist.byrow)\n",
    "\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"distmm_db_d (reverse): thin1_thin1 x thin2^T\")\n",
    "        correct = torch.mm(thin1_thin1, torch.t(thin2))\n",
    "        print(correct)\n",
    "    rslt_dist = distmm_db_d(thin2_dist.t(), thin1_thin1_bd, True)\n",
    "    print(\"rslt in rand %d: \"%(rank,), rslt_dist.chunk)\n",
    "    print(rslt_dist.byrow)\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"_distmm_fatthin_byrow: fat x thin1\")\n",
    "        correct = torch.mm(fat, thin1)\n",
    "        print(correct)\n",
    "    rslt_dist = distmm_fatthin(fat_dist, thin1_dist)\n",
    "    print(\"rslt in rand %d: \"%(rank,), rslt_dist.chunk)\n",
    "    print(rslt_dist.byrow)\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"_distmm_fatthin_byrow (reverse): thin1^T x fat^T\")\n",
    "        correct = torch.mm(torch.t(thin1), torch.t(fat))\n",
    "        print(correct)\n",
    "    rslt_dist = distmm_fatthin(fat_dist.t(), thin1_dist.t(), reverse=True)\n",
    "    print(\"rslt in rand %d: \"%(rank,), rslt_dist.chunk)\n",
    "    print(rslt_dist.byrow)\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"_distmm_thinfat_byrow: thin2^T x fat\" )\n",
    "        # Note: this is reverse for distmm_fatthin, non-reverse for inner ftn\n",
    "        correct = torch.mm(torch.t(thin2), fat)\n",
    "        print(correct)\n",
    "    rslt_dist = distmm_fatthin(fat_dist, thin2_dist.t(), reverse=True, \n",
    "                                out_sizes=thin1_dist.sizes)\n",
    "    print(\"rslt in rand %d: \"%(rank,), rslt_dist.chunk)\n",
    "    print(rslt_dist.byrow)\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"_distmm_thinfat_byrow (reverse): fat^T x thin2\" )\n",
    "        # Note: this is reverse for distmm_fatthin, non-reverse for inner ftn\n",
    "        correct = torch.mm(torch.t(fat), thin2)\n",
    "        print(correct)\n",
    "    rslt_dist = distmm_fatthin(fat_dist.t(), thin2_dist, reverse=False, \n",
    "                                out_sizes=thin1_dist.sizes)\n",
    "    print(\"rslt in rand %d: \"%(rank,), rslt_dist.chunk)\n",
    "    print(rslt_dist.byrow)\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"distmm_thinthin_outer: thin1 x thin2^T\" )\n",
    "        correct = torch.mm(thin1, torch.t(thin2))\n",
    "        print(correct)\n",
    "    rslt_dist = distmm_thinthin_outer(thin1_dist, thin2_dist.t())\n",
    "    print(\"rslt in rank %d: \"%(rank,), rslt_dist.chunk)\n",
    "    print(rslt_dist.byrow)\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"distmm_db_b: thin1^T x thin1(dense)\" )\n",
    "        correct = torch.mm(torch.t(thin1), thin1)\n",
    "        print(correct)\n",
    "    rslt_dist = distmm_db_b(thin1_dist.t(), thin1)\n",
    "    print(\"rslt in rank %d: \"%(rank,), rslt_dist)\n",
    "\n",
    "\n",
    "    dist.barrier()\n",
    "    if rank==0:\n",
    "        print(\"now we check distributed sparse matrices.\")\n",
    "\n",
    "    def to_sparse(x):\n",
    "        \"\"\" converts dense tensor x to sparse format \"\"\"\n",
    "        x_typename = torch.typename(x).split('.')[-1]\n",
    "        sparse_tensortype = getattr(torch.sparse, x_typename)\n",
    "\n",
    "        indices = torch.nonzero(x)\n",
    "        if len(indices.shape) == 0:  # if all elements are zeros\n",
    "            return sparse_tensortype(*x.shape)\n",
    "        indices = indices.t()\n",
    "        values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "        return sparse_tensortype(indices, values, x.size())\n",
    "    \n",
    "    thin1_sparse_chunk = to_sparse(thin1_dist.chunk)\n",
    "    thin1_sparse_dist = THDistMat.from_chunks(thin1_sparse_chunk)\n",
    "    thin2_sparse_chunk = to_sparse(thin2_dist.chunk)\n",
    "    thin2_sparse_dist = THDistMat.from_chunks(thin2_sparse_chunk)\n",
    "    print(thin1_sparse_chunk.shape)\n",
    "    print(thin1_sparse_dist.t().shape)\n",
    "    print(thin1_dist.shape)\n",
    "    if rank==0:\n",
    "        print(\"correct: \", torch.mm(thin1.t(), thin1))\n",
    "\n",
    "    r =  distmat.mm(thin1_sparse_dist.t(), thin1_dist )\n",
    "    print(\"rslt: \", r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distmm_thinthin_inner: thin1^T x thin1\n",
      "thin1^T x thin1:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rslt in rank 0:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)\n",
      "rslt in rank 1:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)rslt in rank 2:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)rslt in rank 3:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "distmm_db_d: thin2 x thin1_thin1\n",
      "tensor([[ 20.4717, -20.9301],\n",
      "        [ -0.3866,  -2.3102],\n",
      "        [  3.4683,  -1.9985],\n",
      "        [ 10.0434, -21.7488],\n",
      "        [ -8.9008,   1.7786],\n",
      "        [ 14.3755, -13.4916],\n",
      "        [ 14.6289,   6.1289],\n",
      "        [-11.1466,  -3.2908],\n",
      "        [ -4.6314,  17.0375],\n",
      "        [ -4.6952,  -2.8811],\n",
      "        [ 27.1388,  -5.2625],\n",
      "        [ -7.5063,  -3.7725]], dtype=torch.float64)\n",
      "rslt in rand 3:  tensor([[-4.6952, -2.8811],\n",
      "        [27.1388, -5.2625],\n",
      "        [-7.5063, -3.7725]], dtype=torch.float64)rslt in rand 0:  tensor([[ 20.4717, -20.9301],\n",
      "        [ -0.3866,  -2.3102],\n",
      "        [  3.4683,  -1.9985]], dtype=torch.float64)\n",
      "\n",
      "rslt in rand 1:  tensor([[ 10.0434, -21.7488],\n",
      "        [ -8.9008,   1.7786],\n",
      "        [ 14.3755, -13.4916]], dtype=torch.float64)rslt in rand 2:  tensor([[ 14.6289,   6.1289],\n",
      "        [-11.1466,  -3.2908],\n",
      "        [ -4.6314,  17.0375]], dtype=torch.float64)\n",
      "True\n",
      "True\n",
      "\n",
      "True\n",
      "True\n",
      "distmm_db_d (reverse): thin1_thin1 x thin2^T\n",
      "tensor([[ 20.4717,  -0.3866,   3.4683,  10.0434,  -8.9008,  14.3755,  14.6289,\n",
      "         -11.1466,  -4.6314,  -4.6952,  27.1388,  -7.5063],\n",
      "        [-20.9301,  -2.3102,  -1.9985, -21.7488,   1.7786, -13.4916,   6.1289,\n",
      "          -3.2908,  17.0375,  -2.8811,  -5.2625,  -3.7725]],\n",
      "       dtype=torch.float64)\n",
      "rslt in rand 0:  tensor([[ 20.4717,  -0.3866,   3.4683],\n",
      "        [-20.9301,  -2.3102,  -1.9985]], dtype=torch.float64)rslt in rand 1:  tensor([[ 10.0434,  -8.9008,  14.3755],\n",
      "        [-21.7488,   1.7786, -13.4916]], dtype=torch.float64)\n",
      "rslt in rand 3:  tensor([[-4.6952, 27.1388, -7.5063],\n",
      "        [-2.8811, -5.2625, -3.7725]], dtype=torch.float64)rslt in rand 2:  tensor([[ 14.6289, -11.1466,  -4.6314],\n",
      "        [  6.1289,  -3.2908,  17.0375]], dtype=torch.float64)\n",
      "False\n",
      "\n",
      "\n",
      "False\n",
      "False\n",
      "False\n",
      "_distmm_fatthin_byrow: fat x thin1\n",
      "tensor([[-2.5992,  2.4633],\n",
      "        [11.2190, -6.3513],\n",
      "        [-1.4552,  2.8982],\n",
      "        [-2.4909, -3.5356],\n",
      "        [-3.3804,  3.2883],\n",
      "        [-2.1293, -1.2122],\n",
      "        [-5.5143,  1.4986],\n",
      "        [-0.4519, -2.3879],\n",
      "        [ 8.8853, -3.7040],\n",
      "        [-1.0206,  6.2059],\n",
      "        [ 4.3791,  4.5227],\n",
      "        [-4.9386, -1.6446]], dtype=torch.float64)\n",
      "rslt in rand 1:  tensor([[-2.4909, -3.5356],\n",
      "        [-3.3804,  3.2883],\n",
      "        [-2.1293, -1.2122]], dtype=torch.float64)rslt in rand 2:  tensor([[-5.5143,  1.4986],\n",
      "        [-0.4519, -2.3879],\n",
      "        [ 8.8853, -3.7040]], dtype=torch.float64)rslt in rand 3:  tensor([[-1.0206,  6.2059],\n",
      "        [ 4.3791,  4.5227],\n",
      "        [-4.9386, -1.6446]], dtype=torch.float64)rslt in rand 0:  tensor([[-2.5992,  2.4633],\n",
      "        [11.2190, -6.3513],\n",
      "        [-1.4552,  2.8982]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "True\n",
      "True\n",
      "\n",
      "True\n",
      "True\n",
      "_distmm_fatthin_byrow (reverse): thin1^T x fat^T\n",
      "tensor([[-2.5992, 11.2190, -1.4552, -2.4909, -3.3804, -2.1293, -5.5143, -0.4519,\n",
      "          8.8853, -1.0206,  4.3791, -4.9386],\n",
      "        [ 2.4633, -6.3513,  2.8982, -3.5356,  3.2883, -1.2122,  1.4986, -2.3879,\n",
      "         -3.7040,  6.2059,  4.5227, -1.6446]], dtype=torch.float64)\n",
      "rslt in rand 0:  tensor([[-2.5992, 11.2190, -1.4552],\n",
      "        [ 2.4633, -6.3513,  2.8982]], dtype=torch.float64)\n",
      "rslt in rand 2:  tensor([[-5.5143, -0.4519,  8.8853],\n",
      "        [ 1.4986, -2.3879, -3.7040]], dtype=torch.float64)rslt in rand 1:  tensor([[-2.4909, -3.3804, -2.1293],\n",
      "        [-3.5356,  3.2883, -1.2122]], dtype=torch.float64)False\n",
      "rslt in rand 3:  tensor([[-1.0206,  4.3791, -4.9386],\n",
      "        [ 6.2059,  4.5227, -1.6446]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "False\n",
      "False\n",
      "False\n",
      "_distmm_thinfat_byrow: thin2^T x fat\n",
      "tensor([[ 3.2638, -2.0531, -1.4650,  1.3398, -2.1822,  2.2660,  5.9041,  0.0584],\n",
      "        [-5.1742,  0.1255,  0.3624, -2.2235,  0.0063,  3.7953,  3.0134, -1.3422]],\n",
      "       dtype=torch.float64)\n",
      "rslt in rand 1:  tensor([[ 0.6312, -0.4432],\n",
      "        [-0.8251, -0.9634]], dtype=torch.float64)rslt in rand 0:  tensor([[ 70.2478,  -2.0531],\n",
      "        [-44.7147,   0.1255]], dtype=torch.float64)rslt in rand 3:  tensor([[ 3.2755, -0.7753],\n",
      "        [ 0.7638, -0.0223]], dtype=torch.float64)rslt in rand 2:  tensor([[-2.4445,  0.2279],\n",
      "        [-4.8714,  1.9004]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "_distmm_thinfat_byrow (reverse): fat^T x thin2\n",
      "tensor([[ 3.2638, -5.1742],\n",
      "        [-2.0531,  0.1255],\n",
      "        [-1.4650,  0.3624],\n",
      "        [ 1.3398, -2.2235],\n",
      "        [-2.1822,  0.0063],\n",
      "        [ 2.2660,  3.7953],\n",
      "        [ 5.9041,  3.0134],\n",
      "        [ 0.0584, -1.3422]], dtype=torch.float64)\n",
      "rslt in rand 2:  tensor([[-2.4445, -4.8714],\n",
      "        [ 0.2279,  1.9004]], dtype=torch.float64)rslt in rand 1:  tensor([[ 0.6312, -0.8251],\n",
      "        [-0.4432, -0.9634]], dtype=torch.float64)rslt in rand 0:  tensor([[ 70.2478, -44.7147],\n",
      "        [ -2.0531,   0.1255]], dtype=torch.float64)\n",
      "rslt in rand 3:  tensor([[ 3.2755,  0.7638],\n",
      "        [-0.7753, -0.0223]], dtype=torch.float64)True\n",
      "\n",
      "\n",
      "\n",
      "True\n",
      "True\n",
      "True\n",
      "distmm_thinthin_outer: thin1 x thin2^T\n",
      "tensor([[-2.3927,  0.0871, -0.4293, -0.9960,  1.1537, -1.6988, -2.0365,  1.5303,\n",
      "          0.3507,  0.6678, -3.5203,  1.0547],\n",
      "        [-1.8305, -0.0793, -0.2450, -1.3813,  0.4877, -1.2347, -0.4204,  0.3784,\n",
      "          0.9320,  0.0965, -1.4802,  0.1893],\n",
      "        [ 3.0447,  0.2430,  0.3439,  2.7690, -0.5105,  2.0041, -0.1665, -0.0263,\n",
      "         -2.0554,  0.1550,  1.5387,  0.1552],\n",
      "        [ 1.3633, -0.0686,  0.2555,  0.4869, -0.7087,  0.9764,  1.3082, -0.9749,\n",
      "         -0.1135, -0.4344,  2.1634, -0.6812],\n",
      "        [-2.0370, -0.4245, -0.0803, -2.9638, -0.3671, -1.2241,  2.1522, -1.4039,\n",
      "          2.5659, -0.8472,  1.1468, -1.2117],\n",
      "        [ 2.7335, -0.0078,  0.4381,  1.5268, -1.0700,  1.9000,  1.6121, -1.2506,\n",
      "         -0.8175, -0.5026,  3.2598, -0.8170],\n",
      "        [-2.2222, -0.5058, -0.0631, -3.4148, -0.5163, -1.3163,  2.6815, -1.7639,\n",
      "          2.9938, -1.0457,  1.6067, -1.5030],\n",
      "        [ 2.2380,  0.0890,  0.3041,  1.6549, -0.6178,  1.5131,  0.5763, -0.5060,\n",
      "         -1.1032, -0.1406,  1.8760, -0.2653]], dtype=torch.float64)\n",
      "rslt in rank 2:  tensor([[-2.0370, -0.4245, -0.0803, -2.9638, -0.3671, -1.2241,  2.1522, -1.4039,\n",
      "          2.5659, -0.8472,  1.1468, -1.2117],\n",
      "        [ 2.7335, -0.0078,  0.4381,  1.5268, -1.0700,  1.9000,  1.6121, -1.2506,\n",
      "         -0.8175, -0.5026,  3.2598, -0.8170]], dtype=torch.float64)\n",
      "rslt in rank 3:  tensor([[-2.2222, -0.5058, -0.0631, -3.4148, -0.5163, -1.3163,  2.6815, -1.7639,\n",
      "          2.9938, -1.0457,  1.6067, -1.5030],\n",
      "        [ 2.2380,  0.0890,  0.3041,  1.6549, -0.6178,  1.5131,  0.5763, -0.5060,\n",
      "         -1.1032, -0.1406,  1.8760, -0.2653]], dtype=torch.float64)rslt in rank 0:  tensor([[-2.3927,  0.0871, -0.4293, -0.9960,  1.1537, -1.6988, -2.0365,  1.5303,\n",
      "          0.3507,  0.6678, -3.5203,  1.0547],\n",
      "        [-1.8305, -0.0793, -0.2450, -1.3813,  0.4877, -1.2347, -0.4204,  0.3784,\n",
      "          0.9320,  0.0965, -1.4802,  0.1893]], dtype=torch.float64)rslt in rank 1:  tensor([[ 3.0447,  0.2430,  0.3439,  2.7690, -0.5105,  2.0041, -0.1665, -0.0263,\n",
      "         -2.0554,  0.1550,  1.5387,  0.1552],\n",
      "        [ 1.3633, -0.0686,  0.2555,  0.4869, -0.7087,  0.9764,  1.3082, -0.9749,\n",
      "         -0.1135, -0.4344,  2.1634, -0.6812]], dtype=torch.float64)\n",
      "True\n",
      "\n",
      "True\n",
      "\n",
      "True\n",
      "distmm_db_b: thin1^T x thin1(dense)\n",
      "True\n",
      "tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)\n",
      "rslt in rank 0:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)\n",
      "rslt in rank 3:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)rslt in rank 2:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)rslt in rank 1:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "now we check distributed sparse matrices.\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "[2, 8]\n",
      "[2, 8]\n",
      "[2, 8]\n",
      "[2, 8]\n",
      "[8, 2]\n",
      "[8, 2]\n",
      "[8, 2]\n",
      "correct:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)[8, 2]\n",
      "\n",
      "rslt:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)rslt:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)rslt:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "rslt:  tensor([[18.8603, -5.5108],\n",
      "        [-5.5108, 13.0393]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, test_distmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonnegative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a simplified version. The full object-oriented version is available at the [GitHub repo](https://github.com/kose-y/dist_stat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nmf(rank, size):\n",
    "    # TODO: initialization here\n",
    "    for i in range(maxiter):\n",
    "        XWt =  distmat.mm(data, W.t())\n",
    "        WWt =  distmat.mm(W, W.t())\n",
    "        VWWt = distmat.mm(V, WWt)\n",
    "        V.mul_(XWt).div_(VWWt)\n",
    "\n",
    "        VtX  = distmat.mm(V.t(), data, out_sizes=W.sizes)\n",
    "        VtV  = distmat.mm(V.t(), V)\n",
    "        VtVW = distmat.mm(VtV, W)\n",
    "        W = W.mul_(VtX).div_(VtVW)\n",
    "        if i % 10 == 0:\n",
    "            # print obj\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$-regularized Cox Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cox_l1(rank, size):\n",
    "    # TODO: initialization here\n",
    "    lambd = 0.01\n",
    "    soft_threshold = torch.nn.Softshrink(lambd)\n",
    "    for i in range(maxiter):\n",
    "        Xbeta = distmat.mm(data, beta)\n",
    "        w = Xbeta.exp()\n",
    "        W = w.cumsum(0)\n",
    "        w_dist = distmat.dist_data(w, TType=w.TType)\n",
    "        pi = (w_dist/W.t()) * pi_ind\n",
    "        pd  = distmat.mm(pi, delta)\n",
    "        dmpd = delta_dist - pd\n",
    "        grad = distmat.mm(datat, dmpd)\n",
    "        beta = (beta + grad * sigma).apply(soft_threshold)\n",
    "        if i % 10 == 0:\n",
    "            # print obj\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate 10,000 x 10,000 examples on 2-8 GPUs on our server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure can also be utilized on multi-node clusters. The structure was used for the analysis of 200,000 x 500,000 UK Biobank data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI-only, lightweight, more flexible version in Julia is in preparation. CUDA-aware MPI support for the central MPI interface [MPI.jl](https://github.com/JuliaParallel/MPI.jl) was added in the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

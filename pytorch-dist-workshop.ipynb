{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# A Distributed Matrix Data Structure and Its Statistical Applications on PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**The Programming Workshop at the Inaugural Kenneth Lange Symposium, Feb 21-22, 2020**\n",
    "\n",
    "_Seyoon Ko and Joong-Ho (Johann) Won_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We developed a distributed matrix operation package suitable for distributed matrix-vector operations and distributed tall-and-thin (or wide-and-short) matrices.\n",
    "The code runs on both multi-node machines and multi-GPU machines using PyTorch.\n",
    "We have applied this package for four statistical applications, namely, nonnegative matrix factorization (NMF), multidimensional scaling (MDS), positron emission tomography (PET), and $\\ell_1$-regularized Cox regression.\n",
    "In particular, $\\ell_1$-regularized Cox regression with the UK Biobank dataset was the biggest joint multivariate survival analysis to our knowledge. \n",
    "In this workshop, we provide small examples that run on a single node, and demonstrate multi-GPU usage on our machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "* Brief introduction to PyTorch operations\n",
    "* `torch.distributed` package\n",
    "* Distributed matrix data structure in package `dist_stat`\n",
    "* Applications: Nonnegative matrix factorization and $\\ell_1$-penalized Cox regression\n",
    "* Demonstration on multi-GPU machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. It has two goals of development:\n",
    "* A replacement for NumPy to use the power of GPUs $\\rightarrow$ optimization of numerical operations\n",
    "* A deep learning research platform that provides maximum flexibility and speed $\\rightarrow$ optimization of automatic gradient computation e.g. backpropagation\n",
    "\n",
    "We are trying to exploit the former in a distributed environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Basic PyTorch Operations\n",
    "We introduce simple operations on PyTorch. Note that Python uses 0-based, row-major ordering, like C and C++ (cf. R and Julia have 1-based, column-major ordering). First we import the PyTorch\n",
    "library. This is similar to `library()` in R and equivalent to `import ...` in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may create an uninitialized tensor. This creates a 3 Ã— 4 tensor (matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1604e-35, 3.0899e-41, 2.2561e-43, 0.0000e+00],\n",
       "        [2.5622e-29, 4.5761e-41, 2.1553e-35, 3.0899e-41],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(3, 4) # uninitialized tensor. Julia equivalent: Array{Float32}(undef, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is equivalent to `set.seed()` in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8ff0072b10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100) # Julia equivalent: Random.seed!(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a tensor initialized with random values from (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1117, 0.8158, 0.2626, 0.4839],\n",
       "        [0.6765, 0.7539, 0.2627, 0.0428],\n",
       "        [0.2080, 0.1180, 0.1217, 0.7356]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(3, 4) # from Unif(0, 1). \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate a tensor filled with zeros or ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.ones(3, 4) # torch.zeros(3, 4)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be created from standard Python data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tensor([3, 4, 5, 6])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be created in certain datatype (default: float32) and on certain device (default: CPU) of choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4., 5., 6.], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double precision\n",
    "w = torch.tensor([3, 4, 5, 6], dtype=torch.float64)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # on GPU number zero. will not run if CUDA GPU is not present.\n",
    "# w = torch.tensor([3, 4, 5, 6], device='cuda:0')\n",
    "# w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of a tensor can be accessed by appending `.shape` to the tensor name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can have datatype and location changed by the method `.to()`. The arguments are similar to choosing datatype and device of the new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5, 6], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = w.to(device = \"cpu\", dtype=torch.int32)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are standard method of indexing tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7356)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2, 3] # indexing: zero-based, returns a 0-dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing always returns a (sub)tensor, even for scalars (treated as zero-dimensional tensors).\n",
    "A standard Python number can be returned by using .item()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7355988621711731"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2, 3].item() # A standard Python floating-point number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a column from a tensor, we use the indexing as below. The syntax is similar but slightly\n",
    "different from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4839, 0.0428, 0.7356])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, 3] # 3rd column. The leftmost column is 0th. cf. y[, 4] in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is for taking a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2080, 0.1180, 0.1217, 0.7356])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2, :] # 2nd row. The top row is 0th. cf. y[3, ] in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide an example of simple operations on PyTorch. Addition using the operator â€˜+â€™ acts\n",
    "just like anyone can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = y + z # a simple addition.\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another form of addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.add(y, z) # another syntax for addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operators ending with an underscore (`_`) changes the value of the tensor in-place. Otherwise, the argument never changes. Unlike methods ending with `!` in Julia, this rule is strictly enforced in PyTorch. (The underscore determines usage of the keyword `const` in C++-level.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(z) # in-place addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate the tensors using the function `cat()`, which resembles `c()`, `cbind()`, and\n",
    "`rbind()` in R, `cat()`, `vcat()`, `hcat()` in Julia. The second argument indicates the dimension that the tesors are concatenated\n",
    "along: zero means by concatenation by rows, and one means by columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((y, z), 0) # along the rows. cf. vcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356, 1.0000, 1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((y, z), 1) # along the columns. cf. hcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can reshape a tensor, like changing the attribute `dim` in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1117, 1.8158, 1.2626, 1.4839, 1.6765, 1.7539, 1.2627, 1.0428, 1.2080,\n",
       "        1.1180, 1.1217, 1.7356])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view(12) # 1-dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to one of the arguments of `view()` can be âˆ’1. The size of the reshaped tensor is inferred\n",
    "from the other dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158],\n",
       "        [1.2626, 1.4839],\n",
       "        [1.6765, 1.7539],\n",
       "        [1.2627, 1.0428],\n",
       "        [1.2080, 1.1180],\n",
       "        [1.1217, 1.7356]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape into (6)-by-2 tensor;\n",
    "# (6) is inferred from the other dimension\n",
    "y.view(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistics and Reductions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `.sum()`, `.mean()`, `.std()` methods of a tensor do the obvious. Optional argument determines the dimension of reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1117, 1.8158, 1.2626, 1.4839],\n",
       "        [1.6765, 1.7539, 1.2627, 1.0428],\n",
       "        [1.2080, 1.1180, 1.1217, 1.7356]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5933)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.9962, 4.6878, 3.6469, 4.2623])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(0) # reduces rows, columnwise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.6739, 5.7359, 5.1834])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum(1) # reduces columns, rowwise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5933)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum((0, 1)) # reduces rows and columns -> a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3828)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3321, 1.5626, 1.2156, 1.4208])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean(0) # columnwise mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3058, 0.3384, 0.2961])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.std(1) # rowwise standard deviation (division by (n-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix transpose is performed by appending `.t()` to a tensor. Matrix multiplication is carried out by the method `torch.mm()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6739, 5.6739, 5.6739],\n",
       "        [5.7359, 5.7359, 5.7359],\n",
       "        [5.1834, 5.1834, 5.1834]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(y, z.t()) # Note: y is 3 x 4, z is 3 x 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.distributed`: Distributed subpackage for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.distributed` is the subpackage for distributed operations on PyTorch. The interface is mostly inspired by the message passing interface (MPI). The available backends are:\n",
    "\n",
    "* Gloo, a collective communication library developed by Facebook, included in PyTorch. Full support for CPU, partial collective communication only for GPU.\n",
    "* MPI, a good-old communication standard. The most flexible, but PyTorch needs to be compiled from its source to use it as a backend. Full support for GPU if the MPI installation is \"CUDA-aware\".\n",
    "* NCCL, Nvidia Collective Communications Library, collective communication only for multiple GPUs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we use Gloo for its full functionalities on CPU and runnability on Jupyter Notebook. The experiments in our paper use MPI for running multi-node setting and multi-GPU setting with basically the same code. The interface below is specific for Gloo backend. For MPI backend, please consult with a section from [distributed package tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends) or [our example code](https://github.com/kose-y/dist_stat/tree/master/examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "def init_process(rank, size, fn, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "def run_process(size, fn):\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, fn))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        \n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each distributed function from now on will have `rank` and `size` as the two arguments. When `run_process` is called with the communicator size `size` and the function name `fn`, `size` process will be launched, and each of them will call `fn` with `rank` of each process and the `size`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-to-point communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_images/send_recv.png)\n",
    "\n",
    "Figure courtesy of: https://pytorch.org/tutorials/_images/send_recv.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def point_to_point(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    elif rank == 1:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    dist.barrier()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  0  has data  tensor(1.)\n",
      "Rank  2  has data  tensor(0.)\n",
      "Rank  3  has data  tensor(0.)\n",
      "Rank  1  has data  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, point_to_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collective communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | \n",
    "|:---|:---|\n",
    "| ![](https://pytorch.org/tutorials/_images/scatter.png) | ![](https://pytorch.org/tutorials/_images/gather.png) |\n",
    "| Scatter | Gather |\n",
    "| ![](https://pytorch.org/tutorials/_images/reduce.png) | ![](https://pytorch.org/tutorials/_images/all_reduce.png) |\n",
    "| Reduce | All-reduce |\n",
    "| ![](https://pytorch.org/tutorials/_images/broadcast.png) | ![](https://pytorch.org/tutorials/_images/all_gather.png) |\n",
    "| Broadcast | All-gather |\n",
    "\n",
    "Table courtesy of: https://pytorch.org/tutorials/intermediate/dist_tuto.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor[0] = 7\n",
    "    dist.broadcast(tensor, src=0)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  0  has data  tensor(7.)\n",
      "Rank  2  has data  tensor(7.)\n",
      "Rank  3  has data  tensor(7.)\n",
      "Rank  1  has data  tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, broadcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(rank, size):\n",
    "    tensor = torch.ones(1)\n",
    "    dist.reduce(tensor, 3)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  0  has data  tensor(4.)\n",
      "Rank  3  has data  tensor(4.)\n",
      "Rank  1  has data  tensor(3.)\n",
      "Rank  2  has data  tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_reduce(rank, size):\n",
    "    tensor = torch.ones(1)\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  0  has data  tensor(4.)\n",
      "Rank  2  has data  tensor(4.)\n",
      "Rank  3  has data  tensor(4.)\n",
      "Rank  1  has data  tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, all_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_gather(rank, size):\n",
    "    tensors = [torch.zeros(1) for i in range(size)]\n",
    "    dat = torch.zeros(1)\n",
    "    dat[0] += rank\n",
    "    dist.all_gather(tensors, dat)\n",
    "    print('Rank ', rank, ' has data ', tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank  0  has data  [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\n",
      "Rank  3  has data  [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\n",
      "Rank  1  has data  [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\n",
      "Rank  2  has data  [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]\n"
     ]
    }
   ],
   "source": [
    "run_process(4, all_gather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the code for simple Monte Carlo $\\pi$ estimation. 100,000 $x_i$ and $y_i$ are sampled from $Unif(0,1)$ for each process, and we measure the proportion of $(x_i, y_i)$s inside the unit quartercircle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_pi(n, size):\n",
    "    # this code is executed on each process.\n",
    "    x = torch.rand((n), dtype=torch.float64)\n",
    "    y = torch.rand((n), dtype=torch.float64)\n",
    "    # compute local estimate of pi\n",
    "    r = torch.mean((x**2 + y**2 < 1).to(dtype=torch.float64))*4\n",
    "    dist.all_reduce(r) # sum of 'r's in each device is stored in 'r'\n",
    "    return r / size\n",
    "\n",
    "def run_mc_pi(rank, size):\n",
    "    n = 100000\n",
    "    torch.manual_seed(100 + rank)\n",
    "    r = mc_pi(n, size)\n",
    "    if rank == 0:\n",
    "        print(r.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14356\n"
     ]
    }
   ],
   "source": [
    "run_process(4, run_mc_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `distmat`: Distributed Matrices on PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tensor operations and communication package, we created a data structure for a distributed matrix. \n",
    "* Each process (enumerated by rank) holds a contiguous block of the full data matrix by rows or columns.\n",
    "* The data may be a sparse matrix. \n",
    "* If GPUs are involved, each process controls a GPU whose index matches the process rank. \n",
    "* From now on: [100] Ã— 100 matrix split over four processes means...\n",
    "    * Rank 0 process keeps rows [0:25). of the matrix, in row-major ordering.\n",
    "    * Rank 3 process keeps rows [75:100).\n",
    "* Limitation: length of distributed dimension should be divisible by number of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dist_stat.distmat as distmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `distgen_ones()`: Creates a distributed matrix filled with ones\n",
    "- `distgen_zeros()`: Creates a distributed matrix filled with zeros\n",
    "- `distgen_uniform()`: Creates a distributed matrix from uniform distribution\n",
    "- `distgen_normal()`: Creates a distributed matrix from stndard normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unif(rank, size):\n",
    "    A = distmat.distgen_uniform(8, 8, TType=torch.DoubleTensor)\n",
    "    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A: Rank  3  has data  tensor([[0.6214, 0.7096, 0.7099, 0.7977, 0.8779, 0.1236, 0.5650, 0.0655],\n",
      "        [0.1210, 0.0454, 0.5070, 0.1860, 0.5035, 0.1454, 0.8090, 0.4991]],\n",
      "       dtype=torch.float64)Matrix A: Rank  0  has data  tensor([[0.6941, 0.3464, 0.9751, 0.7911, 0.4274, 0.4460, 0.5522, 0.9559],\n",
      "        [0.9405, 0.2215, 0.3271, 0.1352, 0.6283, 0.3030, 0.1302, 0.1811]],\n",
      "       dtype=torch.float64)\n",
      "\n",
      "Matrix A: Rank  2  has data  tensor([[0.8381, 0.4943, 0.5984, 0.6167, 0.6128, 0.8593, 0.1344, 0.5146],\n",
      "        [0.1479, 0.4238, 0.5144, 0.7051, 0.8133, 0.1795, 0.2721, 0.4631]],\n",
      "       dtype=torch.float64)\n",
      "Matrix A: Rank  1  has data  tensor([[0.5459, 0.3573, 0.2406, 0.5600, 0.6501, 0.9692, 0.5168, 0.2422],\n",
      "        [0.4312, 0.5917, 0.3425, 0.2202, 0.7030, 0.5629, 0.9259, 0.7612]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, create_unif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distributed matrix can also be created from local chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_chunks(rank, size):\n",
    "    torch.manual_seed(100 + rank)\n",
    "    chunk = torch.randn(2, 4)\n",
    "    print(\"rank \", rank, \"has chunk\", chunk)\n",
    "    A = distmat.THDistMat.from_chunks(chunk)\n",
    "    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)    \n",
    "    if rank == 0:\n",
    "        print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank  0 has chunk tensor([[ 0.3607, -0.2859, -0.3938,  0.2429],\n",
      "        [-1.3833, -2.3134, -0.3172, -0.8660]])\n",
      "rank  1 has chunk tensor([[-1.3905, -0.8152, -0.3204,  0.7377],\n",
      "        [-1.7534,  0.6033, -0.2520, -0.4373]])\n",
      "rank  3 has chunk tensor([[ 1.7286, -0.4007,  2.5587,  1.6848],\n",
      "        [-1.6571, -0.2811,  0.7743, -0.9554]])\n",
      "rank  2 has chunk tensor([[ 0.9907,  0.3349,  1.1497, -0.5498],\n",
      "        [-0.1046,  2.0104, -0.7886, -0.1246]])\n",
      "Matrix A: Rank  3  has data  tensor([[ 1.7286, -0.4007,  2.5587,  1.6848],\n",
      "        [-1.6571, -0.2811,  0.7743, -0.9554]])Matrix A: Rank  1  has data  tensor([[-1.3905, -0.8152, -0.3204,  0.7377],\n",
      "        [-1.7534,  0.6033, -0.2520, -0.4373]])Matrix A: Rank  0  has data  tensor([[ 0.3607, -0.2859, -0.3938,  0.2429],\n",
      "        [-1.3833, -2.3134, -0.3172, -0.8660]])Matrix A: Rank  2  has data  tensor([[ 0.9907,  0.3349,  1.1497, -0.5498],\n",
      "        [-0.1046,  2.0104, -0.7886, -0.1246]])\n",
      "\n",
      "\n",
      "\n",
      "[8, 4]\n"
     ]
    }
   ],
   "source": [
    "run_process(4, from_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be distributed from the master process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_data(rank, size):\n",
    "    if rank == 0:\n",
    "        data = torch.rand(4, 2)\n",
    "        print(\"master data: \", data)\n",
    "    else:\n",
    "        data = None\n",
    "    \n",
    "    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n",
    "    print('data_dist: ', 'Rank ', rank, ' has data ', data_dist.chunk)\n",
    "    dist.barrier()\n",
    "    print(data_dist.shape) # shape of the distributed matrix\n",
    "    dist.barrier()\n",
    "    print(data_dist.sizes) # sizes along the distributed dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master data:  tensor([[0.7118, 0.7876],\n",
      "        [0.4183, 0.9014],\n",
      "        [0.9969, 0.7565],\n",
      "        [0.2239, 0.3023]])\n",
      "data_dist:  Rank  0  has data  tensor([[0.7118, 0.7876]], dtype=torch.float64)\n",
      "data_dist:  Rank  3  has data  tensor([[0.2239, 0.3023]], dtype=torch.float64)\n",
      "data_dist:  Rank  2  has data  tensor([[0.9969, 0.7565]], dtype=torch.float64)\n",
      "data_dist:  Rank  1  has data  tensor([[0.4183, 0.9014]], dtype=torch.float64)\n",
      "[4, 2]\n",
      "[4, 2]\n",
      "[4, 2]\n",
      "[4, 2]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "run_process(4, dist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The default is to create a row-major matrix. They can easily be changed to a column-major matrix by transposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the basic functions work naturally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elemwise_1(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_uniform(4, 2)\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk)    \n",
    "    C = A + B\n",
    "    print(\"C: rank \", rank, \"has chunk\", C.chunk)    \n",
    "    C += A\n",
    "    print(\"C: rank \", rank, \"has chunk\", C.chunk)\n",
    "    \n",
    "    logC = C.log()\n",
    "    print(\"logC: rank \", rank, \"has chunk\", logC.chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "B: rank  2 has chunk tensor([[0.6283, 0.3030]], dtype=torch.float64)\n",
      "B: rank  0 has chunk tensor([[0.9405, 0.2215]], dtype=torch.float64)\n",
      "B: rank  3 has chunk tensor([[0.1302, 0.1811]], dtype=torch.float64)\n",
      "B: rank  1 has chunk tensor([[0.3271, 0.1352]], dtype=torch.float64)\n",
      "C: rank  2 has chunk tensor([[1.0556, 0.7490]], dtype=torch.float64)\n",
      "C: rank  1 has chunk tensor([[1.3022, 0.9263]], dtype=torch.float64)\n",
      "C: rank  3 has chunk tensor([[0.6824, 1.1369]], dtype=torch.float64)\n",
      "C: rank  0 has chunk tensor([[1.6346, 0.5680]], dtype=torch.float64)\n",
      "C: rank  2 has chunk tensor([[1.4830, 1.1949]], dtype=torch.float64)\n",
      "C: rank  1 has chunk tensor([[2.2773, 1.7175]], dtype=torch.float64)\n",
      "C: rank  0 has chunk tensor([[2.3287, 0.9144]], dtype=torch.float64)\n",
      "C: rank  3 has chunk tensor([[1.2346, 2.0928]], dtype=torch.float64)\n",
      "logC: rank  1 has chunk tensor([[0.8230, 0.5409]], dtype=torch.float64)\n",
      "logC: rank  3 has chunk tensor([[0.2107, 0.7385]], dtype=torch.float64)\n",
      "logC: rank  2 has chunk tensor([[0.3941, 0.1781]], dtype=torch.float64)\n",
      "logC: rank  0 has chunk tensor([[ 0.8453, -0.0895]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, elemwise_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting (similar to Julia's dot broadcasting) also works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_broadcasting(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2) # [4] x 2 \n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_ones(4, 1) # [4] x 1\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk)\n",
    "    A += B # B treated as [4] x 2 matrix\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    C = 2 * torch.ones(1, 2, dtype=torch.float64) # 1 x 2\n",
    "    A += C # C treated as [4] x 2 matrix\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "B: rank  1 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "B: rank  3 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "B: rank  2 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "B: rank  0 has chunk tensor([[1.]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[1.5522, 1.9559]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[1.9751, 1.7911]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[1.4274, 1.4460]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[1.6941, 1.3464]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[3.9751, 3.7911]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[3.5522, 3.9559]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[3.4274, 3.4460]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[3.6941, 3.3464]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, dim_broadcasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For general functions, we have `.apply()` and `.apply_binary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elemwise_2(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_uniform(4, 2)\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk) \n",
    "    Asqp1 = A.apply(lambda x: x**2 + 1)\n",
    "    print(\"Asqp1: rank \", rank, \"has chunk\", Asqp1.chunk)    \n",
    "    AsqpBsq = A.apply_binary(B, lambda x, y: x**2 + y**2)\n",
    "    print(\"AsqpBsq: rank \", rank, \"has chunk\", AsqpBsq.chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n",
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n",
      "B: rank  2 has chunk tensor([[0.6283, 0.3030]], dtype=torch.float64)\n",
      "B: rank  1 has chunk tensor([[0.3271, 0.1352]], dtype=torch.float64)\n",
      "B: rank  3 has chunk tensor([[0.1302, 0.1811]], dtype=torch.float64)\n",
      "B: rank  0 has chunk tensor([[0.9405, 0.2215]], dtype=torch.float64)\n",
      "Asqp1: rank  2 has chunk tensor([[1.1826, 1.1989]], dtype=torch.float64)\n",
      "Asqp1: rank  3 has chunk tensor([[1.3049, 1.9137]], dtype=torch.float64)\n",
      "Asqp1: rank  1 has chunk tensor([[1.9508, 1.6259]], dtype=torch.float64)\n",
      "Asqp1: rank  0 has chunk tensor([[1.4818, 1.1200]], dtype=torch.float64)\n",
      "AsqpBsq: rank  2 has chunk tensor([[0.5774, 0.2907]], dtype=torch.float64)\n",
      "AsqpBsq: rank  3 has chunk tensor([[0.3219, 0.9465]], dtype=torch.float64)\n",
      "AsqpBsq: rank  1 has chunk tensor([[1.0578, 0.6442]], dtype=torch.float64)\n",
      "AsqpBsq: rank  0 has chunk tensor([[1.3663, 0.1691]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, elemwise_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions (sum, max, min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summations, minimums, and maximums can be carried out in a way similar to local tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reductions(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    print(\"sum of A: \", A.sum())\n",
    "    print(\"maximum of A: \", A.max())\n",
    "    print(\"minimum of A: \", A.min())\n",
    "    \n",
    "    sumA_row = A.sum(0) # row sum, a tensor with the same values on all processes \n",
    "    sumA_col = A.sum(1) # col sum, a distributed matrix\n",
    "    print(\"row sum of A: \", sumA_row)\n",
    "    \n",
    "    print(\"sumA_col: rank \", rank, \"has chunk\", sumA_col.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  0 has chunk tensor([[0.6941, 0.3464]], dtype=torch.float64)\n",
      "A: rank  3 has chunk tensor([[0.5522, 0.9559]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: rank  1 has chunk tensor([[0.9751, 0.7911]], dtype=torch.float64)\n",
      "A: rank  2 has chunk tensor([[0.4274, 0.4460]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "sum of A:  tensor(5.1882, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "maximum of A:  tensor(0.9751, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "minimum of A:  tensor(0.3464, dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "row sum of A:  tensor([[2.6488, 2.5394]], dtype=torch.float64)\n",
      "sumA_col: rank  0 has chunk tensor([[1.0406]], dtype=torch.float64)\n",
      "sumA_col: rank  2 has chunk tensor([[0.8733]], dtype=torch.float64)\n",
      "sumA_col: rank  1 has chunk tensor([[1.7662]], dtype=torch.float64)\n",
      "sumA_col: rank  3 has chunk tensor([[1.5081]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, reductions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonals(rank, size):\n",
    "    if rank == 0:\n",
    "        p = 4\n",
    "        data = torch.randn(p, p)\n",
    "        print(\"master data: \", data)\n",
    "    else:\n",
    "        data = None\n",
    "        \n",
    "    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n",
    "    \n",
    "    diag1 = data_dist.diag() # distributed diagonal\n",
    "    print(\"diag1: rank \", rank, \"has chunk\", diag1.chunk)\n",
    "    \n",
    "    diag2 = data_dist.diag(distribute=False) # diagonal gathered in each process\n",
    "    print(\"diag2: \", diag2)\n",
    "    \n",
    "    data_dist.fill_diag_(0) # fill the diagonals with zeros\n",
    "    print(\"data_dist: rank \", rank, \"has chunk\", data_dist.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master data:  tensor([[ 0.6857,  0.7877, -0.9778,  2.1302],\n",
      "        [-3.1896,  1.5914, -0.0247, -0.8466],\n",
      "        [ 1.4205, -1.5741, -0.3572, -0.3097],\n",
      "        [ 1.1705, -0.5410, -0.7116,  0.0575]])\n",
      "diag1: rank  0 has chunk tensor([[0.6857]], dtype=torch.float64)\n",
      "diag1: rank  3 has chunk tensor([[0.0575]], dtype=torch.float64)\n",
      "diag1: rank  1 has chunk tensor([[1.5914]], dtype=torch.float64)\n",
      "diag1: rank  2 has chunk tensor([[-0.3572]], dtype=torch.float64)\n",
      "diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)diag2:  tensor([[ 0.6857],\n",
      "        [ 1.5914],\n",
      "        [-0.3572],\n",
      "        [ 0.0575]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n",
      "data_dist: rank  0 has chunk tensor([[ 0.0000,  0.7877, -0.9778,  2.1302]], dtype=torch.float64)\n",
      "data_dist: rank  3 has chunk tensor([[ 1.1705, -0.5410, -0.7116,  0.0000]], dtype=torch.float64)\n",
      "data_dist: rank  2 has chunk tensor([[ 1.4205, -1.5741,  0.0000, -0.3097]], dtype=torch.float64)\n",
      "data_dist: rank  1 has chunk tensor([[-3.1896,  0.0000, -0.0247, -0.8466]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, diagonals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six different scenarios of matrix-matrix multiplications, each representing a different configuration of the split dimension of two input\n",
    "matrices and the output matrix, were considered and implemented. \n",
    "\n",
    "| Secnario | $A$ | $B$ | $AB$ | Description | Usage |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "| 1 | $r \\times$ [$p$] | $[p] \\times q$ | $r \\times$ [$q$]| Inner product, result distributed. | $V^T X$ |\n",
    "| 2 | $[p] \\times q$ | $[q] \\times r$ | $[p] \\times r$ | Fat matrix multiplied by a thin and tall matrix. | $X W^T$ |\n",
    "| 3 | $r \\times$ [$p$] | $[p] \\times s$ | $r \\times s$   | Inner product, result broadcasted. Inner product between two thin matrices. | $V^T V$, $W W^T$ |                                                                           \n",
    "| 4 | $[p] \\times r$ | $r \\times$ [$q$] | $[p] \\times q$ | Outer product, may require large amount of memory. For computing objective function. | $VW$ |\n",
    "| 5 | $[p] \\times r$ | $r \\times s$   | $[p] \\times s$ | A distributed matrix multiplied by a small, distributed matrix. | $VC$ where $C = WW^T$; $CW$ where $C = V^T V$ (transposed) |\n",
    "| 6 | $r \\times$ [$p$] | $p \\times s$   | $r \\times s$   | A distributed matrix multiplied by a thin-and-tall broadcasted matrix. | Matrix-broadcasted vector multiplications. |\n",
    "\n",
    "\n",
    "\n",
    "The implementation of each case is carried\n",
    "out using the collective communication directives. Matrix multiplication scenarios are automatically selected based on the shapes of the input matrices A and\n",
    "B, except for the Scenarios 1 and 3 sharing the same input structure. Those two are further\n",
    "distinguished by the shape of output, AB. The nonnegative matrix factorization involves Scenarios 1 to 5.\n",
    "Scenario 6 is for matrix-vector multiplications, where broadcasting small vectors is almost\n",
    "always efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dist_stat.distmm import *\n",
    "def test_distmm(rank, size):\n",
    "    TType = torch.DoubleTensor\n",
    "    p, q, r = 8, 4, 2\n",
    "    if rank==0:\n",
    "        fat   = TType(p, q).normal_()\n",
    "        thin1 = TType(q, r).normal_()\n",
    "        thin2 = TType(p, r).normal_()\n",
    "    else:\n",
    "        fat, thin1, thin2 = None, TType(q,r), TType(p,r)       \n",
    "    # broadcast thin1 and thin2 so they have same values across all processes\n",
    "    dist.broadcast(thin1,0)\n",
    "    dist.broadcast(thin2,0)\n",
    "    # distribute the matrices\n",
    "    fat_dist   = distmat.dist_data(fat, src=0, TType=TType)\n",
    "    thin1_dist = distmat.dist_data(thin1, src=0, TType=TType)\n",
    "    thin2_dist = distmat.dist_data(thin2, src=0, TType=TType)\n",
    "    \n",
    "    if rank==0:\n",
    "        print(\"Scenario 1: thin2^T x fat\" ) \n",
    "        correct = torch.mm(torch.t(thin2), fat)\n",
    "        print(correct)\n",
    "    dist.barrier()\n",
    "    # (r x [p]) x ([p] x q) = (r x [q]). \n",
    "    # `out_sizes` gives information on how to split [q]. Available only if r != q.\n",
    "    rslt_dist = distmat.mm(thin2_dist.t(), fat_dist, out_sizes=thin1_dist.sizes)\n",
    "    print(\"rslt in rand %d: \"%(rank,), rslt_dist.chunk)\n",
    "    assert not rslt_dist.byrow\n",
    "    dist.barrier()\n",
    "    \n",
    "    if rank==0:\n",
    "        print(\"Scenario 1 (transposed): fat^T x thin2\" )\n",
    "        correct = torch.mm(torch.t(fat), thin2)\n",
    "        print(correct)\n",
    "    # (q x [p]) x ([p] x r) = ([q] x r). \n",
    "    # `out_sizes` gives information on how to split q.\n",
    "    rslt_dist = distmat.mm(fat_dist.t(), thin2_dist, out_sizes=thin1_dist.sizes)\n",
    "    print(\"rslt in rand %d: \"%(rank,), rslt_dist.chunk)\n",
    "    assert rslt_dist.byrow\n",
    "    \n",
    "    if rank==0:\n",
    "        print(\"Scenario 2: fat x thin1\")\n",
    "        correct = torch.mm(fat, thin1)\n",
    "        print(correct)\n",
    "    dist.barrier()\n",
    "    # ([p] x q) x ([q] x r) = ([p] x r).\n",
    "    rslt_dist = distmat.mm(fat_dist, thin1_dist) \n",
    "    print(\"rslt in rank %d: \"%(rank,), rslt_dist.chunk)\n",
    "    assert rslt_dist.byrow\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"Scenario 2 (transposed): thin1^T x fat^T\")\n",
    "        correct = torch.mm(torch.t(thin1), torch.t(fat))\n",
    "        print(correct)\n",
    "    dist.barrier()\n",
    "    # (r x [q]) x (q x [p]) = (r x [p]).\n",
    "    rslt_dist = distmat.mm(thin1_dist.t(), fat_dist.t())\n",
    "    print(\"rslt in rank %d: \"%(rank,), rslt_dist.chunk)\n",
    "    assert not rslt_dist.byrow\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0: \n",
    "        print(\"Scenario 3: thin1^T x thin1\")\n",
    "        thin1_thin1 = torch.mm(torch.t(thin1), thin1)\n",
    "        print(\"correct: \", thin1_thin1)\n",
    "    dist.barrier()\n",
    "    # (r x [p]) x ([p] x s) = (r x s).\n",
    "    # Selected when no `out_sizes` is given.\n",
    "    thin1_thin1_bd = distmat.mm(thin1_dist.t(), thin1_dist)\n",
    "    print(\"rslt in rank %d: \"%(rank,), thin1_thin1_bd)\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"Scenario 4: thin1 x thin2^T\" )\n",
    "        correct = torch.mm(thin1, torch.t(thin2))\n",
    "        print(\"correct: \", correct)\n",
    "    dist.barrier()\n",
    "    # ([p] x r) x (r x [q]) = ([p] x q).\n",
    "    rslt_dist = distmat.mm(thin1_dist, thin2_dist.t())\n",
    "    print(\"rslt in rank %d: \"%(rank,), rslt_dist.chunk)\n",
    "    assert rslt_dist.byrow\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"Scenario 5: thin2 x thin1_thin1\")\n",
    "        correct = torch.mm(thin2, thin1_thin1)\n",
    "        print(\"correct: \", correct)\n",
    "    dist.barrier()\n",
    "    rslt_dist = distmat.mm(thin2_dist, thin1_thin1_bd)\n",
    "    print(\"rslt in rank %d: \"%(rank,), rslt_dist.chunk)\n",
    "    assert rslt_dist.byrow # the result must be row-major\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"Scenario 5 (transposed): thin1_thin1 x thin2^T\")\n",
    "        correct = torch.mm(thin1_thin1, torch.t(thin2))\n",
    "        print(\"correct: \", correct)\n",
    "    dist.barrier()\n",
    "    # ([p] x r) x (r x s) = ([p] x s).\n",
    "    rslt_dist = distmat.mm(thin1_thin1_bd, thin2_dist.t())\n",
    "    print(\"rslt in rank %d: \"%(rank,), rslt_dist.chunk)\n",
    "    assert not rslt_dist.byrow # the result is col-major\n",
    "    dist.barrier()\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"Scenario 6: thin1^T x thin1 (local broadcasted)\" )\n",
    "        correct = torch.mm(torch.t(thin1), thin1)\n",
    "        print(\"correct: \", correct)\n",
    "    # (r x [p]) x (p x s) = (r x s).\n",
    "    rslt_dist = distmat.mm(thin1_dist.t(), thin1) #distmm_db_b(thin1_dist.t(), thin1)\n",
    "    print(\"rslt in rank %d (local broadcasted): \"%(rank,), rslt_dist)\n",
    "    dist.barrier()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1: thin2^T x fat\n",
      "tensor([[ 0.1560,  4.6003,  1.0414, -2.2172],\n",
      "        [-0.3434,  5.0076,  1.5957,  2.9931]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rslt in rand 0:  tensor([[ 0.1560],\n",
      "        [-0.3434]], dtype=torch.float64)rslt in rand 2:  tensor([[1.0414],\n",
      "        [1.5957]], dtype=torch.float64)rslt in rand 1:  tensor([[4.6003],\n",
      "        [5.0076]], dtype=torch.float64)rslt in rand 3:  tensor([[-2.2172],\n",
      "        [ 2.9931]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n",
      "Scenario 1 (transposed): fat^T x thin2\n",
      "tensor([[ 0.1560, -0.3434],\n",
      "        [ 4.6003,  5.0076],\n",
      "        [ 1.0414,  1.5957],\n",
      "        [-2.2172,  2.9931]], dtype=torch.float64)\n",
      "rslt in rand 0:  tensor([[ 0.1560, -0.3434]], dtype=torch.float64)\n",
      "rslt in rand 1:  tensor([[4.6003, 5.0076]], dtype=torch.float64)\n",
      "rslt in rand 3:  tensor([[-2.2172,  2.9931]], dtype=torch.float64)\n",
      "rslt in rand 2:  tensor([[1.0414, 1.5957]], dtype=torch.float64)\n",
      "Scenario 2: fat x thin1\n",
      "tensor([[ 3.5298,  1.1274],\n",
      "        [-0.5137,  0.8368],\n",
      "        [-3.9244, -2.0757],\n",
      "        [-1.2072,  0.7788],\n",
      "        [ 0.9921,  2.5060],\n",
      "        [ 0.9099,  1.0310],\n",
      "        [ 0.9737,  0.2209],\n",
      "        [ 0.5159,  2.2222]], dtype=torch.float64)\n",
      "rslt in rank 0:  tensor([[ 3.5298,  1.1274],\n",
      "        [-0.5137,  0.8368]], dtype=torch.float64)rslt in rank 3:  tensor([[0.9737, 0.2209],\n",
      "        [0.5159, 2.2222]], dtype=torch.float64)rslt in rank 2:  tensor([[0.9921, 2.5060],\n",
      "        [0.9099, 1.0310]], dtype=torch.float64)rslt in rank 1:  tensor([[-3.9244, -2.0757],\n",
      "        [-1.2072,  0.7788]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n",
      "Scenario 2 (transposed): thin1^T x fat^T\n",
      "tensor([[ 3.5298, -0.5137, -3.9244, -1.2072,  0.9921,  0.9099,  0.9737,  0.5159],\n",
      "        [ 1.1274,  0.8368, -2.0757,  0.7788,  2.5060,  1.0310,  0.2209,  2.2222]],\n",
      "       dtype=torch.float64)\n",
      "rslt in rank 0:  tensor([[ 3.5298, -0.5137],\n",
      "        [ 1.1274,  0.8368]], dtype=torch.float64)rslt in rank 3:  tensor([[0.9737, 0.5159],\n",
      "        [0.2209, 2.2222]], dtype=torch.float64)rslt in rank 2:  tensor([[0.9921, 0.9099],\n",
      "        [2.5060, 1.0310]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "rslt in rank 1:  tensor([[-3.9244, -1.2072],\n",
      "        [-2.0757,  0.7788]], dtype=torch.float64)\n",
      "Scenario 3: thin1^T x thin1\n",
      "correct:  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)\n",
      "rslt in rank 0:  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)rslt in rank 1:  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)rslt in rank 3:  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)rslt in rank 2:  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n",
      "Scenario 4: thin1 x thin2^T\n",
      "correct:  tensor([[ 1.1175e-02, -6.4276e-01,  3.6403e-01, -1.4644e+00,  7.6965e-01,\n",
      "          7.6863e-01, -1.2183e+00, -6.3738e-01],\n",
      "        [ 6.7342e-01, -3.2122e-02, -1.8491e+00,  4.7996e-03,  1.2569e+00,\n",
      "          2.5120e+00,  1.0808e+00, -6.9450e-04],\n",
      "        [ 9.5521e-01, -1.4435e-01, -2.5621e+00, -2.1846e-01,  1.8981e+00,\n",
      "          3.6750e+00,  1.3429e+00, -9.9027e-02],\n",
      "        [-5.6381e-01,  1.7571e-01,  1.4567e+00,  3.3532e-01, -1.2259e+00,\n",
      "         -2.2716e+00, -6.1843e-01,  1.4827e-01]], dtype=torch.float64)\n",
      "rslt in rank 2:  tensor([[ 0.9552, -0.1444, -2.5621, -0.2185,  1.8981,  3.6750,  1.3429, -0.0990]],\n",
      "       dtype=torch.float64)rslt in rank 3:  tensor([[-0.5638,  0.1757,  1.4567,  0.3353, -1.2259, -2.2716, -0.6184,  0.1483]],\n",
      "       dtype=torch.float64)rslt in rank 1:  tensor([[ 6.7342e-01, -3.2122e-02, -1.8491e+00,  4.7996e-03,  1.2569e+00,\n",
      "          2.5120e+00,  1.0808e+00, -6.9450e-04]], dtype=torch.float64)rslt in rank 0:  tensor([[ 0.0112, -0.6428,  0.3640, -1.4644,  0.7697,  0.7686, -1.2183, -0.6374]],\n",
      "       dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n",
      "Scenario 5: thin2 x thin1_thin1\n",
      "correct:  tensor([[-2.6394e+00, -2.2901e+00],\n",
      "        [-1.0171e-03,  1.0083e+00],\n",
      "        [ 7.3252e+00,  5.7357e+00],\n",
      "        [-3.0822e-01,  2.0339e+00],\n",
      "        [-4.7784e+00, -5.3229e+00],\n",
      "        [-9.7018e+00, -9.5605e+00],\n",
      "        [-4.4803e+00, -1.9448e+00],\n",
      "        [-1.2323e-01,  8.9467e-01]], dtype=torch.float64)\n",
      "rslt in rank 2:  tensor([[-4.7784, -5.3229],\n",
      "        [-9.7018, -9.5605]], dtype=torch.float64)rslt in rank 3:  tensor([[-4.4803, -1.9448],\n",
      "        [-0.1232,  0.8947]], dtype=torch.float64)rslt in rank 0:  tensor([[-2.6394e+00, -2.2901e+00],\n",
      "        [-1.0171e-03,  1.0083e+00]], dtype=torch.float64)rslt in rank 1:  tensor([[ 7.3252,  5.7357],\n",
      "        [-0.3082,  2.0339]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n",
      "Scenario 5 (transposed): thin1_thin1 x thin2^T\n",
      "correct:  tensor([[-2.6394e+00, -1.0171e-03,  7.3252e+00, -3.0822e-01, -4.7784e+00,\n",
      "         -9.7018e+00, -4.4803e+00, -1.2323e-01],\n",
      "        [-2.2901e+00,  1.0083e+00,  5.7357e+00,  2.0339e+00, -5.3229e+00,\n",
      "         -9.5605e+00, -1.9448e+00,  8.9467e-01]], dtype=torch.float64)\n",
      "rslt in rank 0:  tensor([[-2.6394e+00, -1.0171e-03],\n",
      "        [-2.2901e+00,  1.0083e+00]], dtype=torch.float64)rslt in rank 2:  tensor([[-4.7784, -9.7018],\n",
      "        [-5.3229, -9.5605]], dtype=torch.float64)rslt in rank 1:  tensor([[ 7.3252, -0.3082],\n",
      "        [ 5.7357,  2.0339]], dtype=torch.float64)rslt in rank 3:  tensor([[-4.4803, -0.1232],\n",
      "        [-1.9448,  0.8947]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n",
      "Scenario 6: thin1^T x thin1 (local broadcasted)\n",
      "correct:  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)\n",
      "rslt in rank 1 (local broadcasted):  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)rslt in rank 0 (local broadcasted):  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)rslt in rank 3 (local broadcasted):  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)rslt in rank 2 (local broadcasted):  tensor([[4.5482, 2.9523],\n",
      "        [2.9523, 4.1017]], dtype=torch.float64)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_process(4, test_distmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, distributed matrix may have a sparse matrix as local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm_sparse(rank, size):\n",
    "    def to_sparse(x):\n",
    "        \"\"\" converts dense tensor x to sparse format \"\"\"\n",
    "        x_typename = torch.typename(x).split('.')[-1]\n",
    "        sparse_tensortype = getattr(torch.sparse, x_typename)\n",
    "\n",
    "        indices = torch.nonzero(x)\n",
    "        if len(indices.shape) == 0:  # if all elements are zeros\n",
    "            return sparse_tensortype(*x.shape)\n",
    "        indices = indices.t()\n",
    "        values = x[tuple(indices[i] for i in range(indices.shape[0]))]\n",
    "        return sparse_tensortype(indices, values, x.size())\n",
    "\n",
    "    TType = torch.DoubleTensor\n",
    "    q, r = 4, 2\n",
    "    if rank==0:\n",
    "        thin1 = TType(q, r).normal_()\n",
    "    else:\n",
    "        thin1 = TType(q,r)\n",
    "        \n",
    "    # broadcast thin1 and thin2 so they have same values across all processes\n",
    "    dist.broadcast(thin1,0)\n",
    "    \n",
    "    # distribute the matrices\n",
    "    thin1_dist = distmat.dist_data(thin1, src=0, TType=TType) \n",
    "\n",
    "    # construct sparse matrices (no actual zero in this case, but uses sparse matrix data structure)\n",
    "    thin1_sparse_chunk = to_sparse(thin1_dist.chunk)\n",
    "    print(\"Sparse x dense, Scenario 3: in rank \", rank, \"we have: \", thin1_sparse_chunk)\n",
    "    thin1_sparse_dist = THDistMat.from_chunks(thin1_sparse_chunk)\n",
    "\n",
    "    if rank==0:\n",
    "        print(\"correct: \", torch.mm(thin1.t(), thin1))\n",
    "\n",
    "    r =  distmat.mm(thin1_sparse_dist.t(), thin1_dist )\n",
    "    print(\"rslt: \", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse x dense, Scenario 3: in rank  0 we have:  tensor(indices=tensor([[0, 0],\n",
      "                       [0, 1]]),\n",
      "       values=tensor([-0.3172, -0.8660]),\n",
      "       size=(1, 2), nnz=2, dtype=torch.float64, layout=torch.sparse_coo)Sparse x dense, Scenario 3: in rank  3 we have:  tensor(indices=tensor([[0, 0],\n",
      "                       [0, 1]]),\n",
      "       values=tensor([-2.3652, -0.8047]),\n",
      "       size=(1, 2), nnz=2, dtype=torch.float64, layout=torch.sparse_coo)\n",
      "\n",
      "Sparse x dense, Scenario 3: in rank  1 we have:  tensor(indices=tensor([[0, 0],\n",
      "                       [0, 1]]),\n",
      "       values=tensor([ 1.7482, -0.2759]),\n",
      "       size=(1, 2), nnz=2, dtype=torch.float64, layout=torch.sparse_coo)Sparse x dense, Scenario 3: in rank  2 we have:  tensor(indices=tensor([[0, 0],\n",
      "                       [0, 1]]),\n",
      "       values=tensor([-0.9755,  0.4790]),\n",
      "       size=(1, 2), nnz=2, dtype=torch.float64, layout=torch.sparse_coo)\n",
      "\n",
      "correct:  tensor([[9.7023, 1.2284],\n",
      "        [1.2284, 1.7031]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rslt:  tensor([[9.7023, 1.2284],\n",
      "        [1.2284, 1.7031]], dtype=torch.float64)rslt:  tensor([[9.7023, 1.2284],\n",
      "        [1.2284, 1.7031]], dtype=torch.float64)rslt:  tensor([[9.7023, 1.2284],\n",
      "        [1.2284, 1.7031]], dtype=torch.float64)\n",
      "rslt:  tensor([[9.7023, 1.2284],\n",
      "        [1.2284, 1.7031]], dtype=torch.float64)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_process(4, mm_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonnegative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate a nonnegative data matrix $X \\in \\mathbb{R}^{m \\times p}$ by $VW$, $V \\in \\mathbb{R}^{m \\times r}$ and $W \\in \\mathbb{R}^{r \\times p}$. In a simple setting, NMF minimizes\n",
    "\\begin{align*}\n",
    "f(V, W) =  \\|X - VW\\|_\\mathrm{F}^2.\n",
    "\\end{align*}\n",
    "\n",
    "Multiplicative algorithm [Lee and Seung, 1999, 2001]:\n",
    "\\begin{align*}\n",
    "V^{n+1} &= V^n \\odot [X (W^n)^T] \\oslash [V^n W^n (W^n)^T] \\\\\n",
    "W^{n+1} &= W^n \\odot [(V^{n+1})^T X] \\oslash [(V^{n+1})^T V^{n+1} W^n],\n",
    "\\end{align*}\n",
    "where $\\odot$ and $\\oslash$ denote elementwise multiplication and division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a simplified version. The full object-oriented version is included in our package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf(rank, size):\n",
    "    p = 8; q = 12; r = 3\n",
    "    maxiter = 1000\n",
    "    TensorType=torch.DoubleTensor\n",
    "    data = distmat.distgen_uniform(p, q, TType=TensorType)\n",
    "    V = distmat.distgen_uniform(p, r, TType=TensorType)\n",
    "    W = distmat.distgen_uniform(q, r, TType=TensorType).t()\n",
    "    for i in range(maxiter):\n",
    "        XWt =  distmat.mm(data, W.t())\n",
    "        WWt =  distmat.mm(W, W.t())\n",
    "        VWWt = distmat.mm(V, WWt)\n",
    "        V.mul_(XWt).div_(VWWt)\n",
    "\n",
    "        VtX  = distmat.mm(V.t(), data, out_sizes=W.sizes)\n",
    "        VtV  = distmat.mm(V.t(), V)\n",
    "        VtVW = distmat.mm(VtV, W)\n",
    "        W = W.mul_(VtX).div_(VtVW)\n",
    "        if (i+1) % 100 == 0:\n",
    "            # print objective\n",
    "            outer = distmat.mm(V, W)\n",
    "            val = ((data - outer)**2).sum()\n",
    "            if rank == 0:\n",
    "                print(\"Iteration {}: {}\".format(i+1, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100: 2.182438128886236\n",
      "Iteration 200: 2.171416631750278\n",
      "Iteration 300: 2.169680386094355\n",
      "Iteration 400: 2.168717940886758\n",
      "Iteration 500: 2.167978637258126\n",
      "Iteration 600: 2.167353482884231\n",
      "Iteration 700: 2.1668354282699847\n",
      "Iteration 800: 2.1664281438361552\n",
      "Iteration 900: 2.1661187200995116\n",
      "Iteration 1000: 2.165885899848217\n"
     ]
    }
   ],
   "source": [
    "run_process(4, nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the package `dist_stat`, we have implemented...\n",
    "\n",
    "* Nonnegative matrix factorization:\n",
    "    * Multiplicative method\n",
    "    * Alternating proximal gradient method\n",
    "* Positron Emission Tomography:\n",
    "    * with $\\ell_2$-penalty, MM method\n",
    "    * with $\\ell_1$-penlaty, primal-dual method\n",
    "* Multidimensional Scaling\n",
    "* $\\ell_1$-regularized Cox regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_mult(rank, size):\n",
    "    import dist_stat.nmf as nmf\n",
    "    p = 8; q = 12; r = 3\n",
    "    maxiter = 3000\n",
    "    TensorType=torch.DoubleTensor\n",
    "    torch.manual_seed(100)\n",
    "    data = distmat.distgen_uniform(p, q, TType=TensorType, set_from_master=True) # to guarantee same input matrices throughout experiments\n",
    "    driver = nmf.NMF(data, r)\n",
    "    V, W = driver.run(maxiter=maxiter, tol=1e-5, check_interval=100, check_obj=True) \n",
    "    # if check_obj=False, the objective value is not estimated.\n",
    "    # the convergence is determined based on maximum change in V and W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "p=8, q=12, r=3\n",
      "  iter\t      V_maxdiff\t      W_maxdiff\t        reldiff\t            obj\t      time\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   100\t2.269086434e-04\t7.859795175e-04\t            inf\t2.450653178e+00\t   0.43880\n",
      "   200\t4.741822670e-05\t1.487613723e-04\t3.867071318e-04\t2.449319301e+00\t   0.43996\n",
      "   300\t2.064260964e-05\t7.563094246e-05\t2.621296866e-05\t2.449228887e+00\t   0.40669\n",
      "   400\t2.000570493e-05\t7.475431599e-05\t2.011233401e-05\t2.449159516e+00\t   0.42035\n",
      "   500\t2.812954891e-05\t9.523429625e-05\t3.124107173e-05\t2.449051764e+00\t   0.40650\n",
      "   600\t4.381995135e-05\t1.446528443e-04\t5.162722771e-05\t2.448873708e+00\t   0.43803\n",
      "   700\t6.619583670e-05\t2.162175833e-04\t8.868082228e-05\t2.448567886e+00\t   0.39906\n",
      "   800\t9.643257393e-05\t3.092734985e-04\t1.528160691e-04\t2.448040970e+00\t   0.40759\n",
      "   900\t1.325321757e-04\t4.134715304e-04\t2.479164259e-04\t2.447186356e+00\t   0.39362\n",
      "  1000\t1.650634082e-04\t4.965977026e-04\t3.522855042e-04\t2.445972390e+00\t   0.39836\n",
      "  1100\t1.783495447e-04\t5.148614934e-04\t4.124739364e-04\t2.444551602e+00\t   0.39160\n",
      "  1200\t1.640825205e-04\t4.541542368e-04\t3.852711367e-04\t2.443225027e+00\t   0.39599\n",
      "  1300\t1.314835115e-04\t3.496689842e-04\t2.881911715e-04\t2.442233006e+00\t   0.39511\n",
      "  1400\t9.656804741e-05\t2.477632207e-04\t1.817436148e-04\t2.441607516e+00\t   0.39734\n",
      "  1500\t6.842890808e-05\t1.702706661e-04\t1.049224902e-04\t2.441246451e+00\t   0.39490\n",
      "  1600\t4.845288571e-05\t1.175770934e-04\t5.961547715e-05\t2.441041312e+00\t   0.40239\n",
      "  1700\t3.491156633e-05\t8.303925294e-05\t3.477617383e-05\t2.440921650e+00\t   0.40315\n",
      "  1800\t2.577709669e-05\t6.035303685e-05\t2.118545696e-05\t2.440848754e+00\t   0.39594\n",
      "  1900\t1.952574334e-05\t4.515135725e-05\t1.351999715e-05\t2.440802234e+00\t   0.39606\n",
      "  2000\t1.515135760e-05\t3.469121126e-05\t9.012603356e-06\t2.440771224e+00\t   0.39143\n",
      "--------------------------------------------------------------------------------\n",
      "Completed. total time: 8.117448568344116\n"
     ]
    }
   ],
   "source": [
    "run_process(4, nmf_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternating projected gradient (APG) with ridge penalties:\n",
    "\n",
    "\\begin{align*}\n",
    "f(V, W; \\epsilon) =  \\|X - VW\\|_\\mathrm{F}^2 + \\frac{\\epsilon}{2} \\|V\\|_\\mathrm{F}^2 + \\frac{\\epsilon}{2} \\|W\\|_\\mathrm{F}^2\n",
    "\\end{align*}\n",
    "is minimized. \n",
    "The corresponding APG update is given by\n",
    "\\begin{align*}\n",
    "V^{n+1} &= P_+ \\left((1 - \\sigma_n \\epsilon) V^n - \\sigma_n (V^n W^n (W^n)^T - X (W^n)^T) \\right) \\\\\n",
    "W^{n+1} &= P_+ \\left((1 - \\tau_n \\epsilon) W^n - \\tau_n ((V^{n+1})^T V^{n+1} W^n - (V^{n+1})^TX ) \\right).\n",
    "\\end{align*}\n",
    "\n",
    "The below is the APG with $\\epsilon=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_apg(rank, size):\n",
    "    import dist_stat.nmf_pg as nmf\n",
    "    p = 8; q = 12; r = 3\n",
    "    maxiter = 3000\n",
    "    TensorType=torch.DoubleTensor\n",
    "    torch.manual_seed(100)\n",
    "    data = distmat.distgen_uniform(p, q, TType=TensorType, set_from_master=True)\n",
    "    driver = nmf.NMF(data, r)\n",
    "    V, W = driver.run(maxiter=maxiter, tol=1e-5, check_interval=100, check_obj=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "p=8, q=12, r=3\n",
      "  iter\t      V_maxdiff\t      W_maxdiff\t        reldiff\t            obj\t      time\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   100\t2.424162256e-03\t3.832382365e-03\t            inf\t2.491386053e+00\t   0.61891\n",
      "   200\t4.781538175e-04\t4.835312406e-04\t1.270388614e-02\t2.447588284e+00\t   0.62712\n",
      "   300\t1.557436503e-04\t1.730102196e-04\t4.787130896e-04\t2.445938668e+00\t   0.57892\n",
      "   400\t1.891190770e-04\t1.971794805e-04\t1.873943553e-04\t2.445293040e+00\t   0.57223\n",
      "   500\t1.988674663e-04\t2.022453200e-04\t1.796544182e-04\t2.444674189e+00\t   0.57298\n",
      "   600\t2.006114927e-04\t2.003183483e-04\t1.786643789e-04\t2.444058858e+00\t   0.56944\n",
      "   700\t1.974948598e-04\t1.938984347e-04\t1.720535717e-04\t2.443466398e+00\t   0.56738\n",
      "   800\t1.902900729e-04\t1.837831141e-04\t1.594379137e-04\t2.442917466e+00\t   0.56681\n",
      "   900\t1.795209595e-04\t1.706543356e-04\t1.419437735e-04\t2.442428835e+00\t   0.56539\n",
      "  1000\t1.658913058e-04\t1.553432628e-04\t1.213973120e-04\t2.442010984e+00\t   0.56436\n",
      "  1100\t1.502856314e-04\t1.387773767e-04\t9.986312239e-05\t2.441667288e+00\t   0.56695\n",
      "  1200\t1.336529807e-04\t1.218600833e-04\t7.919658999e-05\t2.441394742e+00\t   0.57079\n",
      "  1300\t1.168795022e-04\t1.053656452e-04\t6.073685599e-05\t2.441185735e+00\t   0.56559\n",
      "  1400\t1.006952236e-04\t8.987846338e-05\t4.520438315e-05\t2.441030185e+00\t   0.56437\n",
      "  1500\t8.562931901e-05\t7.577858676e-05\t3.277190496e-05\t2.440917420e+00\t   0.56561\n",
      "  1600\t7.200861323e-05\t6.326137565e-05\t2.322727525e-05\t2.440837498e+00\t   0.57356\n",
      "  1700\t5.998482412e-05\t5.237538927e-05\t1.614933164e-05\t2.440781932e+00\t   0.56671\n",
      "  1800\t4.957510521e-05\t4.306504944e-05\t1.104878262e-05\t2.440743916e+00\t   0.57537\n",
      "  1900\t4.070410505e-05\t3.520949373e-05\t7.458682630e-06\t2.440718253e+00\t   0.56758\n",
      "--------------------------------------------------------------------------------\n",
      "Completed. total time: 10.92510199546814\n"
     ]
    }
   ],
   "source": [
    "run_process(4, nmf_apg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it goes large-scale with GPU or SIMD acceleration, the time difference becomes much smaller with faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$-regularized Cox Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maximize\n",
    "$$\n",
    "f(\\beta) = L(\\beta) - \\lambda \\|\\beta\\|_1,\n",
    "$$\n",
    "where $L(\\beta)$ is the Log-partial likelihood of Cox proportional hazards model:\n",
    "\\begin{align*}\n",
    "L (\\beta) = \\sum_{i=1}^m \\delta_i \\left[\\beta^T x_i - \\log \\left(\\sum_{j: y_j \\ge y_i} \\exp(\\beta^T x_j)\\right)\\right]. \n",
    "\\end{align*}\n",
    "\n",
    "* $y_i = \\min \\{t_i, c_i\\}$\n",
    "    * $t_i$: time to event\n",
    "    * $c_i$: right-censoring time for that sample\n",
    "* $\\delta = (\\delta_1, \\dotsc, \\delta_m)^T$\n",
    "    * $\\delta_i= I_{\\{t_i \\le c_i\\}}$: indicator for censoredness of sample $i$.  \n",
    "\n",
    "The gradient of $L(\\beta)$ is given by \n",
    "\\begin{align*}\n",
    "\\nabla L(\\beta) = X^T (I-P) \\delta,\n",
    "\\end{align*} \n",
    "  \n",
    "where $w_i = \\exp(x_i^T \\beta)$, $W_j = \\sum_{i: y_i \\ge y_j} w_i$, $P = (\\pi_{ij})$, and \n",
    "$$\n",
    "\\pi_{ij} = I(y_i \\ge y_j) w_i/W_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the proximal gradient method:\n",
    "\n",
    "\\begin{align*}\n",
    "w_i^{n+1} &= \\exp(x_i^T \\beta); \\;\\; W_j^{n+1} = \\sum_{i: y_i \\ge y_j} w_i^{n+1}\\\\\n",
    "\\pi_{ij}^{n+1} &= I(t_i \\ge t_j) w_i^{n+1} / W_j^{n+1} \\\\\n",
    "\\Delta^{n+1} &= X^T (I - P^{n+1}) \\delta, \\;\\; \\text{where $P^{n+1} = (\\pi_{ij}^{n+1})$} \\\\\n",
    "\\beta^{n+1} &= \\mathcal{S}_{\\lambda}(\\beta^n + \\sigma \\Delta^{n+1}),\n",
    "\\end{align*}\n",
    "\n",
    "* $\\mathcal{S}_\\lambda(\\cdot)$ is the soft-thresholding operator, the proximity operator of $\\lambda \\|\\cdot \\|_1$. \n",
    "$$\n",
    "  [\\mathcal{S}_{\\lambda}(u)]_i = \\mathrm{sign}(u_i) (|u_i| - \\lambda)_+.\n",
    "$$\n",
    "* Convergence guaranteed when $\\sigma \\le 1/(2 \\|X\\|_2^2)$.\n",
    "* $W_j$ can be computed using `cumsum` function when the data are sorted in nonincreasing order of $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cox_l1(rank, size):\n",
    "    import dist_stat.cox as cox\n",
    "    n = 8; p = 12\n",
    "    lambd = 0.001\n",
    "    maxiter = 3000\n",
    "    torch.manual_seed(100)\n",
    "    TensorType = torch.DoubleTensor\n",
    "    # The below is how one would create a column-major matrix.\n",
    "    # We want column-major matrix to invoke matrix multiplication scenario 3. (`beta` is distributed.)\n",
    "    X = distmat.distgen_normal(p, n, TType=TensorType, set_from_master=True).t()     \n",
    "    torch.manual_seed(200)\n",
    "    delta = torch.multinomial(torch.tensor([1., 1.]), n, replacement=True).float().view(-1, 1).type(TensorType) # 50% censored, 50% noncensored\n",
    "    dist.broadcast(delta, 0) # same delta shared across processes\n",
    "    cox_driver = cox.COX(X, delta, lambd, seed=300, TType=TensorType, sigma='power') # power iteration to estimate matrix norm\n",
    "    beta = cox_driver.run(maxiter, tol=1e-5, check_interval=100, check_obj=True)\n",
    "    zeros = (beta == 0).type(torch.int64).sum() # elementwise equality (resulting in uint8 type) casted to int64 then summed up. \n",
    "                                                                    # omitting the casting will cause overflow on high-dimensional data.\n",
    "    if rank == 0:\n",
    "        print(\"number of zeros:\", zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n",
      "/home/kose/.conda/envs/pytorch-cpu/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing max singular value...\n",
      "iteration 0\n",
      "done computing max singular value:  tensor(5.5130, dtype=torch.float64)\n",
      "step size:  0.016451195454478245\n",
      "Starting...\n",
      "n=8, p=12\n",
      "  iter\t        maxdiff\t        reldiff\t            obj\t      time\n",
      "--------------------------------------------------------------------------------\n",
      "   100\t4.263906103e-03\t            inf\t-6.928162346e-01\t   0.38736\n",
      "   200\t2.137461323e-03\t1.988562069e-03\t-4.120260836e-01\t   0.38458\n",
      "   300\t1.407481825e-03\t7.553056657e-04\t-3.128646711e-01\t   0.37742\n",
      "   400\t1.007443583e-03\t3.915195766e-04\t-2.634000845e-01\t   0.35287\n",
      "   500\t7.746264875e-04\t2.342474319e-04\t-2.344826456e-01\t   0.38189\n",
      "   600\t6.264630031e-04\t1.527405316e-04\t-2.159107600e-01\t   0.36084\n",
      "   700\t5.266433517e-04\t1.052134521e-04\t-2.032509415e-01\t   0.34661\n",
      "   800\t4.567976771e-04\t7.524416395e-05\t-1.942647959e-01\t   0.34587\n",
      "   900\t4.066494457e-04\t5.527596186e-05\t-1.876996717e-01\t   0.33700\n",
      "  1000\t3.700033916e-04\t4.142489420e-05\t-1.827999355e-01\t   0.33781\n",
      "  1100\t3.429047528e-04\t3.152303512e-05\t-1.790831076e-01\t   0.34030\n",
      "  1200\t2.903550321e-04\t2.943983961e-05\t-1.756220950e-01\t   0.34063\n",
      "  1300\t2.754370962e-04\t2.312241598e-05\t-1.729100437e-01\t   0.33765\n",
      "  1400\t2.627810270e-04\t1.848812238e-05\t-1.707455549e-01\t   0.33663\n",
      "  1500\t2.498584959e-04\t1.492477462e-05\t-1.690008475e-01\t   0.33606\n",
      "  1600\t2.418480673e-04\t1.209101955e-05\t-1.675891132e-01\t   0.34338\n",
      "  1700\t2.457311543e-04\t9.832383412e-06\t-1.664422225e-01\t   0.33732\n",
      "--------------------------------------------------------------------------------\n",
      "Completed. total time: 5.987252950668335\n",
      "number of zeros: tensor(5)\n"
     ]
    }
   ],
   "source": [
    "run_process(4, cox_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate 10,000 x 10,000 examples on 2-8 GPUs on our server. The scripts in the `examples` directory are designed to automatically select the GPU device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure can also be utilized on multi-node clusters. The structure was used for the analysis of 200,000 x 500,000 UK Biobank data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI-only, lightweight, more flexible version in Julia is in preparation. CUDA-aware MPI support for the central MPI interface [MPI.jl](https://github.com/JuliaParallel/MPI.jl) was added in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "pytorch-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

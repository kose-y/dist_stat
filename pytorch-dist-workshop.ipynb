{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# A Distributed Matrix Data Structure and Its Statistical Applications on PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**The Programming Workshop at the Inaugural Kenneth Lange Symposium, Feb 21-22, 2020**\n",
    "\n",
    "_Seyoon Ko and Joong-Ho (Johann) Won_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We developed a distributed matrix operation package suitable for distributed matrix-vector operations and distributed tall-and-thin (or wide-and-short) matrices.\n",
    "The code runs on both multi-node machines and multi-GPU machines using PyTorch.\n",
    "We have applied this package for four statistical applications, namely, nonnegative matrix factorization (NMF), multidimensional scaling (MDS), positron emission tomography (PET), and $\\ell_1$-regularized Cox regression.\n",
    "In particular, $200,000 \\times 500,000$ $\\ell_1$-regularized Cox regression with the UK Biobank dataset was the biggest joint multivariate survival analysis to our knowledge. \n",
    "In this workshop, we provide small examples that run on a single node, and demonstrate multi-GPU usage on our machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Contents\n",
    "\n",
    "* Brief introduction to PyTorch operations\n",
    "* `torch.distributed` package\n",
    "* Distributed matrix data structure in package `dist_stat`\n",
    "* Applications: Nonnegative matrix factorization and $\\ell_1$-penalized Cox regression\n",
    "* Demonstration on multi-GPU machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. It has two goals of development:\n",
    "* A replacement for NumPy to use the power of GPUs $\\rightarrow$ optimization of numerical operations\n",
    "* A deep learning research platform that provides maximum flexibility and speed $\\rightarrow$ optimization of automatic gradient computation e.g. backpropagation\n",
    "\n",
    "We are trying to exploit the former in a distributed environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Basic PyTorch Operations\n",
    "We introduce simple operations on PyTorch. Note that Python uses 0-based, row-major ordering, like C and C++ (cf. R and Julia have 1-based, column-major ordering). First we import the PyTorch\n",
    "library. This is similar to `library()` in R and equivalent to `import ...` in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning) # hide `UserWarning`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may create an uninitialized tensor. This creates a 3 × 4 tensor (matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.empty(3, 4) # uninitialized tensor. Julia equivalent: Array{Float32}(undef, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is equivalent to `set.seed()` in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100) # Julia equivalent: Random.seed!(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a tensor initialized with random values from (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(3, 4) # from Unif(0, 1). \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate a tensor filled with zeros or ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.ones(3, 4) # torch.zeros(3, 4)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be created from standard Python data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([3, 4, 5, 6])\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can be created in certain datatype (default: float32) and on certain device (default: CPU) of choice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double precision\n",
    "w = torch.tensor([3, 4, 5, 6], dtype=torch.float64)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # on GPU number zero. will not run if CUDA GPU is not present.\n",
    "# w = torch.tensor([3, 4, 5, 6], device='cuda:0')\n",
    "# w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of a tensor can be accessed by appending `.shape` to the tensor name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensor can have datatype and location changed by the method `.to()`. The arguments are similar to choosing datatype and device of the new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w.to(device = \"cpu\", dtype=torch.int32)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are standard method of indexing tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[2, 3] # indexing: zero-based, returns a 0-dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indexing always returns a (sub)tensor, even for scalars (treated as zero-dimensional tensors).\n",
    "A standard Python number can be returned by using .item()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[2, 3].item() # A standard Python floating-point number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a column from a tensor, we use the indexing as below. The syntax is similar but slightly\n",
    "different from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:, 3] # 3rd column. The leftmost column is 0th. cf. y[, 4] in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is for taking a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[2, :] # 2nd row. The top row is 0th. cf. y[3, ] in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide an example of simple operations on PyTorch. Addition using the operator ‘+’ acts\n",
    "just like anyone can expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y + z # a simple addition.\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another form of addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.add(y, z) # another syntax for addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operators ending with an underscore (`_`) changes the value of the tensor in-place. Otherwise, the argument never changes. Unlike methods ending with `!` in Julia, this rule is strictly enforced in PyTorch. (The underscore determines usage of the keyword `const` in C++-level.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.add_(z) # in-place addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concatenate the tensors using the function `cat()`, which resembles `c()`, `cbind()`, and\n",
    "`rbind()` in R, `cat()`, `vcat()`, `hcat()` in Julia. The second argument indicates the dimension that the tesors are concatenated\n",
    "along: zero means by concatenation by rows, and one means by columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((y, z), 0) # along the rows. cf. vcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((y, z), 1) # along the columns. cf. hcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summation/reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `.sum()`, `.prod()`, `mean()` methods of a tensor do the obvious. Optional argument determines the dimension of reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum(0) # reduces rows, columnwise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum(1) # reduces columns, rowwise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum((0, 1)) # reduces rows and columns -> a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix transpose is performed by appending `.t()` to a tensor. Matrix multiplication is carried out by the method `torch.mm()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(y, z.t()) # Note: y is 3 x 4, z is 3 x 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.distributed`: Distributed subpackage for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.distributed` is the subpackage for distributed operations on PyTorch. The interface is mostly inspired by the message passing interface (MPI). The available backends are:\n",
    "\n",
    "* Gloo, a collective communication library developed by Facebook, included in PyTorch. Full support for CPU, partial collective communication only for GPU.\n",
    "* MPI, a good-old communication standard. The most flexible, but PyTorch needs to be compiled from its source to use it as a backend. Full support for GPU if the MPI installation is \"CUDA-aware\".\n",
    "* NCCL, Nvidia Collective Communications Library, collective communication only for multiple GPUs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we use Gloo for its full functionalities on CPU and runnability on Jupyter Notebook. The experiments in our paper use MPI for running multi-node setting and multi-GPU setting with basically the same code. The interface below is specific for Gloo backend. For MPI backend, please consult with a section from [distributed package tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends) or [our example code](https://github.com/kose-y/dist_stat/tree/master/examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "\n",
    "def init_process(rank, size, fn, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "    fn(rank, size)\n",
    "\n",
    "def run_process(size, fn):\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, args=(rank, size, fn))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        \n",
    "    for p in processes:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each distributed function from now on will have `rank` and `size` as the two arguments. When `run_process` is called with the communicator size `size` and the function name `fn`, `size` process will be launched, and each of them will call `fn` with `rank` of each process and the `size`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-to-point communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_images/send_recv.png)\n",
    "\n",
    "Figure courtesy of: https://pytorch.org/tutorials/_images/send_recv.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Blocking point-to-point communication.\"\"\"\n",
    "\n",
    "def point_to_point(rank, size):\n",
    "    tensor = torch.zeros(1)\n",
    "    if rank == 0:\n",
    "        tensor += 1\n",
    "        # Send the tensor to process 1\n",
    "        dist.send(tensor=tensor, dst=1)\n",
    "    elif rank == 1:\n",
    "        # Receive tensor from process 0\n",
    "        dist.recv(tensor=tensor, src=0)\n",
    "    dist.barrier()\n",
    "    print('Rank ', rank, ' has data ', tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, point_to_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collective communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | | \n",
    "|:---|:---|\n",
    "| ![](https://pytorch.org/tutorials/_images/scatter.png) | ![](https://pytorch.org/tutorials/_images/gather.png) |\n",
    "| Scatter | Gather |\n",
    "| ![](https://pytorch.org/tutorials/_images/reduce.png) | ![](https://pytorch.org/tutorials/_images/all_reduce.png) |\n",
    "| Reduce | All-reduce |\n",
    "| ![](https://pytorch.org/tutorials/_images/broadcast.png) | ![](https://pytorch.org/tutorials/_images/all_gather.png) |\n",
    "| Broadcast | All-gather |\n",
    "\n",
    "Table courtesy of: https://pytorch.org/tutorials/intermediate/dist_tuto.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the code for simple Monte Carlo $\\pi$ estimation. 100,000 $x_i$ and $y_i$ are sampled from $Unif(0,1)$ for each process, and we measure the proportion of $(x_i, y_i)$s inside the unit quartercircle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_pi(n, size):\n",
    "    # this code is executed on each process.\n",
    "    x = torch.rand((n), dtype=torch.float64)\n",
    "    y = torch.rand((n), dtype=torch.float64)\n",
    "    # compute local estimate of pi\n",
    "    r = torch.mean((x**2 + y**2 < 1).to(dtype=torch.float64))*4\n",
    "    dist.all_reduce(r) # sum of 'r's in each device is stored in 'r'\n",
    "    return r / size\n",
    "\n",
    "def run_mc_pi(rank, size):\n",
    "    n = 100000\n",
    "    torch.manual_seed(100 + rank)\n",
    "    r = mc_pi(n, size)\n",
    "    if rank == 0:\n",
    "        print(r.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, run_mc_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `distmat`: Distributed Matrices on PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tensor operations and communication package, we created a data structure for a distributed matrix. \n",
    "* Each process (enumerated by rank) holds a contiguous block of the full data matrix by rows or columns.\n",
    "* The data may be a sparse matrix. \n",
    "* If GPUs are involved, each process controls a GPU whose index matches the process rank. \n",
    "* From now on: [100] × 100 matrix split over four processes means...\n",
    "    * Rank 0 process keeps rows [0:25). of the matrix, in row-major ordering.\n",
    "    * Rank 3 process keeps rows [75:100).\n",
    "* Limitation: length of distributed dimension should be divisible by number of processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dist_stat.distmat as distmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `distgen_ones()`: Creates a distributed matrix filled with ones\n",
    "- `distgen_zeros()`: Creates a distributed matrix filled with zeros\n",
    "- `distgen_uniform()`: Creates a distributed matrix from uniform distribution\n",
    "- `distgen_normal()`: Creates a distributed matrix from stndard normal distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unif(rank, size):\n",
    "    A = distmat.distgen_uniform(8, 8, TType=torch.DoubleTensor) # the default is row-distributed, row-major ordering data. \n",
    "    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, create_unif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distributed matrix can also be created from local chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_chunks(rank, size):\n",
    "    torch.manual_seed(100 + rank)\n",
    "    chunk = torch.randn(2, 4)\n",
    "    print(\"rank \", rank, \"has chunk\", chunk)\n",
    "    A = distmat.THDistMat.from_chunks(chunk)\n",
    "    print('Matrix A:', 'Rank ', rank, ' has data ', A.chunk)    \n",
    "    if rank == 0:\n",
    "        print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, from_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can be distributed from a master process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_data(rank, size):\n",
    "    if rank == 0:\n",
    "        data = torch.rand(4, 2)\n",
    "        print(\"master data: \", data)\n",
    "    else:\n",
    "        data = None\n",
    "    \n",
    "    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n",
    "    print('data_dist: ', 'Rank ', rank, ' has data ', data_dist.chunk)\n",
    "    dist.barrier()\n",
    "    print(data_dist.shape) # shape of the distributed matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, dist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: The default is to create a row-major matrix. They can easily be changed to a column-major matrix by transposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the basic functions work naturally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elemwise_1(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier() # dist.barrier() waits until all other processes reach the same line. It is used to make the output easier to read. \n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier()\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk)    \n",
    "    C = A + B\n",
    "    dist.barrier()\n",
    "    print(\"C: rank \", rank, \"has chunk\", C.chunk)    \n",
    "    C.add_(A) # functionally equivalent to C += A\n",
    "    dist.barrier()\n",
    "    print(\"C: rank \", rank, \"has chunk\", C.chunk)\n",
    "    \n",
    "    logC = C.log()\n",
    "    dist.barrier()\n",
    "    print(\"logC: rank \", rank, \"has chunk\", logC.chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, elemwise_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting (similar to Julia's dot operation) also works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_broadcasting(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2) # [4] x 2 \n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_ones(4, 1) # [4] x 1\n",
    "    dist.barrier()\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk)\n",
    "    A.add_(B) # B treated as [4] x 2 matrix\n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    C = 2 * torch.ones(1, 2, dtype=torch.float64) # 1 x 2\n",
    "    A.add_(C) # C treated as [4] x 2 matrix\n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, dim_broadcasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For general functions, we have `.apply()` and `.apply_binary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elemwise_2(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    B = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier()\n",
    "    print(\"B: rank \", rank, \"has chunk\", B.chunk) \n",
    "    Asqp1 = A.apply(lambda x: x**2 + 1) # anonymous function definition: x -> x .^ 2 .+ 1 in Julia\n",
    "    dist.barrier()\n",
    "    print(\"Asqp1: rank \", rank, \"has chunk\", Asqp1.chunk)    \n",
    "    AsqpBsq = A.apply_binary(B, lambda x, y: x**2 + y**2) # anonymous function definition: (x, y) -> x.^2 .+ y .^2 in Julia\n",
    "    dist.barrier()\n",
    "    print(\"AsqpBsq: rank \", rank, \"has chunk\", AsqpBsq.chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, elemwise_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reductions (sum, max, min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summations, minimums, and maximums can be carried out in a way similar to local tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reductions(rank, size):\n",
    "    A = distmat.distgen_uniform(4, 2)\n",
    "    dist.barrier()\n",
    "    print(\"A: rank \", rank, \"has chunk\", A.chunk)\n",
    "    dist.barrier()\n",
    "    print(\"sum of A: \", A.sum())\n",
    "    dist.barrier()\n",
    "    print(\"maximum of A: \", A.max())\n",
    "    dist.barrier()\n",
    "    print(\"minimum of A: \", A.min())\n",
    "    \n",
    "    sumA_row = A.sum(0) # row sum, output: a tensor with the same values on all processes \n",
    "    sumA_col = A.sum(1) # col sum, output: a distributed matrix\n",
    "    dist.barrier()\n",
    "    print(\"row sum of A: \", sumA_row)\n",
    "    dist.barrier()\n",
    "    print(\"sumA_col: rank \", rank, \"has chunk\", sumA_col.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, reductions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonals(rank, size):\n",
    "    if rank == 0:\n",
    "        p = 4\n",
    "        data = torch.randn(p, p)\n",
    "        print(\"master data: \", data)\n",
    "    else:\n",
    "        data = None\n",
    "        \n",
    "    data_dist = distmat.dist_data(data, src=0, TType=torch.DoubleTensor)\n",
    "    \n",
    "    diag1 = data_dist.diag() # distributed diagonal\n",
    "    print(\"diag1: rank \", rank, \"has chunk\", diag1.chunk)\n",
    "    \n",
    "    diag2 = data_dist.diag(distribute=False) # diagonal gathered for each process\n",
    "    print(\"diag2: \", diag2)\n",
    "    \n",
    "    data_dist.fill_diag_(0) # fill the diagonals with zeros\n",
    "    print(\"data_dist: rank \", rank, \"has chunk\", data_dist.chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, diagonals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Six different scenarios of matrix-matrix multiplications, each representing a different configuration of the split dimension of two input\n",
    "matrices and the output matrix, were considered and implemented. \n",
    "\n",
    "| Secnario | $A$ | $B$ | $AB$ | Description | Usage |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "| 1 | $r \\times$ [$p$] | $[p] \\times q$ | $r \\times$ [$q$]| Inner product, result distributed. | $V^T X$ |\n",
    "| 2 | $[p] \\times q$ | $[q] \\times r$ | $[p] \\times r$ | Fat matrix multiplied by a thin and tall matrix. | $X W^T$ |\n",
    "| 3 | $r \\times$ [$p$] | $[p] \\times s$ | $r \\times s$   | Inner product, result broadcasted. Inner product between two thin matrices. | $V^T V$, $W W^T$ |                                                                           \n",
    "| 4 | $[p] \\times r$ | $r \\times$ [$q$] | $[p] \\times q$ | Outer product, may require large amount of memory. For computing objective function. | $VW$ |\n",
    "| 5 | $[p] \\times r$ | $r \\times s$   | $[p] \\times s$ | A distributed matrix multiplied by a small, distributed matrix. | $VC$ where $C = WW^T$; $CW$ where $C = V^T V$ (transposed) |\n",
    "| 6 | $r \\times$ [$p$] | $p \\times s$   | $r \\times s$   | A distributed matrix multiplied by a thin-and-tall broadcasted matrix. | Matrix-broadcasted vector multiplications. |\n",
    "\n",
    "\n",
    "\n",
    "The implementation of each case is carried\n",
    "out using the collective communication directives. Matrix multiplication scenarios are automatically selected based on the shapes of the input matrices A and\n",
    "B, except for the Scenarios 1 and 3 sharing the same input structure. Those two are further\n",
    "distinguished by the shape of output, AB. The nonnegative matrix factorization involves Scenarios 1 to 5.\n",
    "Scenario 6 is for matrix-vector multiplications, where broadcasting small vectors is almost\n",
    "always efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonnegative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate a nonnegative data matrix $X \\in \\mathbb{R}^{m \\times p}$ by $VW$, $V \\in \\mathbb{R}^{m \\times r}$ and $W \\in \\mathbb{R}^{r \\times p}$. In a simple setting, NMF minimizes\n",
    "\\begin{align*}\n",
    "f(V, W) =  \\|X - VW\\|_\\mathrm{F}^2.\n",
    "\\end{align*}\n",
    "\n",
    "Multiplicative algorithm [Lee and Seung, 1999, 2001], which can be understood as a case of MM algorithm:\n",
    "\\begin{align*}\n",
    "V^{n+1} &= V^n \\odot [X (W^n)^T] \\oslash [V^n W^n (W^n)^T] \\\\\n",
    "W^{n+1} &= W^n \\odot [(V^{n+1})^T X] \\oslash [(V^{n+1})^T V^{n+1} W^n],\n",
    "\\end{align*}\n",
    "where $\\odot$ and $\\oslash$ denote elementwise multiplication and division."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a simplified version. The full object-oriented version is included in our package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf(rank, size):\n",
    "    p = 8; q = 12; r = 3 # p is \"big\", q is \"big\", r is \"small\".\n",
    "    maxiter = 1000\n",
    "    TensorType=torch.DoubleTensor\n",
    "    data = distmat.distgen_uniform(p, q, TType=TensorType) # [p] x q\n",
    "    V = distmat.distgen_uniform(p, r, TType=TensorType) # [p] x r\n",
    "    W = distmat.distgen_uniform(q, r, TType=TensorType).t() # r x [q]\n",
    "    for i in range(maxiter):\n",
    "        XWt =  distmat.mm(data, W.t()) # Scenario 2, input ([p] x q), ([q] x r), output ([p] x r)\n",
    "        WWt =  distmat.mm(W, W.t()) # Scenario 3, input (r x [q]), ([q] x r), output (r x r)\n",
    "        VWWt = distmat.mm(V, WWt) # Scenario 5, input ([p] x r), (r x r), output ([p] x r)\n",
    "        V.mul_(XWt).div_(VWWt) # in-place op\n",
    "\n",
    "        VtX  = distmat.mm(V.t(), data, out_sizes=W.sizes) # Scenario 1, input (r x [p]), ([p] x q), output (r x [q])\n",
    "        VtV  = distmat.mm(V.t(), V) # Scenario 3, input (r x [p]), ([p] x r), output r x r\n",
    "        VtVW = distmat.mm(VtV, W) # Scenario 5 (transposed), input (r x r), (r x [q]), output r x [q]\n",
    "        W = W.mul_(VtX).div_(VtVW) # in-place op\n",
    "        if (i+1) % 100 == 0:\n",
    "            # print objective\n",
    "            outer = distmat.mm(V, W) # Scenario 4, input ([p] x r), (r x [q]), output [p] x q\n",
    "            val = ((data - outer)**2).sum()\n",
    "            if rank == 0:\n",
    "                print(\"Iteration {}: {}\".format(i+1, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the package `dist_stat`, we have implemented...\n",
    "\n",
    "* Nonnegative matrix factorization:\n",
    "    * Multiplicative method\n",
    "    * Alternating proximal gradient method\n",
    "* Positron Emission Tomography:\n",
    "    * with $\\ell_2$-penalty, MM method\n",
    "    * with $\\ell_1$-penlaty, primal-dual method\n",
    "* Multidimensional Scaling\n",
    "* $\\ell_1$-regularized Cox regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_mult(rank, size):\n",
    "    import dist_stat.nmf as nmf\n",
    "    p = 8; q = 12; r = 3\n",
    "    maxiter = 3000\n",
    "    TensorType=torch.DoubleTensor\n",
    "    torch.manual_seed(100)\n",
    "    data = distmat.distgen_uniform(p, q, TType=TensorType, set_from_master=True) # to guarantee same input matrices throughout experiments\n",
    "    nmf_driver = nmf.NMF(data, r)\n",
    "    V, W = nmf_driver.run(maxiter=maxiter, tol=1e-5, check_interval=100, check_obj=True) \n",
    "    # if check_obj=False, the objective value is not estimated.\n",
    "    # the convergence is determined based on maximum change in V and W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, nmf_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternating projected gradient (APG) with ridge penalties:\n",
    "\n",
    "\\begin{align*}\n",
    "f(V, W; \\epsilon) =  \\|X - VW\\|_\\mathrm{F}^2 + \\frac{\\epsilon}{2} \\|V\\|_\\mathrm{F}^2 + \\frac{\\epsilon}{2} \\|W\\|_\\mathrm{F}^2\n",
    "\\end{align*}\n",
    "is minimized. \n",
    "\n",
    "The corresponding APG update is given by\n",
    "\\begin{align*}\n",
    "V^{n+1} &= P_+ \\left((1 - \\sigma_n \\epsilon) V^n - \\sigma_n (V^n W^n (W^n)^T - X (W^n)^T) \\right) \\\\\n",
    "W^{n+1} &= P_+ \\left((1 - \\tau_n \\epsilon) W^n - \\tau_n ((V^{n+1})^T V^{n+1} W^n - (V^{n+1})^TX ) \\right).\n",
    "\\end{align*}\n",
    "\n",
    "The below is the APG with $\\epsilon=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmf_apg(rank, size):\n",
    "    import dist_stat.nmf_pg as nmf\n",
    "    p = 8; q = 12; r = 3\n",
    "    maxiter = 3000\n",
    "    TensorType=torch.DoubleTensor\n",
    "    torch.manual_seed(100)\n",
    "    data = distmat.distgen_uniform(p, q, TType=TensorType, set_from_master=True)\n",
    "    nmf_driver = nmf.NMF(data, r)\n",
    "    V, W = nmf_driver.run(maxiter=maxiter, tol=1e-5, check_interval=100, check_obj=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, nmf_apg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it goes large-scale with GPU or SIMD acceleration, the time difference becomes much smaller with faster convergence. APG also avoids numerical underflow that may happen in multiplicative algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$-regularized Cox Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maximize\n",
    "$$\n",
    "f(\\beta) = L(\\beta) - \\lambda \\|\\beta\\|_1,\n",
    "$$\n",
    "where $L(\\beta)$ is the Log-partial likelihood of Cox proportional hazards model:\n",
    "\\begin{align*}\n",
    "L (\\beta) = \\sum_{i=1}^m \\delta_i \\left[\\beta^T x_i - \\log \\left(\\sum_{j: y_j \\ge y_i} \\exp(\\beta^T x_j)\\right)\\right]. \n",
    "\\end{align*}\n",
    "\n",
    "* $y_i = \\min \\{t_i, c_i\\}$\n",
    "    * $t_i$: time to event\n",
    "    * $c_i$: right-censoring time for that sample\n",
    "* $\\delta = (\\delta_1, \\dotsc, \\delta_m)^T$\n",
    "    * $\\delta_i= I_{\\{t_i \\le c_i\\}}$: indicator for censoredness of sample $i$.  \n",
    "\n",
    "The gradient of $L(\\beta)$ is given by \n",
    "\\begin{align*}\n",
    "\\nabla L(\\beta) = X^T (I-P) \\delta,\n",
    "\\end{align*} \n",
    "  \n",
    "where $w_i = \\exp(x_i^T \\beta)$, $W_j = \\sum_{i: y_i \\ge y_j} w_i$, $P = (\\pi_{ij})$, and \n",
    "$$\n",
    "\\pi_{ij} = I(y_i \\ge y_j) w_i/W_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the proximal gradient method:\n",
    "\n",
    "\\begin{align*}\n",
    "w_i^{n+1} &= \\exp(x_i^T \\beta); \\;\\; W_j^{n+1} = \\sum_{i: y_i \\ge y_j} w_i^{n+1}\\\\\n",
    "\\pi_{ij}^{n+1} &= I(t_i \\ge t_j) w_i^{n+1} / W_j^{n+1} \\\\\n",
    "\\Delta^{n+1} &= X^T (I - P^{n+1}) \\delta, \\;\\; \\text{where $P^{n+1} = (\\pi_{ij}^{n+1})$} \\\\\n",
    "\\beta^{n+1} &= \\mathcal{S}_{\\lambda}(\\beta^n + \\sigma \\Delta^{n+1}),\n",
    "\\end{align*}\n",
    "\n",
    "* $\\mathcal{S}_\\lambda(\\cdot)$ is the soft-thresholding operator, the proximity operator of $\\lambda \\|\\cdot \\|_1$. \n",
    "$$\n",
    "  [\\mathcal{S}_{\\lambda}(u)]_i = \\mathrm{sign}(u_i) (|u_i| - \\lambda)_+.\n",
    "$$\n",
    "* Convergence guaranteed when $\\sigma \\le 1/(2 \\|X\\|_2^2)$.\n",
    "* $W_j$ can be computed using `cumsum` function when the data are sorted in nonincreasing order of $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cox_l1(rank, size):\n",
    "    import dist_stat.cox_breslow as cox\n",
    "    n = 8; p = 12\n",
    "    lambd = 0.001\n",
    "    maxiter = 3000\n",
    "    torch.manual_seed(100)\n",
    "    TensorType = torch.DoubleTensor\n",
    "    # The below is how one would create a column-major matrix.\n",
    "    # We want column-major matrix to invoke matrix multiplication scenario 3. (`beta` is distributed.)\n",
    "    X = distmat.distgen_normal(p, n, TType=TensorType, set_from_master=True).t()     \n",
    "    torch.manual_seed(200)\n",
    "    delta = torch.multinomial(torch.tensor([1., 1.]), n, replacement=True).float().view(-1, 1).type(TensorType) # 50% censored, 50% noncensored\n",
    "    dist.broadcast(delta, 0) # same delta shared across processes\n",
    "    t = torch.arange(n, 0, -1) # Just assuming decreasing time\n",
    "    cox_driver = cox.COX(X, delta, lambd, time=t, seed=300, TType=TensorType, sigma='power') # power iteration to estimate matrix norm\n",
    "    beta = cox_driver.run(maxiter, tol=1e-5, check_interval=100, check_obj=True)\n",
    "    zeros = (beta == 0).type(torch.int64).sum() # elementwise equality (resulting in uint8 type) casted to int64 then summed up. \n",
    "                                                                    # omitting the casting will cause overflow on high-dimensional data.\n",
    "    if rank == 0:\n",
    "        print(\"number of zeros:\", zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_process(4, cox_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate 10,000 x 10,000 examples on 2-8 GPUs on our server. The scripts in the `examples` directory are designed to automatically select the GPU device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structure can also be utilized on multi-node clusters. The structure was used for the analysis of 200,000 x 500,000 UK Biobank data with 20 c5.18xlarge instances (36 physical cores, 144GB memory each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI-only, lightweight, more flexible version in Julia is in preparation. CUDA-aware MPI support for the central MPI interface [MPI.jl](https://github.com/JuliaParallel/MPI.jl) was added in the process. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "pytorch-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
